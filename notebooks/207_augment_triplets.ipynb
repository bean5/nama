{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139bee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028823c5",
   "metadata": {},
   "source": [
    "# Augment triplets\n",
    "\n",
    "Add easy-negatives and under-represented tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0364890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.data.utils import read_csv\n",
    "from src.models.tokenizer import get_tokenize_function_and_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774de195",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_surname = \"given\"\n",
    "num_common_names = 10000\n",
    "\n",
    "num_easy_pos_negs = 100\n",
    "num_easy_neg_negs = 5\n",
    "num_common_negs = 1000\n",
    "num_common_neg_copies = 2\n",
    "num_anchor_pos_neg_copies = 1\n",
    "under_represented_threshold = 500\n",
    "\n",
    "max_tokens = 10\n",
    "use_phonemes = False\n",
    "use_edit_subwords = False\n",
    "vocab_type = 'f'  # tokenizer based upon training name frequency\n",
    "use_bigrams = False\n",
    "use_pretrained_embeddings = False\n",
    "subword_vocab_size = 2000  # 500, 1000, 1500, 2000\n",
    "\n",
    "triplets_path=f\"../data/processed/tree-hr-{given_surname}-triplets-v2-1000.csv.gz\"\n",
    "\n",
    "pref_path = f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\"\n",
    "common_non_negatives_path = f\"../data/processed/common_{given_surname}_non_negatives-augmented.csv\"\n",
    "nama_bucket = 'nama-data'\n",
    "subwords_path=f\"../data/models/fs-{given_surname}-subword-tokenizer-{subword_vocab_size}{vocab_type}.json\"\n",
    "\n",
    "augmented_path=f\"../data/processed/tree-hr-{given_surname}-triplets-v2-1000-augmented.csv.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c030b7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read triplets\n",
    "triplets_df = read_csv(triplets_path)\n",
    "print(len(triplets_df))\n",
    "triplets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique anchor-pos pairs and anchor-neg pairs\n",
    "anchor_pos = set()\n",
    "anchor_neg = set()\n",
    "for tup in tqdm(triplets_df.itertuples()):\n",
    "    anchor = tup.anchor\n",
    "    pos = tup.positive\n",
    "    neg = tup.negative\n",
    "    anchor_pos.add(f\"{anchor}:{pos}\")\n",
    "    anchor_neg.add(f\"{anchor}:{neg}\")\n",
    "print(len(anchor_pos))\n",
    "print(len(anchor_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d0d78",
   "metadata": {},
   "source": [
    "### read common names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee56867",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_df = read_csv(pref_path)\n",
    "common_names = [name for name in pref_df['name'][:num_common_names].tolist() \\\n",
    "                if len(name) > 1 and re.fullmatch(r'[a-z]+', name)]\n",
    "pref_df = None\n",
    "len(common_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ad99c",
   "metadata": {},
   "source": [
    "### read common non-negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_non_negatives = set()\n",
    "\n",
    "common_non_negatives_df = read_csv(common_non_negatives_path)\n",
    "for name1, name2 in common_non_negatives_df.values.tolist():\n",
    "    common_non_negatives.add((name1, name2))\n",
    "len(common_non_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a6eec",
   "metadata": {},
   "source": [
    "## Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't add anchor-pos pairs that teach the model bad habits\n",
    "bad_anchor_pos_pairs = [('maria', 'annamaria'), \n",
    "                        ('marie', 'annamarie'),\n",
    "                       ]\n",
    "\n",
    "def is_bad_anchor_pos_pair(name1, name2):\n",
    "    for bad_name1, bad_name2 in bad_anchor_pos_pairs:\n",
    "        if (name1 == bad_name1 and name2 == bad_name2) or \\\n",
    "           (name2 == bad_name1 and name1 == bad_name2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47871b51",
   "metadata": {},
   "source": [
    "### Get tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize, tokenizer_vocab = get_tokenize_function_and_vocab(\n",
    "    max_tokens=max_tokens,\n",
    "    subwords_path=subwords_path,\n",
    "    nama_bucket=nama_bucket,\n",
    ")\n",
    "len(tokenizer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('dallan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181fba23",
   "metadata": {},
   "source": [
    "### Add anchor-pos-neg triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment data with easy negatives\n",
    "aug_data = []\n",
    "seen_anchor_pos = set()\n",
    "seen_anchor_neg = set()\n",
    "for tup in tqdm(triplets_df.itertuples()):\n",
    "    anchor = tup.anchor\n",
    "    pos = tup.positive\n",
    "    neg = tup.negative\n",
    "    if is_bad_anchor_pos_pair(anchor, pos):\n",
    "        continue\n",
    "    # anchor, positive, hard-negative\n",
    "    for _ in range(num_anchor_pos_neg_copies):\n",
    "        aug_data.append({\n",
    "            'anchor': anchor,\n",
    "            'positive': pos,\n",
    "            'negative': neg,\n",
    "            'positive_score': tup.positive_score,\n",
    "            'negative_score': tup.negative_score,\n",
    "        })\n",
    "\n",
    "    # add anchor-pos-easy-negatives\n",
    "    # only add easy negatives the first time we see this anchor,pos pair\n",
    "    anchor_pos = f\"{anchor},{pos}\"\n",
    "    if anchor_pos not in seen_anchor_pos:\n",
    "        seen_anchor_pos.add(anchor_pos)\n",
    "        ix = 0\n",
    "        while ix < num_easy_pos_negs:\n",
    "            # anchor, positive, easy-negative\n",
    "            easy_neg = random.choice(common_names)\n",
    "            # only add anchor-pos-easy-neg if easy-neg isn't really a non-negative\n",
    "            if anchor == easy_neg or pos == easy_neg or (anchor, easy_neg) in common_non_negatives:\n",
    "                continue\n",
    "            aug_data.append({\n",
    "                'anchor': anchor,\n",
    "                'positive': pos,\n",
    "                'negative': easy_neg,\n",
    "                'positive_score': tup.positive_score,\n",
    "                'negative_score': 0.0,\n",
    "            })\n",
    "            ix += 1\n",
    "\n",
    "    # add anchor-neg-easy-negatives\n",
    "    # only add easy negatives the first time we see this anchor,neg pair\n",
    "    neg_anchor = f\"{neg},{anchor}\"\n",
    "    if neg_anchor not in seen_anchor_neg:\n",
    "        seen_anchor_neg.add(neg_anchor)\n",
    "        ix = 0\n",
    "        while ix < num_easy_neg_negs:\n",
    "            easy_neg = random.choice(common_names)\n",
    "            # only add anchor-neg-easy-neg if easy-neg isn't really a non-negative\n",
    "            if anchor == easy_neg or neg == easy_neg or (anchor, easy_neg) in common_non_negatives:\n",
    "                continue\n",
    "            aug_data.append({\n",
    "                'anchor': anchor,\n",
    "                'positive': neg,\n",
    "                'negative': easy_neg,\n",
    "                'positive_score': tup.negative_score,\n",
    "                'negative_score': 0.0,\n",
    "            })\n",
    "            ix += 1\n",
    "            \n",
    "len(aug_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9f680",
   "metadata": {},
   "source": [
    "### Add pos-pos-easyneg triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a15d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for pos in tqdm(common_names[:num_common_negs]):\n",
    "    for neg in common_names[:num_common_negs]:\n",
    "        if pos == neg or (pos, neg) in common_non_negatives:\n",
    "            continue\n",
    "        for _ in range(num_common_neg_copies):\n",
    "            aug_data.append({\n",
    "                'anchor': pos,\n",
    "                'positive': pos,\n",
    "                'negative': neg,\n",
    "                'positive_score': 1.0,\n",
    "                'negative_score': 0.0,\n",
    "            })\n",
    "            cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ca1ed",
   "metadata": {},
   "source": [
    "### Add triplets for names we want to push apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32997ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_apart_pairs = [('charles', 'frances'),\n",
    "                    ('marie', 'annie'),\n",
    "                    ('james', 'jane'),\n",
    "                    ('jane', 'janos'),\n",
    "                    ('hannah', 'hans'),\n",
    "                    ('frank', 'frederick'),\n",
    "                    ('anne', 'anders'),\n",
    "                    ('maria', 'manuel'),\n",
    "                    ('maria', 'manuela'),\n",
    "                    ('juan','julia'),\n",
    "                    ('margaret','violet'),\n",
    "                    ('antonio','emilio'),\n",
    "                    ('edward','edwin'),\n",
    "                    ('samuel','smith'),\n",
    "                    ('martin','augustin'),\n",
    "                    ('eva','evan'),\n",
    "                    ('dejesus','de'),\n",
    "                    ('dejesus','dean'),\n",
    "                    ('eliza','luiza'),\n",
    "                    ('frank','mark'),\n",
    "                    ('benjamin','benita'),\n",
    "                    ('andrew','matthew'),\n",
    "                    ('andrew','mathew'),\n",
    "                    ('guadalupe','guy'),\n",
    "                    ('jeanne','susanne'),\n",
    "                    ('delacruz','delaconcepcion'),\n",
    "                    ('rebecca','veronica'),\n",
    "                    ('rebecca','francesca'),\n",
    "                    ('karl','karen'),\n",
    "                    ('karl','karin'),\n",
    "                    ('adam','ada'),\n",
    "                    ('adam','addie'),\n",
    "                    ('bertha','bruce'),\n",
    "                    ('edith','edmond'),\n",
    "                    ('mathias','elias'),\n",
    "                    ('anton','anta'),\n",
    "                    ('ethel','effie'),\n",
    "                    ('delcarmen','oscar'),\n",
    "                    ('santiago','santos'),\n",
    "                    ('vicente','clemente'),\n",
    "                    ('ysabel','ysidro'),\n",
    "                    ('karen','karolina'),\n",
    "                    ('ralph','christoph'),\n",
    "                    ('raymond','reyes'),\n",
    "                    ('maren','christen'),\n",
    "                    ('christoph','jph'),\n",
    "                    ('erzsebet','jozsef'),\n",
    "                    ('carlos','marcos'),\n",
    "                    ('ada','adamus'),\n",
    "                    ('delaluz','dela'),\n",
    "                    ('jennie','jemima'),\n",
    "                    ('lorenzo','vincenzo'),\n",
    "                    ('stina','stella'),\n",
    "                    ('pearl','per'),\n",
    "                    ('pearl','pehr'),\n",
    "                    ('oscar','encarnacion'),\n",
    "                    ('veronica','francesca'),\n",
    "                    ('sebastiana','victoriana'),\n",
    "                    ('elias','matias'),\n",
    "                    ('myrtle','estelle'),\n",
    "                    ('bernardo','leonardo'),\n",
    "                    ('amy','amos'),\n",
    "                    ('leslie','lester'),\n",
    "                    ('rosario','hilario'),\n",
    "                    ('karin','karolina'),\n",
    "                    ('nora','norma'),\n",
    "                    ('michaela','mc'),\n",
    "                    ('christiana','luciana'),\n",
    "                    ('chen','chester'),\n",
    "                    ('angelina','augustina'),\n",
    "                    ('sam','smith'),\n",
    "                    ('soledad','solomon'),\n",
    "                    ('mari','jacobi'),\n",
    "                    ('mari','eli'),\n",
    "                    ('mari','josephi'),\n",
    "                    ('mari','li'),\n",
    "                    ('delacrus','delaconcepcion'),\n",
    "                    ('etta','etienne'),\n",
    "                    ('imre','ines'),\n",
    "                    ('florentina','valentina'),\n",
    "                    ('jacobi','josephi'),\n",
    "                    ('joanne','susanne'),\n",
    "                    ('bernardino','florentino'),\n",
    "                    ('josefina','rufina'),\n",
    "                    ('eli','josephi'),\n",
    "                    ('dean','delia'),\n",
    "                    ('emilio','mario'),\n",
    "                    ('jenny','jemima'),\n",
    "                    ('paulino','antonino'),\n",
    "                    # 13 Oct 2023\n",
    "                    ('anne','anders'),\n",
    "                    ('ann','amy'),\n",
    "                    ('dejesus','de'),\n",
    "                    ('dejesus','dedios'),\n",
    "                    ('anders','an'),\n",
    "                    ('ana','anastacia'),\n",
    "                    ('de','dedios'),\n",
    "                    ('mae','mette'),\n",
    "                    ('betty','bell'),\n",
    "                    ('jesus','julius'),\n",
    "                    ('joao','joel'),\n",
    "                    ('clarence','claire'),\n",
    "                    ('martina','marta'),\n",
    "                    ('roy','ray'),\n",
    "                    ('pearl','per'),\n",
    "                    ('veronica','domenica'),\n",
    "                    ('elias','elisa'),\n",
    "                    ('lidia','li'),\n",
    "                    ('sven','sue'),\n",
    "                    ('bernardino','paulino'),\n",
    "                    ('bernardino','antonino'),\n",
    "                    ('eli','elin'),\n",
    "                    ('emilio','mario'),\n",
    "                    ('antal','an'),\n",
    "                    ('anta','an'),\n",
    "                    ('paulino','antonino'),\n",
    "                   ]\n",
    "push_apart_copies = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb430d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anchor, neg in push_apart_pairs:\n",
    "    for _ in range(push_apart_copies):\n",
    "        aug_data.append({\n",
    "            'anchor': anchor,\n",
    "            'positive': anchor,\n",
    "            'negative': neg,\n",
    "            'positive_score': 1.0,\n",
    "            'negative_score': 0.0,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99431d39",
   "metadata": {},
   "source": [
    "## Analyze training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c83ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id2text = {}\n",
    "for text, id_ in tokenizer_vocab.items():\n",
    "    token_id2text[id_] = text\n",
    "\n",
    "counter = Counter()\n",
    "for row in tqdm(aug_data):\n",
    "    for key in ['anchor', 'positive', 'negative']:\n",
    "        for token in tokenize(row[key]):\n",
    "            if token == 1:\n",
    "                break\n",
    "            counter[token] += 1\n",
    "for ix, (token, cnt) in enumerate(counter.most_common()):\n",
    "    print(ix, token, token_id2text[token], cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('jewel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c6b49",
   "metadata": {},
   "source": [
    "### Find names that contain under-represented tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_represented_token_ids = set([id_ for id_ in tokenizer_vocab.values() if counter[id_] < under_represented_threshold])\n",
    "print(len(under_represented_token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = set()\n",
    "for tup in tqdm(triplets_df.itertuples()):\n",
    "    anchor = tup.anchor\n",
    "    pos = tup.positive\n",
    "    neg = tup.negative\n",
    "    all_names.add(anchor)\n",
    "    all_names.add(pos)\n",
    "    all_names.add(neg)\n",
    "all_names.update(common_names)\n",
    "len(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e66b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_represented_names = set()\n",
    "for name in all_names:\n",
    "    found_token = False\n",
    "    for token in tokenize(name):\n",
    "        if token == 1:\n",
    "            break\n",
    "        if token in under_represented_token_ids:\n",
    "            found_token = True\n",
    "            break\n",
    "    if found_token:\n",
    "        under_represented_names.add(name)\n",
    "print(len(under_represented_names))\n",
    "for name in under_represented_names:\n",
    "    token_counts = []\n",
    "    for token in tokenize(name):\n",
    "        if token == 1:\n",
    "            break\n",
    "        token_counts.append((token, token_id2text[token], counter[token]))\n",
    "    print(name, token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add any under-represented tokens that don't start with ## as under-represented names\n",
    "for token, id_ in tokenizer_vocab.items():\n",
    "    if counter[id_] >= under_represented_threshold:\n",
    "        continue\n",
    "    if '[' in token or '#' in token:\n",
    "        continue\n",
    "    print(token)\n",
    "    under_represented_names.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(under_represented_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0b3de",
   "metadata": {},
   "source": [
    "### Add names that contain under-represented tokens to aug_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce437155",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for pos in tqdm(under_represented_names):\n",
    "    for neg in common_names[:under_represented_threshold]:\n",
    "        if pos == neg or (pos, neg) in common_non_negatives:\n",
    "            continue\n",
    "        pos_neg = f\"{pos},{neg}\"\n",
    "        if pos_neg in seen_anchor_pos:\n",
    "            continue\n",
    "        neg_pos = f\"{neg},{pos}\"\n",
    "        if neg_pos in seen_anchor_pos:\n",
    "            continue\n",
    "        aug_data.append({\n",
    "            'anchor': pos,\n",
    "            'positive': pos,\n",
    "            'negative': neg,\n",
    "            'positive_score': 1.0,\n",
    "            'negative_score': 0.0,\n",
    "        })\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5debcca1",
   "metadata": {},
   "source": [
    "## Re-Analyze training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for row in tqdm(aug_data):\n",
    "    for key in ['anchor', 'positive', 'negative']:\n",
    "        for token in tokenize(row[key]):\n",
    "            if token == 1:\n",
    "                break\n",
    "            counter[token] += 1\n",
    "for ix, (token, cnt) in enumerate(counter.most_common()):\n",
    "    print(ix, token, token_id2text[token], cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, id_ in tokenizer_vocab.items():\n",
    "    if counter[id_] == 0:\n",
    "        print(id_, token, counter[id_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('zetty')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df5a95",
   "metadata": {},
   "source": [
    "## Save augmented triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f3464",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aug_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe649b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(aug_data)\n",
    "df.to_csv(augmented_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31da69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
