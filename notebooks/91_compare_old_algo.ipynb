{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Get Statistics for the Old Clusters/Buckets\n",
    "The old clusters come from an algorithm I wrote around 15 years ago. It wasn't very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "\n",
    "from src.data.normalize import normalize_freq_names\n",
    "from src.data.utils import load_dataset\n",
    "from src.models.cluster import get_validation_results, get_names_to_cluster\n",
    "from src.models.utils import add_padding, remove_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# configure\n",
    "given_surname = \"surname\"\n",
    "vocab_size = 610000 if given_surname == \"given\" else 2100000\n",
    "n_to_cluster = 200000 if given_surname == \"given\" else 500000\n",
    "sample_size = 1000\n",
    "embed_dim = 100\n",
    "verbose = True\n",
    "n_jobs = 1\n",
    "\n",
    "Config = namedtuple(\"Config\", [\n",
    "    \"eval_path\",\n",
    "    \"freq_path\",\n",
    "    \"embed_dim\",\n",
    "])\n",
    "config = Config(\n",
    "    eval_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train.csv.gz\",\n",
    "    freq_path=f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\",\n",
    "    embed_dim=embed_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"nama\",\n",
    "    entity=\"nama\",\n",
    "    name=\"91_compare_old_algo\",\n",
    "    group=given_surname,\n",
    "    notes=\"\",\n",
    "    config=config._asdict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_eval, weighted_actual_names_eval, candidate_names_eval = load_dataset(config.eval_path, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all candidate_names_eval are in actual_names_eval\n",
    "actual_names_eval = set([name for wans in weighted_actual_names_eval for name, _, _ in wans])\n",
    "candidate_names_eval = np.array(list(actual_names_eval))\n",
    "del actual_names_eval\n",
    "print(len(candidate_names_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.read_csv(config.freq_path, na_filter=False)\n",
    "name_freq = normalize_freq_names(freq_df, is_surname=given_surname != \"given\", add_padding=True)\n",
    "freq_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "names_to_cluster = get_names_to_cluster(name_freq, n_to_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Make sure all names to cluster have clusters in the old name-cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, input_names_sample, _, weighted_actual_names_sample = \\\n",
    "    train_test_split(input_names_eval, weighted_actual_names_eval, test_size=sample_size)\n",
    "candidate_names_sample = candidate_names_eval\n",
    "\n",
    "print(\"input_names\", len(input_names_sample))\n",
    "print(\"weighted_actual_names\", len(weighted_actual_names_sample))\n",
    "print(\"candidate_names\", len(candidate_names_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the old cluster map\n",
    "with open(f\"../std_{given_surname}.txt\", \"rt\") as f:\n",
    "    lines = f.readlines()\n",
    "old_name_cluster_map = {}\n",
    "for line in lines:\n",
    "    line = line.replace(':', ' '). strip()\n",
    "    cluster = None\n",
    "    for name in line.split(' '):\n",
    "        name = name.strip()\n",
    "        if not name:\n",
    "            continue\n",
    "        if cluster is None:\n",
    "            cluster = name\n",
    "        old_name_cluster_map[add_padding(name)] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read additional name->cluster assignments\n",
    "with open(f\"../names_not_found_{given_surname}.txt\", \"rt\") as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    name, cluster = line.split(' ')\n",
    "    old_name_cluster_map[add_padding(name)] = cluster \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_name_cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many names to cluster are not in the lookup table?\n",
    "names_not_found = set()\n",
    "for name in names_to_cluster:\n",
    "    if name not in old_name_cluster_map:\n",
    "        names_not_found.add(remove_padding(name))\n",
    "for name in set(input_names_sample).union(candidate_names_sample):\n",
    "    if name not in old_name_cluster_map:\n",
    "        names_not_found.add(remove_padding(name))\n",
    "print(len(names_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out names not in the lookup table\n",
    "with open(\"new_names_not_found.txt\", \"wt\") as f:\n",
    "    for name in names_not_found:\n",
    "        f.write(name+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clusters for names to cluster\n",
    "def generate_clusters_from_old_map(names_to_cluster, verbose=False):\n",
    "    result = {}\n",
    "    for name in names_to_cluster:\n",
    "        cluster = old_name_cluster_map[name]\n",
    "        result[name] = cluster\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cluster_old = generate_clusters_from_old_map(names_to_cluster=names_to_cluster,\n",
    "                                                  verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(name_cluster_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = defaultdict(set)\n",
    "for name, cluster in name_cluster_old.items():\n",
    "    cluster_names[cluster].add(name)\n",
    "cluster_sizes_df = pd.DataFrame([len(names) for names in cluster_names.values()])\n",
    "print(\"names to cluster\", len(names_to_cluster))\n",
    "print(\"number of clusters\", len(set(name_cluster_old.values())))\n",
    "print(\"max cluster_size\", max([len(names) for names in cluster_names.values()]))\n",
    "cluster_sizes_df.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = defaultdict(int)\n",
    "cluster_names = defaultdict(list)\n",
    "for name, cluster in name_cluster_old.items():\n",
    "    cluster_counts[cluster] += name_freq.get(name, 0)\n",
    "    cluster_names[cluster].append(name)\n",
    "cluster_counts_df = pd.DataFrame.from_dict(cluster_counts, \n",
    "                                           orient='index',\n",
    "                                           columns=['counts'],\n",
    "                                          )\n",
    "cluster_counts_df.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts_df.nlargest(20, 'counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in cluster_counts_df.nlargest(20, 'counts').itertuples():\n",
    "    cluster = tup[0]\n",
    "    count = tup[1]\n",
    "    print(cluster, count, len(cluster_names[cluster]), cluster_names[cluster])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we've added all names to the lookup table\n",
    "names_to_cluster_old = list(set(names_to_cluster).union(set(input_names_sample)).union(set(candidate_names_sample)))\n",
    "name_cluster_old = generate_clusters_from_old_map(\n",
    "             names_to_cluster=names_to_cluster_old,\n",
    "             verbose=verbose)\n",
    "print(len(name_cluster_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_validation_results(input_names_eval=input_names_sample,\n",
    "                              weighted_actual_names_eval=weighted_actual_names_sample,\n",
    "                              candidate_names_eval=candidate_names_sample,\n",
    "                              name_cluster=name_cluster_old,\n",
    "                              name_freq=None,\n",
    "                              swivel_model=None,\n",
    "                              swivel_vocab=None,\n",
    "                              tfidf_vectorizer=None,\n",
    "                              ensemble_model=None,\n",
    "                              num_matches=None,\n",
    "                              max_clusters=None,\n",
    "                              search_threshold=0.5,\n",
    "                              lookup_mode=True,\n",
    "                              sample_size=sample_size,\n",
    "                              validation_sizes=[0],\n",
    "                              n_jobs=n_jobs,\n",
    "                              verbose=verbose)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
