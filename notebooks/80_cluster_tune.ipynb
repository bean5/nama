{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.integration.wandb import WandbLoggerCallback\n",
    "import torch\n",
    "\n",
    "from src.data.utils import load_datasets, select_frequent_k\n",
    "from src.data.filesystem import fopen\n",
    "from src.eval.metrics import (\n",
    "    avg_precision_at_threshold, \n",
    "    avg_weighted_recall_at_threshold,\n",
    "    precision_weighted_recall_curve_at_threshold,\n",
    ")\n",
    "from src.models.cluster import (\n",
    "    get_sorted_similarities,\n",
    "    generate_closures,\n",
    "    generate_clusters,\n",
    "    get_clusters,\n",
    "    get_best_cluster_matches,\n",
    ")\n",
    "from src.models.swivel import SwivelModel, get_swivel_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# configure\n",
    "wandb_api_key_file = \"../.wandb-api-key\"\n",
    "given_surname = \"surname\"\n",
    "vocab_size = 600000 if given_surname == \"given\" else 2100000\n",
    "embed_dim = 100\n",
    "\n",
    "DEFAULT_NAMES_TO_CLUSTER = vocab_size\n",
    "DEFAULT_CLOSURE_THRESHOLD = 5000\n",
    "DEFAULT_SEARCH_THRESHOLD = 0.55\n",
    "DEFAULT_ALGO = \"agglomerative\"\n",
    "# agglomerative options\n",
    "DEFAULT_CLUSTER_THRESHOLD = 0.7\n",
    "DEFAULT_CLUSTER_LINKAGE = \"complete\"\n",
    "# optics and hdbscan options\n",
    "DEFAULT_MIN_SAMPLES = 2\n",
    "DEFAULT_EPS = 0.2\n",
    "# optics options\n",
    "DEFAULT_MAX_EPS = 1.0\n",
    "DEFAULT_XI = 0.15\n",
    "# hdbscan options\n",
    "DEFAULT_SELECTION_METHOD = \"eom\"\n",
    "DEFAULT_MIN_CLUSTER_SIZE = 2\n",
    "\n",
    "Config = namedtuple(\"Config\", \"train_path embed_dim swivel_vocab_path swivel_model_path\")\n",
    "config = Config(\n",
    "    train_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train.csv.gz\",\n",
    "    embed_dim=embed_dim,\n",
    "    swivel_vocab_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-vocab-{vocab_size}.csv\",\n",
    "    swivel_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-model-{vocab_size}-{embed_dim}.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[train] = load_datasets([config.train_path])\n",
    "input_names_train, weighted_actual_names_train, candidate_names_train = train\n",
    "\n",
    "vocab_df = pd.read_csv(fopen(config.swivel_vocab_path, \"rb\"))\n",
    "swivel_vocab = {name: _id for name, _id in zip(vocab_df[\"name\"], vocab_df[\"index\"])}\n",
    "\n",
    "swivel_model = SwivelModel(len(swivel_vocab), config.embed_dim)\n",
    "swivel_model.load_state_dict(torch.load(fopen(config.swivel_model_path, \"rb\"), map_location=torch.device(device)))\n",
    "swivel_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ray_training_function(config,\n",
    "                          swivel_model,\n",
    "                          swivel_vocab,\n",
    "                          input_names_train,\n",
    "                          weighted_actual_names_train,\n",
    "                          candidate_names_train,\n",
    "                          checkpoint_dir=None):\n",
    "\n",
    "    # filter names to cluster from train\n",
    "    input_names_cluster, weighted_actual_names_cluster, candidate_names_cluster = \\\n",
    "        select_frequent_k(input_names_train, \n",
    "                          weighted_actual_names_train, \n",
    "                          candidate_names_train,\n",
    "                          config[\"n_to_cluster\"])\n",
    "\n",
    "    # validate on names to cluster\n",
    "    input_names_validate = input_names_cluster\n",
    "    weighted_actual_names_validate = weighted_actual_names_cluster\n",
    "    candidate_names_validate = candidate_names_cluster\n",
    "    input_names_train = weighted_actual_names_train = candidate_names_train = None  # release memory\n",
    "    \n",
    "    # get names to cluster\n",
    "    cluster_names = list(set(input_names_cluster).union(set(candidate_names_cluster)))\n",
    "    input_names_cluster = candidate_names_cluster = None  # release memory\n",
    "\n",
    "    # get embeddings for names to cluster\n",
    "    cluster_embeddings = get_swivel_embeddings(swivel_model, swivel_vocab, cluster_names).astype('float32')\n",
    "    cluster_names = None  # release memory\n",
    "\n",
    "    # get sorted_similarities from embeddings\n",
    "    sorted_similarities = get_sorted_similarities(cluster_embeddings, threshold=0.4)\n",
    "\n",
    "    # generate closures from sorted similarities\n",
    "    _, closure2ids, _, max_score_not_merged = generate_closures(sorted_similarities, config[\"closure_threshold\"])\n",
    "    sorted_similarities = None  # release memory\n",
    "\n",
    "    # generate clusters from closures and embeddings\n",
    "    id2cluster = generate_clusters(closure2ids,\n",
    "                                   cluster_embeddings,\n",
    "                                   cluster_algo=config[\"cluster_algo\"],\n",
    "                                   # agglomerative options\n",
    "                                   cluster_linkage=config[\"cluster_linkage\"],\n",
    "                                   cluster_threshold=config[\"cluster_threshold\"],\n",
    "                                   # optics or hdbscan options\n",
    "                                   min_samples=config[\"min_samples\"],\n",
    "                                   eps=config[\"eps\"],\n",
    "                                   # optics options\n",
    "                                   cluster_method=config[\"cluster_method\"],\n",
    "                                   max_eps=config[\"max_eps\"],\n",
    "                                   xi=config[\"xi\"],\n",
    "                                   # hdbscan options\n",
    "                                   selection_method=config[\"selection_method\"],\n",
    "                                   min_cluster_size=config[\"min_cluster_size\"],\n",
    "                                   # other options\n",
    "                                   n_jobs=1,\n",
    "                                   verbose=False,\n",
    "                                  )\n",
    "    closure2ids = None  # release memory\n",
    "\n",
    "    # get validate names and embeddings\n",
    "    validate_names = list(set(input_names_validate).union(set(candidate_names_validate)))\n",
    "    validate_embeddings = get_swivel_embeddings(swivel_model, swivel_vocab, validate_names).astype('float32')\n",
    "    candidate_names_validate = swivel_model = swivel_vocab = None  # release memory\n",
    "\n",
    "    # assign all names to clusters\n",
    "    name2clusters, cluster2names = get_clusters(validate_names,\n",
    "                                                validate_embeddings,\n",
    "                                                id2cluster,\n",
    "                                                cluster_embeddings,\n",
    "                                                k=100,\n",
    "                                                max_clusters=5,\n",
    "                                                verbose=False,\n",
    "                                               )\n",
    "    all_names = all_embeddings = id2cluster = cluster_embeddings = None  # release memory\n",
    "\n",
    "    num_clusters = len(cluster2names)\n",
    "    max_cluster_size = max([len(names) for names in cluster2names.values()])\n",
    "    \n",
    "#     print(\"max_score_not_merged\", max_score_not_merged)\n",
    "#     print(\"num_clusters\", num_clusters)\n",
    "#     print(\"max_cluster_size\", max_cluster_size)\n",
    "\n",
    "    # get best matches\n",
    "    best_matches = get_best_cluster_matches(name2clusters, cluster2names, input_names_validate)\n",
    "    name2clusters = cluster2names = input_names_validate = None  # release memory\n",
    "\n",
    "    # eval f1\n",
    "    precision = avg_precision_at_threshold(weighted_actual_names_validate, best_matches, config[\"search_threshold\"])\n",
    "    recall = avg_weighted_recall_at_threshold(weighted_actual_names_validate, best_matches, config[\"search_threshold\"])\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # Report the metrics to Ray\n",
    "    tune.report(f1=f1,\n",
    "                precision=precision, \n",
    "                recall=recall,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_params={\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": DEFAULT_NAMES_TO_CLUSTER,  # tune.qrandint(100000, 500000, 100000),\n",
    "    \"closure_threshold\": DEFAULT_CLOSURE_THRESHOLD,\n",
    "    \"search_threshold\": DEFAULT_SEARCH_THRESHOLD,\n",
    "    \"cluster_threshold\": tune.grid_search([0.95, 0.9, 0.85, 0.8]),\n",
    "    \"cluster_linkage\": DEFAULT_CLUSTER_LINKAGE,  # tune.choice([\"average\", \"single\", \"complete\", \"ward\"]),\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,  # tune.choice([\"eom\", \"leaf\"]),\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,\n",
    "}\n",
    "\n",
    "current_best_params = [{\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": DEFAULT_NAMES_TO_CLUSTER,\n",
    "    \"closure_threshold\": DEFAULT_CLOSURE_THRESHOLD,\n",
    "    \"search_threshold\": DEFAULT_SEARCH_THRESHOLD,\n",
    "    \"cluster_threshold\": DEFAULT_CLUSTER_THRESHOLD,\n",
    "    \"cluster_linkage\": DEFAULT_CLUSTER_LINKAGE,\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI,\n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-hyperopt\n",
    "search_alg = HyperOptSearch(points_to_evaluate=current_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "callbacks = []\n",
    "if wandb_api_key_file:\n",
    "    callbacks.append(WandbLoggerCallback(\n",
    "        project=\"nama\",\n",
    "        entity=\"nama\",\n",
    "        group=\"80_cluster_tune_\"+given_surname+\"_600\",\n",
    "        notes=\"\",\n",
    "        config=config._asdict(),\n",
    "        api_key_file=wandb_api_key_file\n",
    "    ))\n",
    "\n",
    "result = tune.run(\n",
    "    tune.with_parameters(ray_training_function,\n",
    "                         swivel_model=swivel_model,\n",
    "                         swivel_vocab=swivel_vocab,\n",
    "                         input_names_train=input_names_train,\n",
    "                         weighted_actual_names_train=weighted_actual_names_train,\n",
    "                         candidate_names_train=candidate_names_train),\n",
    "    resources_per_trial={\"cpu\": 2.0, \"gpu\": 0.0},\n",
    "    max_concurrent_trials=4,\n",
    "    config=config_params,\n",
    "#     search_alg=search_alg,\n",
    "#     num_samples=6,\n",
    "#     metric=\"f1\",\n",
    "#     mode=\"max\",\n",
    "#     checkpoint_score_attr=\"f1\",\n",
    "#     time_budget_s=4*3600,\n",
    "    progress_reporter=tune.JupyterNotebookReporter(\n",
    "        overwrite=False,\n",
    "        max_report_frequency=5*60\n",
    "    ),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get trial that has the highest F1\n",
    "best_trial = result.get_best_trial(metric='f1', mode='max', scope='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters with the highest F1\n",
    "best_trial.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Best trial final train f1: {best_trial.last_result['f1']}\")\n",
    "print(f\"Best trial final train precision: {best_trial.last_result['precision']}\")\n",
    "print(f\"Best trial final train recall: {best_trial.last_result['recall']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get all trials as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# All trials as pandas dataframe\n",
    "df = result.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = [\"<john>\", \"<jonathan>\", \"<mary>\", \"<marie>\", \"<maria>\", \"<george>\"]\n",
    "closure2ids = {\"c\": [0,1,2,3,4,5]}\n",
    "cluster_embeddings = get_swivel_embeddings(swivel_model, swivel_vocab, cluster_names).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id2cluster = generate_clusters(closure2ids, cluster_embeddings, 0.15, \"average\", n_jobs=1)\n",
    "print(id2cluster)\n",
    "id2cluster = generate_clusters(closure2ids, cluster_embeddings, 0.99, \"average\", n_jobs=1)\n",
    "print(id2cluster)\n",
    "id2cluster = generate_clusters(closure2ids, cluster_embeddings, 0.01, \"average\", n_jobs=1)\n",
    "print(id2cluster)\n",
    "id2cluster = generate_clusters(closure2ids, cluster_embeddings, 0.01, \"ward\", n_jobs=1)\n",
    "print(id2cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "cluster_embeddings = normalize(cluster_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "\n",
    "min_samples=2\n",
    "max_eps=0.7\n",
    "xi=0.05   # 0.01..0.20, 0.01\n",
    "metric=\"cosine\"\n",
    "eps=0.45  # 0.45..0.70, 0.05\n",
    "\n",
    "\n",
    "clust = OPTICS(min_samples=min_samples, \n",
    "               xi=xi, \n",
    "               max_eps=max_eps,\n",
    "               metric=metric,\n",
    "              )\n",
    "clust.fit(cluster_embeddings)\n",
    "\n",
    "labels = cluster_optics_dbscan(\n",
    "    reachability=clust.reachability_,\n",
    "    core_distances=clust.core_distances_,\n",
    "    ordering=clust.ordering_,\n",
    "    eps=eps,\n",
    ")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "min_samples=2\n",
    "eps=0.0\n",
    "selection_method=\"leaf\"\n",
    "min_cluster_size=2\n",
    "\n",
    "clust = hdbscan.HDBSCAN(min_samples=min_samples,\n",
    "                        cluster_selection_epsilon=eps,\n",
    "                        cluster_selection_method=selection_method,\n",
    "                        min_cluster_size=min_cluster_size,\n",
    "                        metric=\"euclidean\",\n",
    "                        )\n",
    "clust.fit(cluster_embeddings)\n",
    "\n",
    "clust.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cluster = max(clust.labels_)\n",
    "max_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,1,0,1,-1,-1,0,1,-1]\n",
    "max_cluster = max(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for label in labels:\n",
    "    if label < 0:\n",
    "        max_cluster += 1\n",
    "        label = max_cluster\n",
    "    results.append(label)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
