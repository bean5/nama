{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter searches for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.integration.wandb import WandbLoggerCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "from src.data.normalize import normalize_freq_names\n",
    "from src.data.utils import load_dataset, select_frequent_k\n",
    "from src.data.filesystem import fopen\n",
    "from src.eval.metrics import (\n",
    "    avg_precision_at_threshold, \n",
    "    avg_weighted_recall_at_threshold,\n",
    "    precision_weighted_recall_curve_at_threshold,\n",
    ")\n",
    "from src.models.cluster import (\n",
    "    get_sorted_similarities,\n",
    "    generate_closures,\n",
    "    generate_clusters,\n",
    "    get_clusters,\n",
    "    get_best_cluster_matches,\n",
    ")\n",
    "from src.models.ensemble import get_best_ensemble_matches\n",
    "from src.models.swivel import SwivelModel, get_swivel_embeddings\n",
    "from src.models.swivel_encoder import SwivelEncoderModel\n",
    "from src.models.utils import add_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative\n",
    "#### min, max, avg, avg w extra entries for frequent names? x5\n",
    "#### How many names to cluster (50k, 100k, 150k, 200k, max)? x5\n",
    "### Consider other algos\n",
    "##### at this point we've decided how many names to cluster and whether to add extra entries for frequent names\n",
    "#### Optics vs HDBscan? x2\n",
    "#### Various parameters x8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate clusters\n",
    "### How to choose a metric for how far away a cluster is from a name?\n",
    "#### min, max, avg distance?\n",
    "#### if avg, weight frequent names more in the average calculation?\n",
    "## How to create super-clusters?\n",
    "### Can we cluster the clusters?\n",
    "### If we re-cluster the names, what happens if a super-cluster splits a cluster?\n",
    "#### maybe that doesn't happen very often, and is it always bad?\n",
    "## Fallback\n",
    "#### use nysiis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# configure\n",
    "wandb_api_key_file = \"../.wandb-api-key\"\n",
    "given_surname = \"surname\"\n",
    "vocab_size = 610000 if given_surname == \"given\" else 2100000\n",
    "embed_dim = 100\n",
    "encoder_layers = 2\n",
    "num_matches = 5000\n",
    "batch_size = 256\n",
    "\n",
    "DEFAULT_NAMES_TO_CLUSTER = 50000  # TODO tune\n",
    "DEFAULT_SEARCH_THRESHOLD = 0.0\n",
    "DEFAULT_REPEAT_FREQ_NAMES = False\n",
    "DEFAULT_ALGO = \"agglomerative\"\n",
    "# agglomerative options\n",
    "DEFAULT_CLUSTER_THRESHOLD = 0.3\n",
    "DEFAULT_CLUSTER_LINKAGE = \"average\"\n",
    "# optics and hdbscan options\n",
    "DEFAULT_MIN_SAMPLES = 2\n",
    "DEFAULT_EPS = 0.2\n",
    "# optics options\n",
    "DEFAULT_MAX_EPS = 1.0\n",
    "DEFAULT_XI = 0.15\n",
    "# hdbscan options\n",
    "DEFAULT_SELECTION_METHOD = \"eom\"\n",
    "DEFAULT_MIN_CLUSTER_SIZE = 2\n",
    "\n",
    "MAX_NAMES_TO_CLUSTER = 200000\n",
    "\n",
    "Config = namedtuple(\"Config\", [ \n",
    "    \"eval_path\",\n",
    "    \"freq_path\",\n",
    "    \"embed_dim\",\n",
    "    \"swivel_vocab_path\",\n",
    "    \"swivel_model_path\",\n",
    "    \"tfidf_path\",\n",
    "    \"ensemble_model_path\"\n",
    "])\n",
    "config = Config(\n",
    "    eval_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train.csv.gz\",\n",
    "    freq_path=f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\",\n",
    "    embed_dim=embed_dim,\n",
    "    swivel_vocab_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-vocab-{vocab_size}-augmented.csv\",\n",
    "    swivel_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-model-{vocab_size}-{embed_dim}-augmented.pth\",\n",
    "    tfidf_path=f\"s3://nama-data/data/models/fs-{given_surname}-tfidf.joblib\",\n",
    "    ensemble_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-ensemble-model-{vocab_size}-{embed_dim}-augmented-40-40-25.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"  # force CPU because we want to run multiple trials in parallel\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_eval, weighted_actual_names_eval, candidate_names_eval = load_dataset(config.eval_path, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.read_csv(config.freq_path, na_filter=False)\n",
    "name_freq = normalize_freq_names(freq_df, is_surname=given_surname != \"given\", add_padding=True)\n",
    "freq_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_df = pd.read_csv(fopen(config.swivel_vocab_path, \"rb\"))\n",
    "swivel_vocab = {name: _id for name, _id in zip(vocab_df[\"name\"], vocab_df[\"index\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swivel_model = SwivelModel(len(swivel_vocab), config.embed_dim)\n",
    "swivel_model.load_state_dict(torch.load(fopen(config.swivel_model_path+\".40\", \"rb\"), map_location=torch.device(device)))\n",
    "swivel_model.to(device)\n",
    "swivel_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = joblib.load(fopen(config.tfidf_path, mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = joblib.load(fopen(config.ensemble_model_path, mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(config,\n",
    "               swivel_model,\n",
    "               swivel_vocab,\n",
    "               tfidf_vectorizer,\n",
    "               ensemble_model,\n",
    "               name_freq,\n",
    "               input_names_eval,\n",
    "               weighted_actual_names_eval,\n",
    "               candidate_names_eval,\n",
    "               n_jobs=1,\n",
    "               verbose=False):\n",
    "    \n",
    "    # get names to score\n",
    "    names_to_score = np.array(list(name_freq.keys())[:config[\"n_to_cluster\"]])\n",
    "\n",
    "    # get ensemble scores\n",
    "    if verbose:\n",
    "        print(\"get ensemble scores\", datetime.now(), len(names_to_score))\n",
    "    similar_names_scores = get_best_ensemble_matches(\n",
    "        model=swivel_model,\n",
    "        vocab=swivel_vocab,\n",
    "        input_names=names_to_score,\n",
    "        candidate_names=names_to_score,\n",
    "        tfidf_vectorizer=tfidf_vectorizer,\n",
    "        ensemble_model=ensemble_model,\n",
    "        name_freq=name_freq,\n",
    "        k=num_matches,\n",
    "        batch_size=batch_size,\n",
    "        add_context=True,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # repeat frequent names?\n",
    "    if config[\"repeat_freq_names\"]:\n",
    "        names_to_cluster = []\n",
    "        for name in names_to_score:\n",
    "            freq = name_freq[name]\n",
    "            for ix in range(0, math.ceil(math.log10(freq+1))):\n",
    "                names_to_cluster.append(name)\n",
    "        names_to_cluster = np.array(names_to_cluster)\n",
    "    else:\n",
    "        names_to_cluster = names_to_score\n",
    "\n",
    "    if len(names_to_cluster) > MAX_NAMES_TO_CLUSTER:\n",
    "        return {\n",
    "            'error': 'too large',\n",
    "        }  \n",
    "        \n",
    "    # create name to index dictionary\n",
    "    name_indices = defaultdict(list)\n",
    "    for ix, name in enumerate(names_to_cluster):\n",
    "        name_indices[name].append(ix)\n",
    "    \n",
    "    # create distances array\n",
    "    # names are initially 2.0 apart; similar names are 1.0 - score apart\n",
    "    if verbose:\n",
    "        print(\"create distances array\", datetime.now(), len(names_to_cluster))\n",
    "    distances = np.full((len(names_to_cluster), len(names_to_cluster)), 2.0, dtype=np.float32)\n",
    "    for name1, names_scores in zip(names_to_score, similar_names_scores):\n",
    "        name1_ixs = name_indices[name1]\n",
    "        for name2, score in names_scores:\n",
    "                name2_ixs = name_indices[name2]\n",
    "                for name1_ix in name1_ixs:\n",
    "                    for name2_ix in name2_ixs:\n",
    "                        distances[name1_ix, name2_ix] = 1.0 - score\n",
    "                        distances[name2_ix, name1_ix] = 1.0 - score\n",
    "    del similar_names_scores\n",
    "    \n",
    "    # generate clusters from distances\n",
    "    if verbose:\n",
    "        print(\"generate clusters\", datetime.now())\n",
    "    clusters = generate_clusters(distances,\n",
    "                                 cluster_algo=config[\"cluster_algo\"],\n",
    "                                 # agglomerative options\n",
    "                                 cluster_linkage=config[\"cluster_linkage\"],\n",
    "                                 cluster_threshold=config[\"cluster_threshold\"],\n",
    "                                 # optics or hdbscan options\n",
    "                                 min_samples=config[\"min_samples\"],\n",
    "                                 eps=config[\"eps\"],\n",
    "                                 # optics options\n",
    "                                 cluster_method=config[\"cluster_method\"],\n",
    "                                 max_eps=config[\"max_eps\"],\n",
    "                                 xi=config[\"xi\"],\n",
    "                                 # hdbscan options\n",
    "                                 selection_method=config[\"selection_method\"],\n",
    "                                 min_cluster_size=config[\"min_cluster_size\"],\n",
    "                                 # other options\n",
    "                                 n_jobs=n_jobs,\n",
    "                                 verbose=False,\n",
    "                                )\n",
    "    del distances\n",
    "\n",
    "    # generate cluster->names and name->cluster\n",
    "    cluster_names = defaultdict(list)\n",
    "    name_cluster = {}\n",
    "    max_cluster_size = 0\n",
    "    max_cluster_id = None\n",
    "    for _id, cluster in enumerate(clusters):\n",
    "        clustered_name = names_to_cluster[_id]\n",
    "        cluster_names[cluster].append(clustered_name)\n",
    "        if len(cluster_names[cluster]) > max_cluster_size:\n",
    "            max_cluster_size = len(cluster_names[cluster])\n",
    "            max_cluster_id = cluster\n",
    "        name_cluster[clustered_name] = cluster\n",
    "    if verbose:\n",
    "        print(\"number of clusters\", datetime.now(), len(cluster_names))\n",
    "        print(\"max cluster size\", datetime.now(), max_cluster_size)\n",
    "        print(\"max cluster\", cluster_names[max_cluster_id])\n",
    "        cluster_sizes_df = pd.DataFrame([len(names) for names in cluster_names.values()])\n",
    "        cluster_sizes_df.hist(bins=100)\n",
    "    \n",
    "    # validate on validation sets of various sizes\n",
    "    validation_sizes = [25000, 50000, 100000, 200000, 0]\n",
    "    sample_size = 20000\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    f2s = []\n",
    "    for size in validation_sizes:\n",
    "        if verbose:\n",
    "            print(\"validate\", datetime.now(), size)\n",
    "        if size == 0:\n",
    "            input_names_validate, weighted_actual_names_validate, candidate_names_validate = \\\n",
    "                input_names_eval, weighted_actual_names_eval, candidate_names_eval\n",
    "        else:\n",
    "            input_names_validate, weighted_actual_names_validate, candidate_names_validate = \\\n",
    "                select_frequent_k(input_names_eval, \n",
    "                                  weighted_actual_names_eval, \n",
    "                                  candidate_names_eval,\n",
    "                                  size)\n",
    "\n",
    "        # sample the validation set\n",
    "        if len(input_names_validate) > sample_size:\n",
    "            _, input_names_validate, _, weighted_actual_names_validate = \\\n",
    "                train_test_split(input_names_validate, weighted_actual_names_validate, test_size=sample_size)\n",
    "            # filter the candidate names to just those in weighted actual names\n",
    "            # however, we don't do this in notebook 90, so don't do it here either so the numbers a fair comparison\n",
    "#             candidate_names_validate = np.array(list(set(\n",
    "#                 name for wans in weighted_actual_names_validate for name, _, _ in wans)))\n",
    "        \n",
    "        # get validate names\n",
    "        all_names_validate = list(set(input_names_validate).union(set(candidate_names_validate)))\n",
    "        \n",
    "        # assign all names to clusters\n",
    "        if verbose:\n",
    "            print(\"get_clusters\", datetime.now(), len(all_names_validate))\n",
    "        name2clusters, cluster2names = get_clusters(all_names_validate,\n",
    "                                                    name_cluster,\n",
    "                                                    swivel_model,\n",
    "                                                    swivel_vocab,\n",
    "                                                    tfidf_vectorizer,\n",
    "                                                    ensemble_model,\n",
    "                                                    name_freq,\n",
    "                                                    max_clusters=1,\n",
    "                                                    n_jobs=n_jobs,\n",
    "                                                    verbose=False,\n",
    "                                                   )\n",
    "\n",
    "#         print(\"name2clusters\", len(name2clusters),\n",
    "#               min(len(clusters) for clusters in name2clusters.values()),\n",
    "#               max(len(clusters) for clusters in name2clusters.values()))\n",
    "#         print(\"cluster2names\", len(cluster2names), \\\n",
    "#               min(len(names) for names in cluster2names.values()),\n",
    "#               max(len(names) for names in cluster2names.values()))\n",
    "#         print(\"maria cluster\", name2clusters[\"<maria>\"])\n",
    "#         print(\"maria cluster names\", cluster2names[name2clusters[\"<maria>\"][0][0]])\n",
    "#         for input_name, wans in zip(input_names_validate, weighted_actual_names_validate):\n",
    "#             if input_name != \"<maria>\":\n",
    "#                 continue\n",
    "#             print(\"maria weighted actual names\", wans)\n",
    "        \n",
    "        # get best matches\n",
    "        if verbose:\n",
    "            print(\"get_best_cluster_matches\", datetime.now())\n",
    "        best_matches = get_best_cluster_matches(name2clusters, cluster2names, \n",
    "                                                input_names_validate)\n",
    "\n",
    "        # eval f1\n",
    "        precision = avg_precision_at_threshold(weighted_actual_names_validate, best_matches, config[\"search_threshold\"])\n",
    "        recall = avg_weighted_recall_at_threshold(weighted_actual_names_validate, best_matches, config[\"search_threshold\"])\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        f2 = 5 * (precision * recall) / (4 * precision + recall)\n",
    "        if verbose:\n",
    "            print(\"result\", datetime.now(), \"precision\", precision, \"recall\", recall, \"f1\", f1, \"f2\", f2)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        f2s.append(f2)\n",
    "        \n",
    "    f1 = (sum(f1s) / len(f1s)) if len(f1s) > 0 else 0\n",
    "    f2 = (sum(f2s) / len(f2s)) if len(f2s) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'f2': f2,\n",
    "        'f1s': f1s,\n",
    "        'f2s': f2s,\n",
    "        'precisions': precisions,\n",
    "        'recalls': recalls,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ray_training_function(config,\n",
    "                          swivel_model,\n",
    "                          swivel_vocab,\n",
    "                          tfidf_vectorizer,\n",
    "                          ensemble_model,\n",
    "                          name_freq,\n",
    "                          input_names_eval,\n",
    "                          weighted_actual_names_eval,\n",
    "                          candidate_names_eval,\n",
    "                          n_jobs=1,\n",
    "                          verbose=False):\n",
    "\n",
    "    result = train_eval(config,\n",
    "                        swivel_model,\n",
    "                        swivel_vocab,\n",
    "                        tfidf_vectorizer,\n",
    "                        ensemble_model,\n",
    "                        name_freq,\n",
    "                        input_names_eval,\n",
    "                        weighted_actual_names_eval,\n",
    "                        candidate_names_eval,\n",
    "                        n_jobs=n_jobs,\n",
    "                        verbose=verbose)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        # Report the metrics to Ray\n",
    "        tune.report(f1=result['f1'],\n",
    "                    f2=result['f2'],\n",
    "                    f1s=result['f1s'],\n",
    "                    f2s=result['f2s'],\n",
    "                    precisions=result['precisions'], \n",
    "                    recalls=result['recalls'],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_params={\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": tune.qrandint(100000, 200000, 50000),\n",
    "    \"search_threshold\": 0.0,  # tune.quniform(0.0, 0.6, 0.1),\n",
    "    \"repeat_freq_names\": False,  # tune.choice([True, False]),\n",
    "    \"cluster_threshold\": tune.quniform(-0.98, -0.78, 0.05),\n",
    "    \"cluster_linkage\": \"average\",  # tune.choice([\"average\", \"single\", \"complete\"]),\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,  # tune.choice([\"eom\", \"leaf\"]),\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,\n",
    "}\n",
    "\n",
    "current_best_params = [{\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": DEFAULT_NAMES_TO_CLUSTER,\n",
    "    \"search_threshold\": DEFAULT_SEARCH_THRESHOLD,\n",
    "    \"repeat_freq_names\": DEFAULT_REPEAT_FREQ_NAMES,\n",
    "    \"cluster_threshold\": DEFAULT_CLUSTER_THRESHOLD,\n",
    "    \"cluster_linkage\": DEFAULT_CLUSTER_LINKAGE,\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI,\n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-hyperopt\n",
    "\n",
    "# search_alg = HyperOptSearch(points_to_evaluate=current_best_params)\n",
    "\n",
    "# ray.shutdown()\n",
    "# ray.init()\n",
    "\n",
    "# callbacks = []\n",
    "# if wandb_api_key_file:\n",
    "#     callbacks.append(WandbLoggerCallback(\n",
    "#         project=\"nama\",\n",
    "#         entity=\"nama\",\n",
    "#         group=\"80_cluster_tune_\"+given_surname+\"_agglomerative\",\n",
    "#         notes=\"\",\n",
    "#         config=config._asdict(),\n",
    "#         api_key_file=wandb_api_key_file\n",
    "#     ))\n",
    "\n",
    "# result = tune.run(\n",
    "#     tune.with_parameters(ray_training_function,\n",
    "#                          swivel_model=swivel_model,\n",
    "#                          swivel_vocab=swivel_vocab,\n",
    "#                          tfidf_vectorizer=tfidf_vectorizer,\n",
    "#                          ensemble_model=ensemble_model,\n",
    "#                          name_freq=name_freq,\n",
    "#                          input_names_eval=input_names_eval,\n",
    "#                          weighted_actual_names_eval=weighted_actual_names_eval,\n",
    "#                          candidate_names_eval=candidate_names_eval),\n",
    "#     resources_per_trial={\"cpu\": 8.0, \"gpu\": 0.0},\n",
    "#     max_concurrent_trials=1,\n",
    "#     config=config_params,\n",
    "#     search_alg=search_alg,\n",
    "#     num_samples=100,\n",
    "#     metric=\"f2\",\n",
    "#     mode=\"max\",\n",
    "#     checkpoint_score_attr=\"f2\",\n",
    "#     time_budget_s=46*3600,\n",
    "#     progress_reporter=tune.JupyterNotebookReporter(\n",
    "#         overwrite=False,\n",
    "#         max_report_frequency=5*60\n",
    "#     ),\n",
    "#     callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Get trial that has the highest F1\n",
    "# best_trial = result.get_best_trial(metric='f2', mode='max', scope='all')\n",
    "\n",
    "# # Parameters with the highest F1\n",
    "# best_trial.config\n",
    "\n",
    "# print(f\"Best trial final train f2: {best_trial.last_result['f2']}\")\n",
    "# print(f\"Best trial final train precision: {best_trial.last_result['precision']}\")\n",
    "# print(f\"Best trial final train recall: {best_trial.last_result['recall']}\")\n",
    "\n",
    "# # All trials as pandas dataframe\n",
    "# df = result.results_df\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_objective_function(swivel_model,\n",
    "                                swivel_vocab,\n",
    "                                tfidf_vectorizer,\n",
    "                                ensemble_model,\n",
    "                                name_freq,\n",
    "                                input_names_eval,\n",
    "                                weighted_actual_names_eval,\n",
    "                                candidate_names_eval,\n",
    "                                n_jobs=1,\n",
    "                                verbose=False):\n",
    "    def objective(config):\n",
    "        config['n_to_cluster'] = int(config['n_to_cluster'])\n",
    "        if verbose:\n",
    "            print(\"config\", datetime.now(), config)\n",
    "        result = train_eval(config,\n",
    "                            swivel_model,\n",
    "                            swivel_vocab,\n",
    "                            tfidf_vectorizer,\n",
    "                            ensemble_model,\n",
    "                            name_freq,\n",
    "                            input_names_eval,\n",
    "                            weighted_actual_names_eval,\n",
    "                            candidate_names_eval,\n",
    "                            n_jobs=n_jobs,\n",
    "                            verbose=verbose)\n",
    "        if verbose:\n",
    "            print(\"result\", datetime.now(), result)\n",
    "\n",
    "        if 'error' in result:\n",
    "            return {\n",
    "                'status': STATUS_FAIL\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'status': STATUS_OK,\n",
    "                'loss': 1.0 - result['f2'],\n",
    "                'config': config,\n",
    "                'f1': result['f1'],\n",
    "                'f2': result['f2'],\n",
    "                'f1s': result['f1s'],\n",
    "                'f2s': result['f2s'],\n",
    "                'precisions': result['precisions'],\n",
    "                'recalls': result['recalls'],\n",
    "            }\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 64\n",
    "\n",
    "# HyperOpt search space\n",
    "space = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,  # hp.quniform('n_to_cluster', 50000, 200000, 50000),\n",
    "    \"search_threshold\": 0.0,  # hp.quniform('search_threshold', 0.0, 0.1, 0.1),\n",
    "    \"repeat_freq_names\": False,  # hp.choice('repeat_freq_names', [True, False]),\n",
    "    \"cluster_threshold\": 0.0,  # hp.quniform('cluster_threshold', -0.5, 0.1, 0.2),\n",
    "    \"cluster_linkage\": \"average\", # hp.choice('cluster_linkage', [\"average\", \"single\"]), \n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,  # tune.choice([\"eom\", \"leaf\"]),\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "objective = hyperopt_objective_function(swivel_model,\n",
    "                                        swivel_vocab,\n",
    "                                        tfidf_vectorizer,\n",
    "                                        ensemble_model,\n",
    "                                        name_freq,\n",
    "                                        input_names_eval,\n",
    "                                        weighted_actual_names_eval,\n",
    "                                        candidate_names_eval,\n",
    "                                        n_jobs=n_jobs,\n",
    "                                        verbose=True)\n",
    "trials = Trials()\n",
    "\n",
    "# minimize the objective over the space\n",
    "best = fmin(objective, \n",
    "            space, \n",
    "            algo=tpe.suggest, \n",
    "            trials=trials,\n",
    "            max_evals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best\", best)\n",
    "print(\"results\", trials.results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 1  # TODO 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 50000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.98,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 150000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": 0.0,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": 0.0,\n",
    "    \"cluster_linkage\": \"single\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": 0.0,\n",
    "    \"cluster_linkage\": \"complete\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 50000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": 0.0,\n",
    "    \"cluster_linkage\": \"complete\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.2,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.4,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.6,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": 0.1,\n",
    "    \"cluster_linkage\": \"single\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": 0.2,\n",
    "    \"cluster_linkage\": \"single\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.8,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.9,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 200000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.9,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.95,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.98,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.98,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"cluster_algo\": DEFAULT_ALGO,\n",
    "    \"n_to_cluster\": 100000,\n",
    "    \"search_threshold\": 0.0,\n",
    "    \"repeat_freq_names\": False,\n",
    "    \"cluster_threshold\": -0.99,\n",
    "    \"cluster_linkage\": \"average\",\n",
    "    \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "    \"eps\": DEFAULT_EPS,\n",
    "    \"max_eps\": DEFAULT_MAX_EPS,\n",
    "    \"cluster_method\": \"dbscan\",\n",
    "    \"xi\": DEFAULT_XI, \n",
    "    \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "    \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "}\n",
    "result = train_eval(config,\n",
    "                    swivel_model,\n",
    "                    swivel_vocab,\n",
    "                    tfidf_vectorizer,\n",
    "                    ensemble_model,\n",
    "                    name_freq,\n",
    "                    input_names_eval,\n",
    "                    weighted_actual_names_eval,\n",
    "                    candidate_names_eval,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = [\"<john>\", \"<jonathan>\", \"<mary>\", \"<marie>\", \"<maria>\", \"<george>\"]\n",
    "closure2ids = {\"c\": [0,1,2,3,4,5]}\n",
    "cluster_embeddings = get_swivel_embeddings(swivel_model, swivel_vocab, cluster_names).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id2cluster = generate_clusters(closure2ids, cluster_embeddings, 0.15, \"average\", n_jobs=1)\n",
    "print(id2cluster)\n",
    "id2cluster = generate_clusters(closure2ids, cluster_embeddings, 0.99, \"average\", n_jobs=1)\n",
    "print(id2cluster)\n",
    "id2cluster = generate_clusters(closure2ids, cluster_embeddings, 0.01, \"average\", n_jobs=1)\n",
    "print(id2cluster)\n",
    "id2cluster = generate_clusters(closure2ids, cluster_embeddings, 0.01, \"ward\", n_jobs=1)\n",
    "print(id2cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "cluster_embeddings = normalize(cluster_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "\n",
    "min_samples=2\n",
    "max_eps=0.7\n",
    "xi=0.05   # 0.01..0.20, 0.01\n",
    "metric=\"cosine\"\n",
    "eps=0.45  # 0.45..0.70, 0.05\n",
    "\n",
    "\n",
    "clust = OPTICS(min_samples=min_samples, \n",
    "               xi=xi, \n",
    "               max_eps=max_eps,\n",
    "               metric=metric,\n",
    "              )\n",
    "clust.fit(cluster_embeddings)\n",
    "\n",
    "labels = cluster_optics_dbscan(\n",
    "    reachability=clust.reachability_,\n",
    "    core_distances=clust.core_distances_,\n",
    "    ordering=clust.ordering_,\n",
    "    eps=eps,\n",
    ")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "min_samples=2\n",
    "eps=0.0\n",
    "selection_method=\"leaf\"\n",
    "min_cluster_size=2\n",
    "\n",
    "clust = hdbscan.HDBSCAN(min_samples=min_samples,\n",
    "                        cluster_selection_epsilon=eps,\n",
    "                        cluster_selection_method=selection_method,\n",
    "                        min_cluster_size=min_cluster_size,\n",
    "                        metric=\"euclidean\",\n",
    "                        )\n",
    "clust.fit(cluster_embeddings)\n",
    "\n",
    "clust.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cluster = max(clust.labels_)\n",
    "max_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,1,0,1,-1,-1,0,1,-1]\n",
    "max_cluster = max(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for label in labels:\n",
    "    if label < 0:\n",
    "        max_cluster += 1\n",
    "        label = max_cluster\n",
    "    results.append(label)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
