{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hyperparameter search for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "import heapq\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
    "import jellyfish\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.integration.wandb import WandbLoggerCallback\n",
    "import torch\n",
    "\n",
    "from src.data.normalize import normalize_freq_names\n",
    "from src.data.utils import load_dataset, select_frequent_k\n",
    "from src.data.filesystem import fopen\n",
    "from src.eval.metrics import (\n",
    "    avg_precision_at_threshold, \n",
    "    avg_weighted_recall_at_threshold,\n",
    "    precision_weighted_recall_curve_at_threshold,\n",
    ")\n",
    "from src.models.cluster import (\n",
    "    generate_clusters,\n",
    "    get_clusters,\n",
    "    get_best_cluster_matches,\n",
    "    get_names_to_cluster,\n",
    "    get_distances,\n",
    "    generate_clusters_from_distances,\n",
    "    get_validation_results,\n",
    ")\n",
    "from src.models.ensemble import get_best_ensemble_matches\n",
    "from src.models.swivel import SwivelModel, get_swivel_embeddings\n",
    "from src.models.swivel_encoder import SwivelEncoderModel\n",
    "from src.models.utils import add_padding, remove_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# configure\n",
    "wandb_api_key_file = \"../.wandb-api-key\"\n",
    "given_surname = \"given\"\n",
    "vocab_size = 610000 if given_surname == \"given\" else 2100000\n",
    "embed_dim = 100\n",
    "encoder_layers = 2\n",
    "num_matches = 5000\n",
    "batch_size = 256\n",
    "\n",
    "DEFAULT_NAMES_TO_CLUSTER = 50000  # TODO tune\n",
    "DEFAULT_SEARCH_THRESHOLD = 0.0\n",
    "DEFAULT_REPEAT_FREQ_NAMES = False\n",
    "DEFAULT_ALGO = \"agglomerative\"\n",
    "# agglomerative options\n",
    "DEFAULT_CLUSTER_THRESHOLD = 0.3\n",
    "DEFAULT_CLUSTER_LINKAGE = \"average\"\n",
    "# optics and hdbscan options\n",
    "DEFAULT_MIN_SAMPLES = 2\n",
    "DEFAULT_EPS = 0.2\n",
    "# optics options\n",
    "DEFAULT_MAX_EPS = 1.0\n",
    "DEFAULT_XI = 0.15\n",
    "# hdbscan options\n",
    "DEFAULT_SELECTION_METHOD = \"eom\"\n",
    "DEFAULT_MIN_CLUSTER_SIZE = 2\n",
    "\n",
    "MAX_NAMES_TO_CLUSTER = 200000\n",
    "\n",
    "Config = namedtuple(\"Config\", [ \n",
    "    \"eval_path\",\n",
    "    \"freq_path\",\n",
    "    \"embed_dim\",\n",
    "    \"swivel_vocab_path\",\n",
    "    \"swivel_model_path\",\n",
    "    \"tfidf_path\",\n",
    "    \"ensemble_model_path\"\n",
    "])\n",
    "config = Config(\n",
    "    eval_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train.csv.gz\",\n",
    "    freq_path=f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\",\n",
    "    embed_dim=embed_dim,\n",
    "    swivel_vocab_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-vocab-{vocab_size}-augmented.csv\",\n",
    "    swivel_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-model-{vocab_size}-{embed_dim}-augmented.pth\",\n",
    "    tfidf_path=f\"s3://nama-data/data/models/fs-{given_surname}-tfidf.joblib\",\n",
    "    ensemble_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-ensemble-model-{vocab_size}-{embed_dim}-augmented-100.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"  # force CPU because we want to run multiple trials in parallel\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_eval, weighted_actual_names_eval, candidate_names_eval = load_dataset(config.eval_path, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_names_eval = set([name for wans in weighted_actual_names_eval for name, _, _ in wans])\n",
    "candidate_names_eval = np.array(list(actual_names_eval))\n",
    "del actual_names_eval\n",
    "print(len(candidate_names_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.read_csv(config.freq_path, na_filter=False)\n",
    "name_freq = normalize_freq_names(freq_df, is_surname=given_surname != \"given\", add_padding=True)\n",
    "freq_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_df = pd.read_csv(fopen(config.swivel_vocab_path, \"rb\"))\n",
    "swivel_vocab = {name: _id for name, _id in zip(vocab_df[\"name\"], vocab_df[\"index\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swivel_model = SwivelModel(len(swivel_vocab), config.embed_dim)\n",
    "swivel_model.load_state_dict(torch.load(fopen(config.swivel_model_path, \"rb\"), map_location=torch.device(device)))\n",
    "swivel_model.to(device)\n",
    "swivel_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = joblib.load(fopen(config.tfidf_path, mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = joblib.load(fopen(config.ensemble_model_path, mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(config,\n",
    "               swivel_model,\n",
    "               swivel_vocab,\n",
    "               tfidf_vectorizer,\n",
    "               ensemble_model,\n",
    "               name_freq,\n",
    "               input_names_eval,\n",
    "               weighted_actual_names_eval,\n",
    "               candidate_names_eval,\n",
    "               n_jobs=1,\n",
    "               verbose=False):\n",
    "    \n",
    "    names_to_cluster = get_names_to_cluster(name_freq, config[\"n_to_cluster\"])\n",
    "    \n",
    "    distances = get_distances(name_freq, \n",
    "                              names_to_cluster,\n",
    "                              swivel_model=swivel_model,\n",
    "                              swivel_vocab=swivel_vocab,\n",
    "                              tfidf_vectorizer=tfidf_vectorizer,\n",
    "                              ensemble_model=ensemble_model,\n",
    "                              num_matches=num_matches,\n",
    "                              verbose=verbose,\n",
    "                              n_jobs=n_jobs,\n",
    "                             )\n",
    "    \n",
    "    name_cluster = generate_clusters_from_distances(cluster_algo=config[\"cluster_algo\"],\n",
    "                 cluster_linkage=config[\"cluster_linkage\"],\n",
    "                 cluster_threshold=config[\"cluster_threshold\"],\n",
    "                 distances=distances,\n",
    "                 names_to_cluster=names_to_cluster,\n",
    "                 verbose=verbose,\n",
    "                 n_jobs=n_jobs)\n",
    "    \n",
    "    # validate on validation sets of various sizes\n",
    "    return get_validation_results(input_names_eval=input_names_eval,\n",
    "                                  weighted_actual_names_eval=weighted_actual_names_eval,\n",
    "                                  candidate_names_eval=candidate_names_eval,\n",
    "                                  name_freq=name_freq,\n",
    "                                  name_cluster=name_cluster,\n",
    "                                  swivel_model=swivel_model,\n",
    "                                  swivel_vocab=swivel_vocab,\n",
    "                                  tfidf_vectorizer=tfidf_vectorizer,\n",
    "                                  ensemble_model=ensemble_model,\n",
    "                                  search_threshold=config[\"search_threshold\"],\n",
    "                                  num_matches=num_matches,\n",
    "                                  max_clusters=config[\"max_clusters\"],\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def ray_training_function(config,\n",
    "#                           swivel_model,\n",
    "#                           swivel_vocab,\n",
    "#                           tfidf_vectorizer,\n",
    "#                           ensemble_model,\n",
    "#                           name_freq,\n",
    "#                           input_names_eval,\n",
    "#                           weighted_actual_names_eval,\n",
    "#                           candidate_names_eval,\n",
    "#                           n_jobs=1,\n",
    "#                           verbose=False):\n",
    "\n",
    "#     result = train_eval(config,\n",
    "#                         swivel_model,\n",
    "#                         swivel_vocab,\n",
    "#                         tfidf_vectorizer,\n",
    "#                         ensemble_model,\n",
    "#                         name_freq,\n",
    "#                         input_names_eval,\n",
    "#                         weighted_actual_names_eval,\n",
    "#                         candidate_names_eval,\n",
    "#                         n_jobs=n_jobs,\n",
    "#                         verbose=verbose)\n",
    "    \n",
    "#     if 'error' not in result:\n",
    "#         # Report the metrics to Ray\n",
    "#         tune.report(f1=result['f1'],\n",
    "#                     f2=result['f2'],\n",
    "#                     f1s=result['f1s'],\n",
    "#                     f2s=result['f2s'],\n",
    "#                     precisions=result['precisions'], \n",
    "#                     recalls=result['recalls'],\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# config_params={\n",
    "#     \"cluster_algo\": DEFAULT_ALGO,\n",
    "#     \"n_to_cluster\": tune.qrandint(100000, 200000, 50000),\n",
    "#     \"search_threshold\": 0.0,  # tune.quniform(0.0, 0.6, 0.1),\n",
    "#     \"max_clusters\": 10,\n",
    "#     \"repeat_freq_names\": False,  # tune.choice([True, False]),\n",
    "#     \"cluster_threshold\": tune.quniform(-0.98, -0.78, 0.05),\n",
    "#     \"cluster_linkage\": \"average\",  # tune.choice([\"average\", \"single\", \"complete\"]),\n",
    "#     \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "#     \"eps\": DEFAULT_EPS,\n",
    "#     \"max_eps\": DEFAULT_MAX_EPS,\n",
    "#     \"cluster_method\": \"dbscan\",\n",
    "#     \"xi\": DEFAULT_XI, \n",
    "#     \"selection_method\": DEFAULT_SELECTION_METHOD,  # tune.choice([\"eom\", \"leaf\"]),\n",
    "#     \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,\n",
    "# }\n",
    "\n",
    "# current_best_params = [{\n",
    "#     \"cluster_algo\": DEFAULT_ALGO,\n",
    "#     \"n_to_cluster\": DEFAULT_NAMES_TO_CLUSTER,\n",
    "#     \"search_threshold\": DEFAULT_SEARCH_THRESHOLD,\n",
    "#     \"max_clusters\": 10,\n",
    "#     \"repeat_freq_names\": DEFAULT_REPEAT_FREQ_NAMES,\n",
    "#     \"cluster_threshold\": DEFAULT_CLUSTER_THRESHOLD,\n",
    "#     \"cluster_linkage\": DEFAULT_CLUSTER_LINKAGE,\n",
    "#     \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "#     \"eps\": DEFAULT_EPS,\n",
    "#     \"max_eps\": DEFAULT_MAX_EPS,\n",
    "#     \"cluster_method\": \"dbscan\",\n",
    "#     \"xi\": DEFAULT_XI,\n",
    "#     \"selection_method\": DEFAULT_SELECTION_METHOD,\n",
    "#     \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,\n",
    "# }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-hyperopt\n",
    "\n",
    "# search_alg = HyperOptSearch(points_to_evaluate=current_best_params)\n",
    "\n",
    "# ray.shutdown()\n",
    "# ray.init()\n",
    "\n",
    "# callbacks = []\n",
    "# if wandb_api_key_file:\n",
    "#     callbacks.append(WandbLoggerCallback(\n",
    "#         project=\"nama\",\n",
    "#         entity=\"nama\",\n",
    "#         group=\"80_cluster_tune_\"+given_surname+\"_agglomerative\",\n",
    "#         notes=\"\",\n",
    "#         config=config._asdict(),\n",
    "#         api_key_file=wandb_api_key_file\n",
    "#     ))\n",
    "\n",
    "# result = tune.run(\n",
    "#     tune.with_parameters(ray_training_function,\n",
    "#                          swivel_model=swivel_model,\n",
    "#                          swivel_vocab=swivel_vocab,\n",
    "#                          tfidf_vectorizer=tfidf_vectorizer,\n",
    "#                          ensemble_model=ensemble_model,\n",
    "#                          name_freq=name_freq,\n",
    "#                          input_names_eval=input_names_eval,\n",
    "#                          weighted_actual_names_eval=weighted_actual_names_eval,\n",
    "#                          candidate_names_eval=candidate_names_eval),\n",
    "#     resources_per_trial={\"cpu\": 8.0, \"gpu\": 0.0},\n",
    "#     max_concurrent_trials=1,\n",
    "#     config=config_params,\n",
    "#     search_alg=search_alg,\n",
    "#     num_samples=100,\n",
    "#     metric=\"f2\",\n",
    "#     mode=\"max\",\n",
    "#     checkpoint_score_attr=\"f2\",\n",
    "#     time_budget_s=46*3600,\n",
    "#     progress_reporter=tune.JupyterNotebookReporter(\n",
    "#         overwrite=False,\n",
    "#         max_report_frequency=5*60\n",
    "#     ),\n",
    "#     callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Get trial that has the highest F1\n",
    "# best_trial = result.get_best_trial(metric='f2', mode='max', scope='all')\n",
    "\n",
    "# # Parameters with the highest F1\n",
    "# best_trial.config\n",
    "\n",
    "# print(f\"Best trial final train f2: {best_trial.last_result['f2']}\")\n",
    "# print(f\"Best trial final train precision: {best_trial.last_result['precision']}\")\n",
    "# print(f\"Best trial final train recall: {best_trial.last_result['recall']}\")\n",
    "\n",
    "# # All trials as pandas dataframe\n",
    "# df = result.results_df\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hyperopt_objective_function(swivel_model,\n",
    "#                                 swivel_vocab,\n",
    "#                                 tfidf_vectorizer,\n",
    "#                                 ensemble_model,\n",
    "#                                 name_freq,\n",
    "#                                 input_names_eval,\n",
    "#                                 weighted_actual_names_eval,\n",
    "#                                 candidate_names_eval,\n",
    "#                                 n_jobs=1,\n",
    "#                                 verbose=False):\n",
    "#     def objective(config):\n",
    "#         config['n_to_cluster'] = int(config['n_to_cluster'])\n",
    "#         if verbose:\n",
    "#             print(\"config\", datetime.now(), config)\n",
    "#         result = train_eval(config,\n",
    "#                             swivel_model,\n",
    "#                             swivel_vocab,\n",
    "#                             tfidf_vectorizer,\n",
    "#                             ensemble_model,\n",
    "#                             name_freq,\n",
    "#                             input_names_eval,\n",
    "#                             weighted_actual_names_eval,\n",
    "#                             candidate_names_eval,\n",
    "#                             n_jobs=n_jobs,\n",
    "#                             verbose=verbose)\n",
    "#         if verbose:\n",
    "#             print(\"result\", datetime.now(), result)\n",
    "\n",
    "#         if 'error' in result:\n",
    "#             return {\n",
    "#                 'status': STATUS_FAIL\n",
    "#             }\n",
    "#         else:\n",
    "#             return {\n",
    "#                 'status': STATUS_OK,\n",
    "#                 'loss': 1.0 - result['f2'],\n",
    "#                 'config': config,\n",
    "#                 'f1': result['f1'],\n",
    "#                 'f2': result['f2'],\n",
    "#                 'f1s': result['f1s'],\n",
    "#                 'f2s': result['f2s'],\n",
    "#                 'precisions': result['precisions'],\n",
    "#                 'recalls': result['recalls'],\n",
    "#             }\n",
    "#     return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_jobs = 64\n",
    "\n",
    "# # HyperOpt search space\n",
    "# space = {\n",
    "#     \"cluster_algo\": DEFAULT_ALGO,\n",
    "#     \"n_to_cluster\": 100000,  # hp.quniform('n_to_cluster', 50000, 200000, 50000),\n",
    "#     \"search_threshold\": 0.0,  # hp.quniform('search_threshold', 0.0, 0.1, 0.1),\n",
    "#     \"repeat_freq_names\": False,  # hp.choice('repeat_freq_names', [True, False]),\n",
    "#     \"cluster_threshold\": 0.0,  # hp.quniform('cluster_threshold', -0.5, 0.1, 0.2),\n",
    "#     \"cluster_linkage\": \"average\", # hp.choice('cluster_linkage', [\"average\", \"single\"]), \n",
    "#     \"min_samples\": DEFAULT_MIN_SAMPLES,\n",
    "#     \"eps\": DEFAULT_EPS,\n",
    "#     \"max_eps\": DEFAULT_MAX_EPS,\n",
    "#     \"cluster_method\": \"dbscan\",\n",
    "#     \"xi\": DEFAULT_XI, \n",
    "#     \"selection_method\": DEFAULT_SELECTION_METHOD,  # tune.choice([\"eom\", \"leaf\"]),\n",
    "#     \"min_cluster_size\": DEFAULT_MIN_CLUSTER_SIZE,    \n",
    "# }\n",
    "# objective = hyperopt_objective_function(swivel_model,\n",
    "#                                         swivel_vocab,\n",
    "#                                         tfidf_vectorizer,\n",
    "#                                         ensemble_model,\n",
    "#                                         name_freq,\n",
    "#                                         input_names_eval,\n",
    "#                                         weighted_actual_names_eval,\n",
    "#                                         candidate_names_eval,\n",
    "#                                         n_jobs=n_jobs,\n",
    "#                                         verbose=True)\n",
    "# trials = Trials()\n",
    "\n",
    "# # minimize the objective over the space\n",
    "# best = fmin(objective, \n",
    "#             space, \n",
    "#             algo=tpe.suggest, \n",
    "#             trials=trials,\n",
    "#             max_evals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"best\", best)\n",
    "# print(\"results\", trials.results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 8\n",
    "verbose = True\n",
    "n_to_cluster = 20\n",
    "cluster_threshold=0.3\n",
    "search_threshold=0.0\n",
    "max_clusters=20 \n",
    "search_thresholds=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_cluster = get_names_to_cluster(name_freq, n_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = get_distances(name_freq, \n",
    "                          names_to_cluster,\n",
    "                          swivel_model=swivel_model,\n",
    "                          swivel_vocab=swivel_vocab,\n",
    "                          tfidf_vectorizer=tfidf_vectorizer,\n",
    "                          ensemble_model=ensemble_model,\n",
    "                          num_matches=num_matches,\n",
    "                          verbose=verbose,\n",
    "                          n_jobs=n_jobs,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(f\"distances_{given_surname}.npz\", distances=distances)\n",
    "with open(f\"names_to_cluster_{given_surname}.pickle\", 'wb') as handle:\n",
    "    pickle.dump(names_to_cluster, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.load(f\"distances_{given_surname}.npz\", allow_pickle=True)[\"distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"names_to_cluster_{given_surname}.pickle\", \"rb\") as f:\n",
    "    names_to_cluster = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many input and candidate names are not in names to cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_cluster_set = set(names_to_cluster)\n",
    "print(len(names_to_cluster_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100000\n",
    "input_names_validate, weighted_actual_names_validate, candidate_names_validate = \\\n",
    "    select_frequent_k(input_names_eval,\n",
    "                      weighted_actual_names_eval,\n",
    "                      candidate_names_eval,\n",
    "                      size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_names_validate))\n",
    "print(len(candidate_names_validate))\n",
    "print(len([name for name in input_names_validate if name not in names_to_cluster_set]))\n",
    "print(len([name for name in candidate_names_validate if name not in names_to_cluster_set]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate clusters @ 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_threshold = 0.5\n",
    "name_cluster = generate_clusters_from_distances(cluster_algo=\"agglomerative\",\n",
    "             cluster_linkage=\"average\",\n",
    "             cluster_threshold=cluster_threshold,\n",
    "             distances=distances,\n",
    "             names_to_cluster=names_to_cluster,\n",
    "             verbose=verbose,\n",
    "             n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = defaultdict(int)\n",
    "cluster_names = defaultdict(list)\n",
    "for name, cluster in name_cluster.items():\n",
    "    cluster_counts[cluster] += name_freq[name]\n",
    "    cluster_names[cluster].append(name)\n",
    "cluster_counts_df = pd.DataFrame.from_dict(cluster_counts, \n",
    "                                           orient='index',\n",
    "                                           columns=['counts'],\n",
    "                                          )\n",
    "cluster_counts_df.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(name_freq.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts_df.nlargest(20, 'counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in cluster_counts_df.nlargest(20, 'counts').itertuples():\n",
    "    cluster = tup[0]\n",
    "    count = tup[1]\n",
    "    print(cluster, count, len(cluster_names[cluster]), cluster_names[cluster])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_validation_results(input_names_eval=input_names_eval,\n",
    "                              weighted_actual_names_eval=weighted_actual_names_eval,\n",
    "                              candidate_names_eval=candidate_names_eval,\n",
    "                              name_freq=name_freq,\n",
    "                              name_cluster=name_cluster,\n",
    "                              swivel_model=swivel_model,\n",
    "                              swivel_vocab=swivel_vocab,\n",
    "                              tfidf_vectorizer=tfidf_vectorizer,\n",
    "                              ensemble_model=ensemble_model,\n",
    "                              search_threshold=search_thresholds,\n",
    "                              num_matches=num_matches,\n",
    "                              max_clusters=max_clusters,\n",
    "                              n_jobs=n_jobs,\n",
    "                              verbose=verbose)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"name_cluster_{cluster_threshold}.pickle\", 'wb') as handle:\n",
    "    pickle.dump(name_cluster, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate clusters from nysiis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clusters_from_nysiis(names_to_cluster, verbose=False):\n",
    "    result = {}\n",
    "    for name in names_to_cluster:\n",
    "        unpadded_name = remove_padding(name)\n",
    "        code = jellyfish.nysiis(unpadded_name)\n",
    "        result[name] = code\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cluster_nysiis = generate_clusters_from_nysiis(names_to_cluster=names_to_cluster,\n",
    "                                                    verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = defaultdict(set)\n",
    "for name, cluster in name_cluster_nysiis.items():\n",
    "    cluster_names[cluster].add(name)\n",
    "cluster_sizes_df = pd.DataFrame([len(names) for names in cluster_names.values()])\n",
    "print(\"names to cluster\", len(names_to_cluster))\n",
    "print(\"number of clusters\", len(set(name_cluster_nysiis.values())))\n",
    "print(\"max cluster_size\", max([len(names) for names in cluster_names.values()]))\n",
    "cluster_sizes_df.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = defaultdict(int)\n",
    "cluster_names = defaultdict(list)\n",
    "for name, cluster in name_cluster_nysiis.items():\n",
    "    cluster_counts[cluster] += name_freq[name]\n",
    "    cluster_names[cluster].append(name)\n",
    "cluster_counts_df = pd.DataFrame.from_dict(cluster_counts, \n",
    "                                           orient='index',\n",
    "                                           columns=['counts'],\n",
    "                                          )\n",
    "cluster_counts_df.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts_df.nlargest(20, 'counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in cluster_counts_df.nlargest(20, 'counts').itertuples():\n",
    "    cluster = tup[0]\n",
    "    count = tup[1]\n",
    "    print(cluster, count, len(cluster_names[cluster]), cluster_names[cluster])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we've added all names to the lookup table\n",
    "names_to_cluster_nysiis = list(set(names_to_cluster).union(set(input_names_eval)).union(set(candidate_names_eval)))\n",
    "name_cluster_nysiis = generate_clusters_from_nysiis(\n",
    "             names_to_cluster=names_to_cluster_nysiis,\n",
    "             verbose=verbose)\n",
    "print(len(name_cluster_nysiis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_validation_results(input_names_eval=input_names_eval,\n",
    "                              weighted_actual_names_eval=weighted_actual_names_eval,\n",
    "                              candidate_names_eval=candidate_names_eval,\n",
    "                              name_cluster=name_cluster_nysiis,\n",
    "                              name_freq=None,\n",
    "                              swivel_model=None,\n",
    "                              swivel_vocab=None,\n",
    "                              tfidf_vectorizer=None,\n",
    "                              ensemble_model=None,\n",
    "                              num_matches=None,\n",
    "                              max_clusters=None,\n",
    "                              search_threshold=0.5,\n",
    "                              lookup_mode=True,\n",
    "                              n_jobs=n_jobs,\n",
    "                              verbose=verbose)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate clusters from soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clusters_from_soundex(names_to_cluster, verbose=False):\n",
    "    result = {}\n",
    "    for name in names_to_cluster:\n",
    "        unpadded_name = remove_padding(name)\n",
    "        code = jellyfish.soundex(unpadded_name)\n",
    "        result[name] = code\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cluster_soundex = generate_clusters_from_soundex(names_to_cluster=names_to_cluster,\n",
    "                                                      verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = defaultdict(set)\n",
    "for name, cluster in name_cluster_soundex.items():\n",
    "    cluster_names[cluster].add(name)\n",
    "cluster_sizes_df = pd.DataFrame([len(names) for names in cluster_names.values()])\n",
    "print(\"names to cluster\", len(names_to_cluster))\n",
    "print(\"number of clusters\", len(set(name_cluster_soundex.values())))\n",
    "print(\"max cluster_size\", max([len(names) for names in cluster_names.values()]))\n",
    "cluster_sizes_df.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = defaultdict(int)\n",
    "cluster_names = defaultdict(list)\n",
    "for name, cluster in name_cluster_soundex.items():\n",
    "    cluster_counts[cluster] += name_freq[name]\n",
    "    cluster_names[cluster].append(name)\n",
    "cluster_counts_df = pd.DataFrame.from_dict(cluster_counts, \n",
    "                                           orient='index',\n",
    "                                           columns=['counts'],\n",
    "                                          )\n",
    "cluster_counts_df.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts_df.nlargest(20, 'counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in cluster_counts_df.nlargest(20, 'counts').itertuples():\n",
    "    cluster = tup[0]\n",
    "    count = tup[1]\n",
    "    print(cluster, count, len(cluster_names[cluster]), cluster_names[cluster])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we've added all names to the lookup table\n",
    "names_to_cluster_soundex = list(set(names_to_cluster).union(set(input_names_eval)).union(set(candidate_names_eval)))\n",
    "name_cluster_soundex = generate_clusters_from_soundex(\n",
    "             names_to_cluster=names_to_cluster_soundex,\n",
    "             verbose=verbose)\n",
    "print(len(name_cluster_soundex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_validation_results(input_names_eval=input_names_eval,\n",
    "                              weighted_actual_names_eval=weighted_actual_names_eval,\n",
    "                              candidate_names_eval=candidate_names_eval,\n",
    "                              name_cluster=name_cluster_soundex,\n",
    "                              name_freq=None,\n",
    "                              swivel_model=None,\n",
    "                              swivel_vocab=None,\n",
    "                              tfidf_vectorizer=None,\n",
    "                              ensemble_model=None,\n",
    "                              num_matches=None,\n",
    "                              max_clusters=None,\n",
    "                              search_threshold=0.5,\n",
    "                              lookup_mode=True,\n",
    "                              n_jobs=n_jobs,\n",
    "                              verbose=verbose)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate clusters from old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the old cluster map\n",
    "with open(f\"std_{given_surname}.txt\", \"rt\") as f:\n",
    "    lines = f.readlines()\n",
    "old_name_cluster_map = {}\n",
    "for line in lines:\n",
    "    line = line.replace(':', ' '). strip()\n",
    "    cluster = None\n",
    "    for name in line.split(' '):\n",
    "        name = name.strip()\n",
    "        if not name:\n",
    "            continue\n",
    "        if cluster is None:\n",
    "            cluster = name\n",
    "        old_name_cluster_map[add_padding(name)] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read additional name->cluster assignments\n",
    "with open(f\"names_not_found_{given_surname}.txt\", \"rt\") as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    name, cluster = line.split(' ')\n",
    "    old_name_cluster_map[add_padding(name)] = cluster \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_name_cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many names to cluster are not in the lookup table?\n",
    "names_not_found = set()\n",
    "for name in names_to_cluster:\n",
    "    if name not in old_name_cluster_map:\n",
    "        names_not_found.add(remove_padding(name))\n",
    "for name in set(input_names_eval).union(candidate_names_eval):\n",
    "    if name not in old_name_cluster_map:\n",
    "        names_not_found.add(remove_padding(name))\n",
    "print(len(names_not_found))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out names not in the lookup table\n",
    "with open(\"names_not_found.txt\", \"wt\") as f:\n",
    "    for name in names_not_found:\n",
    "        f.write(name+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clusters for names to cluster\n",
    "def generate_clusters_from_old_map(names_to_cluster, verbose=False):\n",
    "    result = {}\n",
    "    for name in names_to_cluster:\n",
    "        cluster = old_name_cluster_map[name]\n",
    "        result[name] = cluster\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cluster_old = generate_clusters_from_old_map(names_to_cluster=names_to_cluster,\n",
    "                                                  verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(name_cluster_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = defaultdict(set)\n",
    "for name, cluster in name_cluster_old.items():\n",
    "    cluster_names[cluster].add(name)\n",
    "cluster_sizes_df = pd.DataFrame([len(names) for names in cluster_names.values()])\n",
    "print(\"names to cluster\", len(names_to_cluster))\n",
    "print(\"number of clusters\", len(set(name_cluster_old.values())))\n",
    "print(\"max cluster_size\", max([len(names) for names in cluster_names.values()]))\n",
    "cluster_sizes_df.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = defaultdict(int)\n",
    "cluster_names = defaultdict(list)\n",
    "for name, cluster in name_cluster_old.items():\n",
    "    cluster_counts[cluster] += name_freq.get(name, 0)\n",
    "    cluster_names[cluster].append(name)\n",
    "cluster_counts_df = pd.DataFrame.from_dict(cluster_counts, \n",
    "                                           orient='index',\n",
    "                                           columns=['counts'],\n",
    "                                          )\n",
    "cluster_counts_df.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts_df.nlargest(20, 'counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in cluster_counts_df.nlargest(20, 'counts').itertuples():\n",
    "    cluster = tup[0]\n",
    "    count = tup[1]\n",
    "    print(cluster, count, len(cluster_names[cluster]), cluster_names[cluster])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we've added all names to the lookup table\n",
    "names_to_cluster_old = list(set(names_to_cluster).union(set(input_names_eval)).union(set(candidate_names_eval)))\n",
    "name_cluster_old = generate_clusters_from_old_map(\n",
    "             names_to_cluster=names_to_cluster_old,\n",
    "             verbose=verbose)\n",
    "print(len(name_cluster_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_validation_results(input_names_eval=input_names_eval,\n",
    "                              weighted_actual_names_eval=weighted_actual_names_eval,\n",
    "                              candidate_names_eval=candidate_names_eval,\n",
    "                              name_cluster=name_cluster_old,\n",
    "                              name_freq=None,\n",
    "                              swivel_model=None,\n",
    "                              swivel_vocab=None,\n",
    "                              tfidf_vectorizer=None,\n",
    "                              ensemble_model=None,\n",
    "                              num_matches=None,\n",
    "                              max_clusters=None,\n",
    "                              search_threshold=0.5,\n",
    "                              lookup_mode=True,\n",
    "                              n_jobs=n_jobs,\n",
    "                              verbose=verbose)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
