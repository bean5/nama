{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from src.data.filesystem import fopen\n",
    "from src.data.utils import load_train_test\n",
    "from src.eval import metrics\n",
    "from src.models.swivel import get_swivel_embeddings, get_best_swivel_matches\n",
    "from src.models.swivel_encoder import SwivelEncoderModel, convert_names_to_model_inputs, train_swivel_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "given_surname = \"given\"\n",
    "size = \"freq\"\n",
    "vocab_size = 500000\n",
    "embed_dim = 200\n",
    "\n",
    "Config = namedtuple(\"Config\", \"train_path test_path embed_dim n_layers char_embed_dim n_hidden_units bidirectional lr batch_size use_adam_opt n_epochs swivel_vocab_path swivel_model_path encoder_model_path\")\n",
    "config = Config(\n",
    "    train_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-similar-train-{size}.csv.gz\",\n",
    "    test_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-similar-test-{size}.csv.gz\",\n",
    "    embed_dim=embed_dim,\n",
    "    n_layers = 3,\n",
    "    char_embed_dim = 32,\n",
    "    n_hidden_units = 400,\n",
    "    bidirectional = True,\n",
    "    lr = 0.03,\n",
    "    batch_size=128,\n",
    "    use_adam_opt = False,\n",
    "    n_epochs=200,\n",
    "    swivel_vocab_path=f\"s3://nama-data/data/models/fs-{given_surname}-{size}-swivel-vocab-{vocab_size}.csv\",\n",
    "    swivel_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-{size}-swivel-model-{vocab_size}-{embed_dim}.pt\",\n",
    "    encoder_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-{size}-encoder-model-{vocab_size}-{embed_dim}.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"nama\",\n",
    "    entity=\"nama\",\n",
    "    name=\"56_swivel_encoder\",\n",
    "    group=given_surname,\n",
    "    notes=\"\",\n",
    "    config=config._asdict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[train, test] = load_train_test([config.train_path, config.test_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_train, weighted_actual_names_train, candidate_names_train = train\n",
    "input_names_test, weighted_actual_names_test, candidate_names_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"input_names_train\", len(input_names_train))\n",
    "print(\"weighted_actual_names_train\", sum(len(wan) for wan in weighted_actual_names_train))\n",
    "print(\"candidate_names_train\", len(candidate_names_train))\n",
    "\n",
    "print(\"input_names_test\", len(input_names_test))\n",
    "print(\"weighted_actual_names_test\", sum(len(wan) for wan in weighted_actual_names_test))\n",
    "print(\"candidate_names_test\", len(candidate_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swivel_vocab_df = pd.read_csv(fopen(config.swivel_vocab_path, \"rb\"))\n",
    "print(swivel_vocab_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swivel_vocab = {name: _id for name, _id in zip(swivel_vocab_df[\"name\"], swivel_vocab_df[\"index\"])}\n",
    "print(swivel_vocab[\"<john>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swivel_model = torch.load(fopen(config.swivel_model_path, \"rb\"))\n",
    "print(swivel_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train using all names in the vocabulary\n",
    "train_names = swivel_vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_embeddings = torch.Tensor(get_swivel_embeddings(swivel_model, swivel_vocab, train_names))\n",
    "print(train_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_model_inputs = convert_names_to_model_inputs(train_names)\n",
    "print(train_model_inputs.shape)\n",
    "print(train_model_inputs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "encoder_model = SwivelEncoderModel(n_layers=config.n_layers,\n",
    "                                   char_embed_dim=config.char_embed_dim,\n",
    "                                   n_hidden_units=config.n_hidden_units,\n",
    "                                   output_dim=config.embed_dim,\n",
    "                                   bidirectional=config.bidirectional,\n",
    "                                   pack=False,\n",
    "                                   device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_swivel_encoder(encoder_model,\n",
    "                     train_model_inputs,\n",
    "                     train_embeddings,\n",
    "                     num_epochs=config.n_epochs,\n",
    "                     batch_size=config.batch_size,\n",
    "                     lr=config.lr,\n",
    "                     use_adam_opt=config.use_adam_opt,\n",
    "                     use_mse_loss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get best matches\n",
    "# NOTE: only considers as potential matches names in candidate_names_test, not names in input_names_test\n",
    "k = 100\n",
    "batch_size = 256\n",
    "add_context = True\n",
    "n_jobs=4\n",
    "best_matches = get_best_swivel_matches(swivel_model,\n",
    "                                       swivel_vocab,\n",
    "                                       input_names_test,\n",
    "                                       candidate_names_test,\n",
    "                                       k,\n",
    "                                       batch_size,\n",
    "                                       add_context=add_context,\n",
    "                                       encoder_model=encoder_model,\n",
    "                                       n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics.precision_weighted_recall_curve_at_threshold(\n",
    "    weighted_actual_names_test, best_matches, min_threshold=0.01, max_threshold=2.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(metrics.get_auc(\n",
    "    weighted_actual_names_test, best_matches, min_threshold=0.01, max_threshold=2.0, step=0.05, distances=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_names = [\"<john>\", \"<johnny>\", \"<jonathan>\",\n",
    "              \"<mary>\", \"<marie>\", \"<maria>\"]\n",
    "test_embeddings = torch.Tensor(get_swivel_embeddings(swivel_model, swivel_vocab, test_names))\n",
    "print(test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(test_names[0:3])\n",
    "print(cosine_similarity(test_embeddings[0:1], test_embeddings[0:3]))\n",
    "print(test_names[3:])\n",
    "print(cosine_similarity(test_embeddings[0:1], test_embeddings[3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_model_inputs = convert_names_to_model_inputs(test_names)\n",
    "print(test_model_inputs.shape)\n",
    "print(test_model_inputs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "n_layers = 1\n",
    "char_embed_dim = 64\n",
    "n_hidden_units = 200\n",
    "embed_dim = 200\n",
    "bidirectional = True\n",
    "pack = False\n",
    "encoder_model = SwivelEncoderModel(n_layers=n_layers, char_embed_dim=char_embed_dim, n_hidden_units=n_hidden_units,\n",
    "                                   output_dim=embed_dim, bidirectional=bidirectional, pack=pack, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epochs=100\n",
    "use_adam_opt = False\n",
    "use_mse_loss = False\n",
    "train_swivel_encoder(encoder_model, test_model_inputs, test_embeddings, num_epochs=n_epochs, batch_size=64, lr=lr,\n",
    "                     use_adam_opt=use_adam_opt, use_mse_loss=use_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_embeddings_predicted = encoder_model(test_model_inputs).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_embeddings_numpy = test_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cosine_similarity(test_embeddings_numpy, test_embeddings_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cosine_similarity(test_embeddings_numpy, test_embeddings_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Replicate model training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create optimizer and loss function\n",
    "batch_size = 16\n",
    "lr = 0.05\n",
    "\n",
    "optimizer = torch.optim.Adam(encoder_model.parameters(), lr=lr)\n",
    "# optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create data loader\n",
    "dataset_train = torch.utils.data.TensorDataset(test_model_inputs, test_embeddings)\n",
    "data_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get batch\n",
    "train_batch, targets_batch = next(iter(data_loader))\n",
    "print(train_batch.shape)\n",
    "print(targets_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.data import constants\n",
    "X = train_batch\n",
    "encoder_model.to(device=device)\n",
    "\n",
    "# Compute forward pass\n",
    "# x_prime = model(train_batch)\n",
    "\n",
    "# Clear out gradient\n",
    "encoder_model.zero_grad()\n",
    "\n",
    "# forward pass\n",
    "X = X.to(device=device)\n",
    "batch_size, seq_len = train_batch.size()\n",
    "print(\"batch_size\", batch_size, \"seq_len\", seq_len)\n",
    "\n",
    "# init hidden state before each batch\n",
    "n_directions = 2 if bidirectional else 1\n",
    "# hidden = (\n",
    "#     torch.randn(n_layers * n_directions, batch_size, n_hidden_units).to(device=device),  # initial hidden state\n",
    "#     torch.randn(n_layers * n_directions, batch_size, n_hidden_units).to(device=device),  # initial cell state\n",
    "# )\n",
    "\n",
    "# sort batch by sequence length\n",
    "# X_lengths = torch.count_nonzero(X, dim=1).to(device=\"cpu\").type(torch.int64)\n",
    "# ixs = torch.argsort(X_lengths, descending=True)\n",
    "# X = X[ixs]\n",
    "# X_lengths = X_lengths[ixs]\n",
    "# print(\"X\", X.get_device(), \"X_lengths\", X_lengths.get_device())\n",
    "\n",
    "\n",
    "eye = torch.eye(constants.VOCAB_SIZE + 1).to(device=device)\n",
    "X = eye[X]\n",
    "\n",
    "# pack sequences\n",
    "# X = pack_padded_sequence(X, X_lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "# run through LSTM\n",
    "# all, hidden = encoder_model.lstm(X.to(device), hidden)\n",
    "all, (hidden, cell) = encoder_model.lstm(X.to(device))\n",
    "print(\"hidden\", hidden.shape, cell.shape)\n",
    "\n",
    "embeddings = encoder_model.linear(hidden[0][-1])  # compute the linear model based on the last hidden state of the last layer\n",
    "print(\"embeddings\", embeddings.shape)\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fn(embeddings, targets_batch.to(encoder_model.device))\n",
    "# do the backward pass and update parameters\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
