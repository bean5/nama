{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ce41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9e91d",
   "metadata": {},
   "source": [
    "# Split existing clusters into sub-clusters using trained model\n",
    "\n",
    "Load the parser and trained model, and use a hierarchal agglomerative clustering algorithm to split existing FamilySearch clusters (buckets) into sub-clusters based upon similarity computed using the trained model. Each sub-cluster will contain names that the model determines are similar to each other. \n",
    "\n",
    "We want to create cohesive sub-clusters, but not too many, because we will eventually need to manually create \"wormholes\" to combine the sub-clusters back into the original clusters.\n",
    "\n",
    "Each sub-cluster will contain:\n",
    "\n",
    "1. a list of names, \n",
    "2. the most-common name as the cluster label, and \n",
    "3. a cluster centroid: a vector depicting the center of the cluster. \n",
    "\n",
    "When determine which sub-cluster a rare name belongs to, we will choose the closest centroid.\n",
    "\n",
    "The questions to answer are:\n",
    "\n",
    "1. Should we weight more-common names more than less-common names when computing the clusters?: log_10(freq)? NO\n",
    "2. Should we use single, average, complete, or ward linkage? AVERAGE\n",
    "3. What should the threshold be? 0.83\n",
    "4. Should we do dimensionality reduction, and if so, using umap or PCA or t-sne? NO - umap and PCA @ 10 didn't work\n",
    "5. Should we weight more-common names more than less-common names when computing the cluster centroid? NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d62bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import umap\n",
    "\n",
    "from src.models.biencoder import BiEncoder\n",
    "from src.models.tokenizer import get_tokenize_function_and_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "given_surname = \"given\"\n",
    "\n",
    "linkage = \"average\"  # ward, average, complete, single\n",
    "distance_threshold = 0.65\n",
    "n_dimensions = 0\n",
    "dim_reduction = ''  # pca or umap\n",
    "# freq_normalizer, freq_normalizer_name = lambda x: math.floor(math.log10(max(1,x))), \"log10\"\n",
    "freq_normalizer, freq_normalizer_name = lambda x: 0, \"none\"\n",
    "experiment_name = f\"{linkage}-{distance_threshold}-{freq_normalizer_name}-{n_dimensions}\"\n",
    "\n",
    "vocab_type = 'f'  # tokenizer based upon training name frequency\n",
    "subword_vocab_size = 2000  # 500, 1000, 1500, 2000\n",
    "\n",
    "nama_bucket = 'nama-data'\n",
    "subwords_path=f\"data/models/fs-{given_surname}-subword-tokenizer-{subword_vocab_size}{vocab_type}.json\"\n",
    "pref_path = f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\"\n",
    "buckets_path = f\"../references/std_{given_surname}.txt\"\n",
    "model_path = f\"../data/models/bi_encoder-{given_surname}.pth\"\n",
    "experiment_dir = f\"../reports/\"\n",
    "sub_clusters_path = f\"../data/models/sub_clusters_{given_surname}-{distance_threshold}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80ae29",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load buckets\n",
    "buckets = []\n",
    "with open(buckets_path, 'rt') as f:\n",
    "    for line in f.readlines():\n",
    "        names = line.strip().replace(':', '').split(' ')\n",
    "        buckets.append(names)\n",
    "len(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc03f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pref names\n",
    "pref_df = pd.read_csv(pref_path, keep_default_na=False)\n",
    "name_freq = {name: freq for name, freq in zip(pref_df['name'], pref_df['frequency'])}\n",
    "pref_df = None\n",
    "print(len(name_freq))\n",
    "freq = name_freq['john']\n",
    "print(freq, freq_normalizer(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_freq_name(names):\n",
    "    most_freq_name = names[0]\n",
    "    most_freq_freq = name_freq.get(most_freq_name, 0)\n",
    "    for name in names:\n",
    "        freq = name_freq.get(name, 0)\n",
    "        if freq > most_freq_freq:\n",
    "            most_freq_name = name\n",
    "            most_freq_freq = freq\n",
    "    return most_freq_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a345de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenize function\n",
    "tokenize, tokenizer_vocab = get_tokenize_function_and_vocab(\n",
    "    subwords_path=subwords_path,\n",
    "    nama_bucket=nama_bucket,\n",
    ")\n",
    "len(tokenizer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b603e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('dallan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445a3e5",
   "metadata": {},
   "source": [
    "## Cluster names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7127a8",
   "metadata": {},
   "source": [
    "### compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48580ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_embedding = {}\n",
    "for bucket in tqdm(buckets):\n",
    "    for name in bucket:\n",
    "        embedding = model.get_embedding(tokenize(name))\n",
    "        if linkage == \"ward\" or (n_dimensions > 0 and dim_reduction == \"umap\"):\n",
    "            embedding /= np.linalg.norm(embedding)            \n",
    "        name_embedding[name] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4932f",
   "metadata": {},
   "source": [
    "### try reducing dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if n_dimensions > 0:\n",
    "    embeddings = []\n",
    "    for name in name_embedding.keys():\n",
    "        embeddings.append(name_embedding[name])\n",
    "    if dim_reduction == 'umap':\n",
    "        reducer = umap.UMAP(\n",
    "            n_neighbors=30,\n",
    "            min_dist=0.0,\n",
    "            n_components=n_dimensions,\n",
    "        )\n",
    "    elif dim_reduction == 'pca':\n",
    "        reducer = PCA(n_dimensions)\n",
    "    else:\n",
    "        print(\"ERROR!\", dim_reduction)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    for name, embedding in zip(name_embedding.keys(), reduced_embeddings):\n",
    "        name_embedding[name] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef160e6",
   "metadata": {},
   "source": [
    "### create clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    metric=\"euclidean\" if linkage == \"ward\" or (n_dimensions > 0 and dim_reduction == \"umap\") else \"cosine\",\n",
    "    linkage=linkage,\n",
    "    distance_threshold=distance_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.get_embedding(tokenize('dallan'))))\n",
    "model.get_embedding(tokenize('dallan'))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e09dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.get_embedding(tokenize('dallan'))\n",
    "print(embedding[:20])\n",
    "norm = np.linalg.norm(embedding)\n",
    "print(norm)\n",
    "embedding /= norm\n",
    "print(math.sqrt(sum([v*v for v in embedding])))\n",
    "embedding[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = ['abraham','abe','aabraham','ab','abaham','abaraham','abarham','abb','abelarde','abera','aberaham']\n",
    "X = []\n",
    "names = []\n",
    "for name in bucket:\n",
    "    embedding = name_embedding[name]\n",
    "    freq = max(1, freq_normalizer(name_freq.get(name, 0)))\n",
    "    for _ in range(freq):\n",
    "        names.append(name)\n",
    "        X.append(embedding)\n",
    "clustering = clusterer.fit(X)\n",
    "sub_clusters = [[] for _ in range(clustering.n_clusters_)]\n",
    "print('n_clusters', clustering.n_clusters_)\n",
    "print('labels', clustering.labels_)\n",
    "print('names', names)\n",
    "for name, sub_cluster in zip(names, clustering.labels_):\n",
    "    sub_clusters[sub_cluster].append(name)\n",
    "for sub_cluster in sub_clusters:\n",
    "    print(list(set(sub_cluster)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# aaltje 0.21964864\n",
    "# altgen 0.45249435\n",
    "# eltje 0.08212702\n",
    "# aeltje 0.18246093\n",
    "# aalken 0.11775353\n",
    "# aaltjen 0.253144\n",
    "\n",
    "bucket = ['altino', 'aaltje', 'altgen', 'eltje', 'aeltje', 'aalken', 'aaltjen', ]\n",
    "emb1 = model.get_embedding(tokenize(bucket[0]))\n",
    "for name in bucket[1:]:\n",
    "    emb2 = model.get_embedding(tokenize(name))\n",
    "    sim = F.cosine_similarity(torch.Tensor(emb1), torch.Tensor(emb2), dim=-1)\n",
    "    print(name, sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5c7e1",
   "metadata": {},
   "source": [
    "### run clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc157fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_sub_buckets = {}\n",
    "for bucket in tqdm(buckets):\n",
    "    first_name = bucket[0]\n",
    "    if len(bucket) == 1:\n",
    "        sub_clusters = [bucket]\n",
    "    else:\n",
    "        X = []\n",
    "        names = []\n",
    "        for name in bucket:\n",
    "            embedding = name_embedding[name]\n",
    "            freq = max(1, freq_normalizer(name_freq.get(name, 0)))\n",
    "            for _ in range(freq):\n",
    "                names.append(name)\n",
    "                X.append(embedding)\n",
    "        clustering = clusterer.fit(X)\n",
    "        sub_clusters = [[] for _ in range(clustering.n_clusters_)]\n",
    "        for name, sub_cluster in zip(names, clustering.labels_):\n",
    "            sub_clusters[sub_cluster].append(name)\n",
    "    bucket_sub_buckets[first_name] = []\n",
    "    for sub_cluster in sub_clusters:\n",
    "        bucket_sub_buckets[first_name].append(list(set(sub_cluster)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c9683",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9fb6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_bucket_count = sum(len(sub_buckets) for sub_buckets in bucket_sub_buckets.values())\n",
    "print(sub_bucket_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98340f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather every 25'th name into an experiment\n",
    "experiment = {}\n",
    "for ix, (label, sub_buckets) in enumerate(bucket_sub_buckets.items()):\n",
    "    if ix % 25 != 0:\n",
    "        continue\n",
    "    experiment[label] = sub_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6aa834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_sort_key(name):\n",
    "    freq = name_freq.get(name, 0)\n",
    "    return f\"{freq:12d}:{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "lines.append(f\"Experiment: {experiment_name}\")\n",
    "lines.append(f\"sub-buckets={sub_bucket_count}\")\n",
    "for label, sub_buckets in experiment.items():\n",
    "    lines.append(label)\n",
    "    sub_buckets.sort(key=lambda bucket: name_sort_key(get_most_freq_name(bucket)), reverse=True)\n",
    "    for sub_bucket in sub_buckets:\n",
    "        sub_bucket.sort(key=name_sort_key, reverse=True)\n",
    "        lines.append(f\"- {get_most_freq_name(sub_bucket)}: {' '.join(sub_bucket)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d3cf7",
   "metadata": {},
   "source": [
    "## Save experiment report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043da90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48404dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_filename = f\"{experiment_name}.txt\"\n",
    "with open(os.path.join(experiment_dir, experiment_filename), 'wt') as f:\n",
    "    f.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9ec48",
   "metadata": {},
   "source": [
    "## Save sub-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for sub_buckets in bucket_sub_buckets.values():\n",
    "    all_names = [name for sub_bucket in sub_buckets for name in sub_bucket]\n",
    "    cluster_label = get_most_freq_name(all_names)\n",
    "    clusters[cluster_label] = {}\n",
    "    for sub_bucket in sub_buckets:\n",
    "        sub_cluster_label = get_most_freq_name(sub_bucket)\n",
    "        clusters[cluster_label][sub_cluster_label] = sub_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_clusters_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450254f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sub_clusters_path, 'wt') as f:\n",
    "    json.dump(clusters, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f24c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
