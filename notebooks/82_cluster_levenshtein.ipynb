{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee625f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f546aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import jellyfish\n",
    "from mpire import WorkerPool\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.data.filesystem import fopen\n",
    "from src.data.normalize import normalize\n",
    "from src.data.utils import load_datasets\n",
    "from src.models.utils import remove_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651e31d",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7688d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 0\n",
    "max_closure_size = 10000\n",
    "max_distance = 0.6\n",
    "cluster_distance_threshold = 0.155\n",
    "super_cluster_distance_threshold = 0.205\n",
    "num_candidates = 1000\n",
    "eps = 0.000001\n",
    "\n",
    "# process_nicknames = True\n",
    "# werelate_names_filename = 'givenname_similar_names.werelate.20210414.tsv'\n",
    "# nicknames_filename = '../data/models/givenname_nicknames.txt'\n",
    "# name_freqs_filename = 'given-final.normal.txt'\n",
    "# clusters_filename = 'givenname_clusters.tsv'\n",
    "# super_clusters_filename = 'givenname_super_clusters.tsv'\n",
    "\n",
    "werelate_names_filename = '../data/external/surname_similar_names.werelate.20210414.tsv'\n",
    "nicknames_filename = ''\n",
    "name_freqs_filename = '../data/external/surname-final.normal.txt'\n",
    "clusters_filename = '../data/models/surname_clusters.tsv'\n",
    "super_clusters_filename = '../data/models/surname_super_clusters.tsv'\n",
    "is_surname = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408467de",
   "metadata": {},
   "source": [
    "### Read WeRelate names into all_names\n",
    "Later, we'll want to read frequent FS names into all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rewrite this in just a few lines using pandas\n",
    "def load_werelate_names(path, is_surname):\n",
    "    name_variants = defaultdict(set)\n",
    "    with fopen(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        is_header = True\n",
    "        for line in f:\n",
    "            if is_header:\n",
    "                is_header = False\n",
    "                continue\n",
    "            fields = line.rstrip().split(\"\\t\")\n",
    "            # normalize should only return a single name piece, but loop just in case\n",
    "            for name_piece in normalize(fields[0], is_surname):\n",
    "                confirmed_variants = fields[1].strip().split(\" \") if len(fields) >= 2 else []\n",
    "                computer_variants = fields[2].strip().split(\" \") if len(fields) == 3 else []\n",
    "                variants = confirmed_variants + computer_variants\n",
    "                for variant in variants:\n",
    "                    for variant_piece in normalize(variant, is_surname):\n",
    "                        name_variants[name_piece].add(variant_piece)\n",
    "    return name_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633cc1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = set()\n",
    "\n",
    "name_variants = load_werelate_names(werelate_names_filename, is_surname)\n",
    "print(len(name_variants))\n",
    "for k, v in name_variants.items():\n",
    "    all_names.add(k)\n",
    "    all_names.update(variant for variant in v)\n",
    "# remove empty name\n",
    "all_names.remove('')\n",
    "print(len(all_names), next(iter(all_names)))\n",
    "\n",
    "name_variants = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c76f5f1",
   "metadata": {},
   "source": [
    "### Read nicknames and remove from names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ef949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nicknames(path):\n",
    "    nicknames = defaultdict(set)\n",
    "    with fopen(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            names = line.rstrip().split(\" \")\n",
    "            # normalize should only return a single name piece, but loop just in case\n",
    "            for name_piece in normalize(names[0], False):\n",
    "                orig_name = name_piece\n",
    "                for nickname in names[1:]:\n",
    "                    for nickname_piece in normalize(nickname, False):\n",
    "                        nicknames[nickname_piece].add(orig_name)\n",
    "    return nicknames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34311aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_nicks = defaultdict(set)\n",
    "if not is_surname:\n",
    "    nick_names = load_nicknames(nicknames_filename)\n",
    "    for nick, names in nick_names.items():\n",
    "        for name in names:\n",
    "            name_nicks[name].add(nick)\n",
    "    print(next(iter(nick_names.items())), \"nick_names\", len(nick_names.keys()), \"name_nicks\", len(name_nicks.keys()))\n",
    "    all_names -= set(nickname for nickname in nick_names.keys())\n",
    "    print(len(all_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804b647",
   "metadata": {},
   "source": [
    "### Map names to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_names_to_ids(names):\n",
    "    ids = range(len(names))\n",
    "    return dict(zip(names, ids)), dict(zip(ids, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ids, id_names = map_names_to_ids(all_names)\n",
    "print(next(iter(name_ids.items())), next(iter(id_names.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6052d",
   "metadata": {},
   "source": [
    "### Read name frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be72eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rewrite this using pandas too\n",
    "def load_name_freqs(path, is_surname):\n",
    "    name_freqs = defaultdict(int)\n",
    "    with fopen(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            fields = line.rstrip().split(\"\\t\")\n",
    "            for name_piece in normalize(fields[0], is_surname):\n",
    "                name_freqs[name_piece] = int(fields[1])\n",
    "    return name_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c38ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_freqs = load_name_freqs(name_freqs_filename, is_surname)\n",
    "# keep only entries in all_names\n",
    "name_freqs = dict((k,v) for k,v in name_freqs.items() if k in all_names)\n",
    "print(len(name_freqs), next(iter(name_freqs.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798241e4",
   "metadata": {},
   "source": [
    "#### Take a sample because encoded names require a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_size <= 0 or sample_size >= len(all_names):\n",
    "    names_sample = np.array(list(all_names))\n",
    "else:\n",
    "    names_sample = np.array(random.sample(all_names, sample_size))\n",
    "print(len(names_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcadb92",
   "metadata": {},
   "source": [
    "### Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65765b80",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_distance_to(name):\n",
    "    def calc_distance(row):\n",
    "        dist = jellyfish.damerau_levenshtein_distance(name, row[0])\n",
    "        return dist / max(len(name), len(row[0]))\n",
    "    return calc_distance\n",
    "\n",
    "def process(shared_objects, name):\n",
    "    names_sample, num_candidates = shared_objects\n",
    "    distances = np.apply_along_axis(calc_distance_to(name), 1, names_sample)\n",
    "    sorted_scores_idx = np.argsort(distances)[:num_candidates]\n",
    "    candidate_names = names_sample[sorted_scores_idx, 0]\n",
    "    candidate_distances = distances[sorted_scores_idx]\n",
    "    return np.dstack((candidate_names, candidate_distances))\n",
    "\n",
    "with WorkerPool(shared_objects=(names_sample[:, np.newaxis], num_candidates)) as pool:\n",
    "    name_candidates = pool.map(process, list(names_sample), progress_bar=True)\n",
    "print(name_candidates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c47e4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# first, repeat each name num_candidates times and convert the 1D array to a 2D array\n",
    "# next, reshape name_candidates to be a 2D array where the columns are (candidate, score)\n",
    "# finally, stack the two arrays into a 2D array where the columns are (name, candidate, score)\n",
    "distances = np.hstack((np.repeat(names_sample, num_candidates)[:, np.newaxis], name_candidates.reshape(-1,2)))\n",
    "# remove distances > max_distance\n",
    "distances = distances[distances[:, -1].astype('float') <= max_distance]\n",
    "# sort \n",
    "distances = distances[distances[:, -1].astype('float').argsort()]\n",
    "print(distances.shape)\n",
    "name_candidates = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d0196",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21ea59",
   "metadata": {},
   "source": [
    "### Compute closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0af2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all distances, create closures and save scores\n",
    "next_closure = 0\n",
    "closure_ids = {}\n",
    "id_closure = {}\n",
    "row_ixs = []\n",
    "col_ixs = []\n",
    "dists = []\n",
    "max_size = 0\n",
    "\n",
    "for row in tqdm(distances):\n",
    "    name1 = row[0]\n",
    "    name2 = row[1]\n",
    "    id1 = name_ids[name1]\n",
    "    id2 = name_ids[name2]\n",
    "    # each distance is in distances twice\n",
    "    if id1 > id2:\n",
    "        continue\n",
    "    distance = max(eps, float(row[2]))\n",
    "    closure1 = id_closure.get(id1)\n",
    "    closure2 = id_closure.get(id2)\n",
    "    if closure1 is None and closure2 is not None:        \n",
    "        id1, id2 = id2, id1\n",
    "        name1, name2 = name2, name1\n",
    "        closure1, closure2 = closure2, closure1\n",
    "    # add to distance matrix\n",
    "    row_ixs.append(id1)\n",
    "    col_ixs.append(id2)\n",
    "    dists.append(distance)\n",
    "    # skip if names are the same\n",
    "    if id1 == id2:\n",
    "        continue\n",
    "    row_ixs.append(id2)\n",
    "    col_ixs.append(id1)\n",
    "    dists.append(distance)\n",
    "    # create closures\n",
    "    if closure1 is None:\n",
    "        # if closure1 is None, then closure2 must be none also due to the above\n",
    "        # so create a new closure with id1 and id2\n",
    "        closure1 = next_closure\n",
    "        next_closure += 1\n",
    "        id_closure[id1] = closure1\n",
    "        id_closure[id2] = closure1\n",
    "        closure_ids[closure1] = [id1, id2]\n",
    "        next_closure += 1\n",
    "    elif closure2 is None:\n",
    "        # put id2 into id1's closure\n",
    "        id_closure[id2] = closure1\n",
    "        closure_ids[closure1].append(id2)\n",
    "    elif closure1 != closure2 and len(closure_ids[closure1]) + len(closure_ids[closure2]) <= max_closure_size:\n",
    "        # move all ids in closure2 into closure1\n",
    "        for id in closure_ids[closure2]:\n",
    "            id_closure[id] = closure1\n",
    "            closure_ids[closure1].append(id)\n",
    "        del closure_ids[closure2]\n",
    "    if len(closure_ids[closure1]) > max_size:\n",
    "        max_size = len(closure_ids[closure1])\n",
    "\n",
    "# create distances matrix\n",
    "dist_matrix = csr_matrix((dists, (row_ixs, col_ixs)))\n",
    "\n",
    "print(\"max closure_size\", max_size)\n",
    "print(\"number of closures\", len(closure_ids), \"number of names enclosed\", len(id_closure))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ebeb36",
   "metadata": {},
   "source": [
    "### Compute clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993237c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clusters(closure_ids, id_names, dist_matrix, linkage, distance_threshold, eps, max_dist):\n",
    "    cluster_names = defaultdict(set)\n",
    "    name_cluster = {}\n",
    "    for closure, ids in tqdm(closure_ids.items()):\n",
    "        clusterer = AgglomerativeClustering(n_clusters=None, affinity='precomputed', linkage=linkage, distance_threshold=distance_threshold)\n",
    "        X = dist_matrix[ids][:, ids].todense()\n",
    "        X[X < eps] = max_dist\n",
    "        labels = clusterer.fit_predict(X)\n",
    "        for id, label in zip(ids, labels):\n",
    "            name = id_names[id]\n",
    "            cluster = f'{closure}_{label}'\n",
    "            cluster_names[cluster].add(name)\n",
    "            name_cluster[name] = cluster\n",
    "    return cluster_names, name_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94818b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try ward, average, single\n",
    "cluster_linkage = 'average'\n",
    "max_dist = 10.0\n",
    "\n",
    "cluster_names, name_cluster = compute_clusters(closure_ids, id_names, dist_matrix, cluster_linkage, cluster_distance_threshold, eps, max_dist)\n",
    "print(len(cluster_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e088c18",
   "metadata": {},
   "source": [
    "#### Add unclustered names as singleton clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_singleton_names(cluster_names, name_cluster, names_sample):\n",
    "    for ix, name in enumerate(names_sample):\n",
    "        if name not in name_cluster:\n",
    "            cluster = f'{ix}'\n",
    "            cluster_names[cluster].add(name)\n",
    "            name_cluster[name] = cluster\n",
    "    return cluster_names, name_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7bd9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names, name_cluster = add_singleton_names(cluster_names, name_cluster, names_sample)\n",
    "print(len(cluster_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb365d",
   "metadata": {},
   "source": [
    "### Eval cluster P/R over Ancestry test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded82d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ancestry data\n",
    "# remove padding of input_names_test, weighted_actual_names_test, candidate_names_test, candidate_names_train\n",
    "[train, test] = load_datasets([\"../data/raw/records25k_data_train.csv\", \"../data/raw/records25k_data_test.csv\"])\n",
    "\n",
    "_, _, candidate_names_train = train\n",
    "\n",
    "candidate_names_train = np.array([remove_padding(name) for name in candidate_names_train])\n",
    "\n",
    "input_names_test, weighted_actual_names_test, candidate_names_test = test\n",
    "\n",
    "input_names_test = [remove_padding(name) for name in input_names_test]\n",
    "weighted_actual_names_test = [[(remove_padding(name), score, freq) for (name, score, freq) in actuals] for actuals in weighted_actual_names_test]\n",
    "candidate_names_test = np.array([remove_padding(name) for name in candidate_names_test])\n",
    "\n",
    "candidate_names_all = np.concatenate((candidate_names_train, candidate_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(names_sample, candidate_names_all, input_names_test, weighted_actual_names_test, cluster_names, name_cluster):\n",
    "    names_sample_set = set(names_sample.tolist())\n",
    "    candidate_names_all_set = set(candidate_names_all.tolist())\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for input_name, weighted_actual_names in zip(input_names_test, weighted_actual_names_test):\n",
    "        if input_name not in names_sample_set:\n",
    "            continue\n",
    "        cluster_id = name_cluster[input_name]\n",
    "        names_in_cluster = cluster_names[cluster_id] & candidate_names_all_set\n",
    "        found_recall = 0.0\n",
    "        total_recall = 0.0\n",
    "        found_count = 0\n",
    "        for name, weight, _ in weighted_actual_names:\n",
    "            if name in names_sample_set:\n",
    "                total_recall += weight\n",
    "            if name in names_in_cluster:\n",
    "                found_recall += weight\n",
    "                found_count += 1\n",
    "        if total_recall == 0.0:\n",
    "            continue\n",
    "        precision = found_count / len(names_in_cluster) if len(names_in_cluster) > 0 else 1.0\n",
    "        recall = found_recall / total_recall\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    return avg_precision, avg_recall, len(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c529ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, total = get_precision_recall(names_sample, candidate_names_all, input_names_test,\n",
    "                                                weighted_actual_names_test, cluster_names, name_cluster)\n",
    "print(\"Total=\", total, \" Precision=\", precision, \" Recall=\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283ccee",
   "metadata": {},
   "source": [
    "### Write clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_clusters(path, cluster_names, name_freqs, name_nicks):\n",
    "    cluster_id_name_map = {}\n",
    "    with fopen(path, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "        for cluster_id, names in cluster_names.items():\n",
    "            # get most-frequent name\n",
    "            cluster_name = max(names, key=(lambda name: name_freqs.get(name, 0)))\n",
    "            # map cluster id to cluster name\n",
    "            cluster_id_name_map[cluster_id] = cluster_name\n",
    "            # add nicknames\n",
    "            nicknames = set()\n",
    "            if name_nicks:\n",
    "                for name in names:\n",
    "                    if name in name_nicks:\n",
    "                        nicknames.update(name_nicks[name])\n",
    "            names = names | nicknames\n",
    "            # write cluster\n",
    "            f.write(f'{cluster_name}\\t{\" \".join(names)}\\n')\n",
    "    return cluster_id_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id_name_map = write_clusters(clusters_filename, cluster_names, name_freqs, name_nicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b080d5",
   "metadata": {},
   "source": [
    "### Create super-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da077c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_cluster_names, name_super_cluster = compute_clusters(closure_ids, id_names, dist_matrix, cluster_linkage, \n",
    "                                                           super_cluster_distance_threshold, eps, max_dist)\n",
    "print(len(super_cluster_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_cluster_names, name_super_cluster = add_singleton_names(super_cluster_names, name_super_cluster, names_sample)\n",
    "print(len(super_cluster_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1514e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, total = get_precision_recall(names_sample, candidate_names_all, input_names_test, weighted_actual_names_test,\n",
    "                                                super_cluster_names, name_super_cluster)\n",
    "print(\"Total=\", total, \" Precision=\", precision, \" Recall=\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster names for each name in super cluster\n",
    "super_cluster_clusters = {id: set([cluster_id_name_map[name_cluster[name]] for name in names]) for id, names in super_cluster_names.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d71a2e",
   "metadata": {},
   "source": [
    "### Write super-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf9eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = write_clusters(super_clusters_filename, super_cluster_clusters, name_freqs, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f983d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
