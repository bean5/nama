{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139bee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028823c5",
   "metadata": {},
   "source": [
    "# Train a bi-encoder\n",
    "\n",
    "learn name-to-vec encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d9cdc",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Hyperparameters\n",
    "* epochs\n",
    "* embedding_dim\n",
    "\n",
    "\n",
    "new data @ 12 epochs:         ??? ??? ??, 189 7, 1011 193, 61728\n",
    "old data @ 12 epochs:         132 124 13,   1 0,  330 197, 56461\n",
    "old data w model @ 12 epochs: 145 110 76,   8 0,  482 246, 48875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0364890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.data.utils import read_csv\n",
    "from src.models.biencoder import BiEncoder\n",
    "from src.models.tokenizer import get_tokenize_function_and_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774de195",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_surname = \"given\"\n",
    "model_type = 'cecommon-0'\n",
    "\n",
    "checkpoint_path = '../data/models/bi_encoder-given-cecommon-0-4.state'\n",
    "path_epochs = [\n",
    "#     (f\"../data/processed/cross-encoder-augmented.csv\", 3),\n",
    "    (f\"../data/processed/cross-encoder-triplets-{given_surname}-0.csv\", 6),\n",
    "#     (f\"../data/processed/cross-encoder-triplets-{given_surname}-common.csv\", 6),\n",
    "#     (f\"../data/processed/tree-hr-{given_surname}-triplets-v2-1000-augmented.csv.gz\", 6),\n",
    "]\n",
    "\n",
    "# hyperparameters\n",
    "embedding_dim = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "use_amsgrad = False\n",
    "\n",
    "report_size = 10000\n",
    "max_tokens = 10\n",
    "vocab_type = 'f'  # tokenizer based upon training name frequency\n",
    "subword_vocab_size = 2000  # 500, 1000, 1500, 2000\n",
    "nama_bucket = 'nama-data'\n",
    "subwords_path=f\"../data/models/fs-{given_surname}-subword-tokenizer-{subword_vocab_size}{vocab_type}.json\"\n",
    "\n",
    "model_path = f\"../data/models/bi_encoder-{given_surname}-{model_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34474d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c030b7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47871b51",
   "metadata": {},
   "source": [
    "### Get tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize, tokenizer_vocab = get_tokenize_function_and_vocab(\n",
    "    max_tokens=max_tokens,\n",
    "    subwords_path=subwords_path,\n",
    "    nama_bucket=nama_bucket,\n",
    ")\n",
    "len(tokenizer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('dallan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181fba23",
   "metadata": {},
   "source": [
    "### Generate anchor-pos-neg triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of (anchor_tokens, pos_tokens, neg_tokens, target_margin)\n",
    "def generate_training_data(train_triplets_df):\n",
    "    all_data = []\n",
    "    for anchor, pos, neg, pos_score, neg_score in tqdm(zip(\n",
    "        train_triplets_df['anchor'],\n",
    "        train_triplets_df['positive'],\n",
    "        train_triplets_df['negative'],\n",
    "        train_triplets_df['positive_score'],\n",
    "        train_triplets_df['negative_score'],\n",
    "    ), mininterval=2):\n",
    "        anchor_tokens = tokenize(anchor)\n",
    "        pos_tokens = tokenize(pos)\n",
    "        neg_tokens = tokenize(neg)\n",
    "        target_margin = pos_score - neg_score\n",
    "        # anchor, positive, hard-negative\n",
    "        all_data.append((\n",
    "            anchor_tokens,\n",
    "            pos_tokens,\n",
    "            neg_tokens,\n",
    "            target_margin,\n",
    "        ))\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5e413",
   "metadata": {},
   "source": [
    "## Train bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbecdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(anchors, positives, negatives, labels):\n",
    "    # anchor_pos_sim = (anchors * positives).sum(dim=-1)\n",
    "    # anchor_neg_sim = (anchors * negatives).sum(dim=-1)\n",
    "    anchor_pos_sim = F.cosine_similarity(anchors, positives, dim=-1)\n",
    "    anchor_neg_sim = F.cosine_similarity(anchors, negatives, dim=-1)\n",
    "    margin_pred = anchor_pos_sim - anchor_neg_sim\n",
    "    return F.mse_loss(margin_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, model_path, verbose=True):\n",
    "    for epoch in range(num_epochs):\n",
    "        # make sure gradient tracking is on\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for ix, data in enumerate(train_loader):\n",
    "            # get batch\n",
    "            anchors, positives, negatives, target_margins = data\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            anchor_embeddings = model(anchors)  # Shape: (batch_size, embedding_dim)\n",
    "            pos_embeddings = model(positives)  # Shape: (batch_size, embedding_dim)\n",
    "            neg_embeddings = model(negatives)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(anchor_embeddings, pos_embeddings, neg_embeddings, target_margins)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate loss and report\n",
    "            if verbose:\n",
    "                running_loss += loss.item()\n",
    "                if ix % report_size == report_size - 1:\n",
    "                    avg_loss = running_loss / report_size  # loss per batch\n",
    "                    print(f\"Epoch {epoch} batch {ix} loss {avg_loss}\")\n",
    "                    running_loss = 0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # disable gradient computation\n",
    "        running_loss = 0\n",
    "        num_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                anchors, positives, negatives, target_margins = data\n",
    "                anchor_embeddings = model(anchors)  # Shape: (batch_size, embedding_dim)\n",
    "                pos_embeddings = model(positives)  # Shape: (batch_size, embedding_dim)\n",
    "                neg_embeddings = model(negatives)  # Shape: (batch_size, embedding_dim)\n",
    "                loss = loss_fn(anchor_embeddings, pos_embeddings, neg_embeddings, target_margins)\n",
    "                running_loss += loss.item()  \n",
    "                num_val_batches += 1\n",
    "\n",
    "        # calculate average validation loss\n",
    "        val_loss = running_loss / num_val_batches\n",
    "        if verbose:\n",
    "            print(f\"VALIDATION: Epoch {epoch} loss {val_loss}\")\n",
    "        # save model state + model\n",
    "        epoch_model_path = f\"{model_path}-{epoch}\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, epoch_model_path+\".state\")\n",
    "        torch.save(model, epoch_model_path+\".pth\")\n",
    "        \n",
    "    # return final epoch validation loss\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fefd66",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b2c2b4e",
   "metadata": {},
   "source": [
    "def hyperopt_objective_function(train_data, \n",
    "                                val_data, \n",
    "                                vocab_size,\n",
    "                                max_tokens,\n",
    "                                pad_token,\n",
    "                                verbose=True,\n",
    "                               ):\n",
    "    \n",
    "    def objective(config):\n",
    "        learning_rate = config['learning_rate']\n",
    "        batch_size = config['batch_size']\n",
    "        embedding_dim = config['embedding_dim']\n",
    "        num_epochs = config['num_epochs']\n",
    "        \n",
    "        if verbose:\n",
    "            print('train', config)\n",
    "        \n",
    "        # Create an instance of the bi-encoder model\n",
    "        model = BiEncoder(embedding_dim, vocab_size, max_tokens, pad_token)\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # model.to(device)\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=use_amsgrad)\n",
    "\n",
    "        # Create data loader\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "        val_loss = train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, verbose=False)\n",
    "        if verbose:\n",
    "            print('val_loss', val_loss)\n",
    "        \n",
    "        return {\n",
    "            'status': STATUS_OK,\n",
    "            'loss': val_loss,\n",
    "            'config': config,            \n",
    "        }\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ccb3a65",
   "metadata": {},
   "source": [
    "# HyperOpt search space\n",
    "search_space = {\n",
    "    \"learning_rate\": hp.loguniform('learning_rate', math.log(1e-4), math.log(1e-2)),\n",
    "    \"batch_size\": hp.choice('batch_size', [8,16,32,64]),\n",
    "    \"embedding_dim\": hp.choice('embedding_dim', [8,16,32,64]),\n",
    "    \"num_epochs\": hp.choice('num_epochs', [5,10,20,40]),\n",
    "}\n",
    "objective = hyperopt_objective_function(train_data=train_data,\n",
    "                                        val_data=val_data,\n",
    "                                        vocab_size=len(tokenizer_vocab),\n",
    "                                        max_tokens=max_tokens,\n",
    "                                        pad_token=tokenizer_vocab['[PAD]'],\n",
    "                                        verbose=True,\n",
    "                                       )\n",
    "trials = Trials()\n",
    "\n",
    "# minimize the objective over the space\n",
    "best = fmin(objective, \n",
    "            search_space, \n",
    "            algo=tpe.suggest, \n",
    "            trials=trials,\n",
    "            max_evals=50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a278ccb0",
   "metadata": {},
   "source": [
    "print(\"best\", best)\n",
    "print(\"results\", trials.results) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a735dc7",
   "metadata": {},
   "source": [
    "batch_size = best_result.config['batch_size']\n",
    "learning_rate = best_result.config['learning_rate']\n",
    "embedding_dim = best_result.config['embedding_dim']\n",
    "num_epochs = best_result.config['num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba77e31",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the bi-encoder model\n",
    "model = BiEncoder(embedding_dim, len(tokenizer_vocab), max_tokens, tokenizer_vocab['[PAD]'])\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=use_amsgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea023d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(batch):\n",
    "    # Transpose the batch (list of tuples) to a tuple of lists\n",
    "    transposed_batch = list(zip(*batch))\n",
    "    # Convert each list in the tuple to a tensor\n",
    "    tensor_batch = tuple(torch.tensor(x) for x in transposed_batch)\n",
    "    return tensor_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce0837",
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint_path:\n",
    "    print(checkpoint_path)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ix, (path, num_epochs) in enumerate(path_epochs):\n",
    "    print(path, num_epochs)\n",
    "    train_triplets_df = read_csv(path)\n",
    "    all_data = generate_training_data(train_triplets_df)\n",
    "    del train_triplets_df\n",
    "    train_data, val_data = train_test_split(all_data, test_size=0.01, random_state=42)\n",
    "    del all_data\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
    "    train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, f\"{model_path}-{ix}\")\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    del train_data\n",
    "    del val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5985e2a",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47966391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.save(model, model_path+\".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51ee5ba",
   "metadata": {},
   "source": [
    "## Misc - Combine triplets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0cb411a",
   "metadata": {},
   "source": [
    "df1 = read_csv(f\"../data/processed/cross-encoder-triplets-{given_surname}-0.csv\")\n",
    "df2 = read_csv(f\"../data/processed/tree-hr-{given_surname}-triplets-v2-1000-augmented.csv.gz\")\n",
    "df = pd.concat([df1, df2])\n",
    "df.to_csv(f\"../data/processed/cross-encoder-augmented.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192dedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
