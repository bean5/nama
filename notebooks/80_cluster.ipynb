{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.data.filesystem import fopen\n",
    "from src.data.utils import load_train_test\n",
    "from src.data.prepare import normalize\n",
    "from src.eval.encoder import eval_encoder\n",
    "from src.models.utils import (\n",
    "    add_padding,\n",
    "    remove_padding,\n",
    "    build_token_idx_maps,\n",
    "    convert_names_to_model_inputs,\n",
    "    get_best_matches,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342e51b",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 0\n",
    "max_closure_size = 10000\n",
    "max_distance = 0.22\n",
    "cluster_distance_threshold = 0.155\n",
    "super_cluster_distance_threshold = 0.205\n",
    "num_candidates = 2000\n",
    "eps = 0.000001\n",
    "model_filename = \"../data/models/anc-triplet-bilstm-100-512-40-05.pth\"\n",
    "\n",
    "# process_nicknames = True\n",
    "# werelate_names_filename = 'givenname_similar_names.werelate.20210414.tsv'\n",
    "# nicknames_filename = '../data/models/givenname_nicknames.txt'\n",
    "# name_freqs_filename = 'given-final.normal.txt'\n",
    "# clusters_filename = 'givenname_clusters.tsv'\n",
    "# super_clusters_filename = 'givenname_super_clusters.tsv'\n",
    "\n",
    "werelate_names_filename = \"../data/external/surname_similar_names.werelate.20210414.tsv\"\n",
    "nicknames_filename = \"\"\n",
    "name_freqs_filename = \"../data/external/surname-final.normal.txt\"\n",
    "clusters_filename = \"../data/models/surname_clusters.tsv\"\n",
    "super_clusters_filename = \"../data/models/surname_super_clusters.tsv\"\n",
    "is_surname = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9fb37",
   "metadata": {},
   "source": [
    "### Read WeRelate names into all_names\n",
    "Later, we'll want to read frequent FS names into all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fdfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rewrite this in just a few lines using pandas\n",
    "def load_werelate_names(path, is_surname):\n",
    "    name_variants = defaultdict(set)\n",
    "    with fopen(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        is_header = True\n",
    "        for line in f:\n",
    "            if is_header:\n",
    "                is_header = False\n",
    "                continue\n",
    "            fields = line.rstrip().split(\"\\t\")\n",
    "            # normalize should only return a single name piece, but loop just in case\n",
    "            for name_piece in normalize(fields[0], is_surname):\n",
    "                confirmed_variants = fields[1].strip().split(\" \") if len(fields) >= 2 else []\n",
    "                computer_variants = fields[2].strip().split(\" \") if len(fields) == 3 else []\n",
    "                variants = confirmed_variants + computer_variants\n",
    "                for variant in variants:\n",
    "                    for variant_piece in normalize(variant, is_surname):\n",
    "                        name_variants[name_piece].add(variant_piece)\n",
    "    return name_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77410cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = set()\n",
    "\n",
    "name_variants = load_werelate_names(werelate_names_filename, is_surname)\n",
    "print(len(name_variants))\n",
    "for k, v in name_variants.items():\n",
    "    all_names.add(add_padding(k))\n",
    "    all_names.update(add_padding(variant) for variant in v)\n",
    "print(len(all_names), next(iter(all_names)))\n",
    "\n",
    "name_variants = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c003ee2b",
   "metadata": {},
   "source": [
    "### Read nicknames and remove from names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fbfd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nicknames(path):\n",
    "    nicknames = defaultdict(set)\n",
    "    with fopen(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            names = line.rstrip().split(\" \")\n",
    "            # normalize should only return a single name piece, but loop just in case\n",
    "            for name_piece in normalize(names[0], False):\n",
    "                orig_name = add_padding(name_piece)\n",
    "                for nickname in names[1:]:\n",
    "                    for nickname_piece in normalize(nickname, False):\n",
    "                        nicknames[add_padding(nickname_piece)].add(orig_name)\n",
    "    return nicknames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ac9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_surname:\n",
    "    nick_names = load_nicknames(nicknames_filename)\n",
    "    name_nicks = defaultdict(set)\n",
    "    for nick, names in nick_names.items():\n",
    "        for name in names:\n",
    "            name_nicks[name].add(nick)\n",
    "    print(next(iter(nick_names.items())), \"nick_names\", len(nick_names.keys()), \"name_nicks\", len(name_nicks.keys()))\n",
    "    all_names -= set(nickname for nickname in nick_names.keys())\n",
    "    print(len(all_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c1a12",
   "metadata": {},
   "source": [
    "### Map names to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_names_to_ids(names):\n",
    "    ids = range(len(names))\n",
    "    return dict(zip(names, ids)), dict(zip(ids, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ids, id_names = map_names_to_ids(all_names)\n",
    "print(next(iter(name_ids.items())), next(iter(id_names.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420e8af",
   "metadata": {},
   "source": [
    "### Read name frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rewrite this using pandas too\n",
    "def load_name_freqs(path, is_surname):\n",
    "    name_freqs = defaultdict(int)\n",
    "    with fopen(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            fields = line.rstrip().split(\"\\t\")\n",
    "            for name_piece in normalize(fields[0], is_surname):\n",
    "                name_freqs[name_piece] = int(fields[1])\n",
    "    return name_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_freqs = load_name_freqs(name_freqs_filename, is_surname)\n",
    "# keep only entries in all_names\n",
    "name_freqs = dict((add_padding(k), v) for k, v in name_freqs.items() if add_padding(k) in all_names)\n",
    "print(len(name_freqs), next(iter(name_freqs.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce469ea0",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3474b0e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412cd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_filename, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c9b30",
   "metadata": {},
   "source": [
    "### Encode names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f88ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NAME_LENGTH = 30\n",
    "char_to_idx_map, idx_to_char_map = build_token_idx_maps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de5854",
   "metadata": {},
   "source": [
    "#### Take a sample because encoded names require a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_size <= 0 or sample_size >= len(all_names):\n",
    "    names_sample = np.array(list(all_names))\n",
    "else:\n",
    "    names_sample = np.array(random.sample(all_names, sample_size))\n",
    "print(names_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef17470",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Compute encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f38f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "names_tensor, _ = convert_names_to_model_inputs(names_sample, char_to_idx_map, MAX_NAME_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6b770",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get encodings for the names from the encoder\n",
    "names_encoded = eval_encoder(model, names_tensor, 1024)\n",
    "names_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab5236",
   "metadata": {},
   "source": [
    "### Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc8904",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This takes an hour for 190k\n",
    "# names_candidates = (names_sample, candidates, (name text, score))\n",
    "name_candidates = get_best_matches(\n",
    "    names_encoded, names_encoded, names_sample, num_candidates=num_candidates, metric=\"euclidean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, repeat each name num_candidates times and convert the 1D array to a 2D array\n",
    "# next, reshape name_candidates to be a 2D array where the columns are (candidate, score)\n",
    "# finally, stack the two arrays into a 2D array where the columns are (name, candidate, score)\n",
    "distances = np.hstack((np.repeat(names_sample, num_candidates)[:, np.newaxis], name_candidates.reshape(-1, 2)))\n",
    "# remove distances > max_distance\n",
    "distances = distances[distances[:, -1].astype(\"float\") <= max_distance]\n",
    "# sort\n",
    "distances = distances[distances[:, -1].astype(\"float\").argsort()]\n",
    "print(distances.shape)\n",
    "name_candidates = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cca22",
   "metadata": {},
   "source": [
    "### Compute closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb030bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all distances, create closures and save scores\n",
    "next_closure = 0\n",
    "closure_ids = {}\n",
    "id_closure = {}\n",
    "row_ixs = []\n",
    "col_ixs = []\n",
    "dists = []\n",
    "max_size = 0\n",
    "\n",
    "for row in tqdm(distances):\n",
    "    name1 = row[0]\n",
    "    name2 = row[1]\n",
    "    id1 = name_ids[name1]\n",
    "    id2 = name_ids[name2]\n",
    "    # each distance is in distances twice\n",
    "    if id1 > id2:\n",
    "        continue\n",
    "    distance = max(eps, float(row[2]))\n",
    "    closure1 = id_closure.get(id1)\n",
    "    closure2 = id_closure.get(id2)\n",
    "    if closure1 is None and closure2 is not None:\n",
    "        id1, id2 = id2, id1\n",
    "        name1, name2 = name2, name1\n",
    "        closure1, closure2 = closure2, closure1\n",
    "    # add to distance matrix\n",
    "    row_ixs.append(id1)\n",
    "    col_ixs.append(id2)\n",
    "    dists.append(distance)\n",
    "    # skip if names are the same\n",
    "    if id1 == id2:\n",
    "        continue\n",
    "    row_ixs.append(id2)\n",
    "    col_ixs.append(id1)\n",
    "    dists.append(distance)\n",
    "    # create closures\n",
    "    if closure1 is None:\n",
    "        # if closure1 is None, then closure2 must be none also due to the above\n",
    "        # so create a new closure with id1 and id2\n",
    "        closure1 = next_closure\n",
    "        next_closure += 1\n",
    "        id_closure[id1] = closure1\n",
    "        id_closure[id2] = closure1\n",
    "        closure_ids[closure1] = [id1, id2]\n",
    "        next_closure += 1\n",
    "    elif closure2 is None:\n",
    "        # put id2 into id1's closure\n",
    "        id_closure[id2] = closure1\n",
    "        closure_ids[closure1].append(id2)\n",
    "    elif closure1 != closure2 and len(closure_ids[closure1]) + len(closure_ids[closure2]) <= max_closure_size:\n",
    "        # move all ids in closure2 into closure1\n",
    "        for id in closure_ids[closure2]:\n",
    "            id_closure[id] = closure1\n",
    "            closure_ids[closure1].append(id)\n",
    "        del closure_ids[closure2]\n",
    "    if len(closure_ids[closure1]) > max_size:\n",
    "        max_size = len(closure_ids[closure1])\n",
    "\n",
    "# create distances matrix\n",
    "dist_matrix = csr_matrix((dists, (row_ixs, col_ixs)))\n",
    "\n",
    "print(\"max closure_size\", max_size)\n",
    "print(\"number of closures\", len(closure_ids), \"number of names enclosed\", len(id_closure))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466050e3",
   "metadata": {},
   "source": [
    "### Compute clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b512694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clusters(closure_ids, id_names, dist_matrix, linkage, distance_threshold, eps, max_dist):\n",
    "    cluster_names = defaultdict(set)\n",
    "    name_cluster = {}\n",
    "    for closure, ids in tqdm(closure_ids.items()):\n",
    "        clusterer = AgglomerativeClustering(\n",
    "            n_clusters=None, affinity=\"precomputed\", linkage=linkage, distance_threshold=distance_threshold\n",
    "        )\n",
    "        X = dist_matrix[ids][:, ids].todense()\n",
    "        X[X < eps] = max_dist\n",
    "        labels = clusterer.fit_predict(X)\n",
    "        for id, label in zip(ids, labels):\n",
    "            name = id_names[id]\n",
    "            cluster = f\"{closure}_{label}\"\n",
    "            cluster_names[cluster].add(name)\n",
    "            name_cluster[name] = cluster\n",
    "    return cluster_names, name_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8003404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try ward, average, single\n",
    "cluster_linkage = \"average\"\n",
    "max_dist = 10.0\n",
    "\n",
    "cluster_names, name_cluster = compute_clusters(\n",
    "    closure_ids, id_names, dist_matrix, cluster_linkage, cluster_distance_threshold, eps, max_dist\n",
    ")\n",
    "print(len(cluster_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ad939",
   "metadata": {},
   "source": [
    "#### Add unclustered names as singleton clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_singleton_names(cluster_names, name_cluster, names_sample):\n",
    "    for ix, name in enumerate(names_sample):\n",
    "        if name not in name_cluster:\n",
    "            cluster = f\"{ix}\"\n",
    "            cluster_names[cluster].add(name)\n",
    "            name_cluster[name] = cluster\n",
    "    return cluster_names, name_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ca043",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names, name_cluster = add_singleton_names(cluster_names, name_cluster, names_sample)\n",
    "print(len(cluster_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2c80d",
   "metadata": {},
   "source": [
    "### Eval cluster P/R over Ancestry test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_train_test(\"../data/raw/records25k_data_train.csv\", \"../data/raw/records25k_data_test.csv\")\n",
    "\n",
    "_, _, candidates_train = train\n",
    "input_names_test, weighted_actual_names_test, candidates_test = test\n",
    "\n",
    "all_candidates = np.concatenate((candidates_train, candidates_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b534952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(\n",
    "    names_sample, all_candidates, input_names_test, weighted_actual_names_test, cluster_names, name_cluster\n",
    "):\n",
    "    names_sample_set = set(names_sample.tolist())\n",
    "    all_candidates_set = set(all_candidates.tolist())\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    missing = set()\n",
    "    # TODO if the input_name is not in the names_sample_set, consider looking up nearest neighbor\n",
    "    for input_name, weighted_actual_names in zip(input_names_test, weighted_actual_names_test):\n",
    "        if input_name not in names_sample_set:\n",
    "            missing.add(input_name)\n",
    "            continue\n",
    "        cluster_id = name_cluster[input_name]\n",
    "        names_in_cluster = cluster_names[cluster_id] & all_candidates_set\n",
    "        found_recall = 0.0\n",
    "        total_recall = 0.0\n",
    "        found_count = 0\n",
    "        for name, weight, _ in weighted_actual_names:\n",
    "            if name not in names_sample_set:\n",
    "                missing.add(name)\n",
    "                continue\n",
    "            total_recall += weight\n",
    "            if name in names_in_cluster:\n",
    "                found_recall += weight\n",
    "                found_count += 1\n",
    "        if total_recall == 0.0:\n",
    "            continue\n",
    "        precision = found_count / len(names_in_cluster) if len(names_in_cluster) > 0 else 1.0\n",
    "        recall = found_recall / total_recall\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    return avg_precision, avg_recall, len(precisions), len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, total, missing = get_precision_recall(\n",
    "    names_sample, all_candidates, input_names_test, weighted_actual_names_test, cluster_names, name_cluster\n",
    ")\n",
    "print(f\"Total={total} Precision={precision} Recall={recall} Missing={missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192261d",
   "metadata": {},
   "source": [
    "### Write clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_clusters(path, cluster_names, name_freqs, name_nicks):\n",
    "    cluster_id_name_map = {}\n",
    "    with fopen(path, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "        for cluster_id, names in cluster_names.items():\n",
    "            # get most-frequent name\n",
    "            cluster_name = max(names, key=(lambda name: name_freqs.get(name, 0)))\n",
    "            # map cluster id to cluster name\n",
    "            cluster_id_name_map[cluster_id] = cluster_name\n",
    "            # add nicknames\n",
    "            nicknames = set()\n",
    "            if name_nicks:\n",
    "                for name in names:\n",
    "                    if name in name_nicks:\n",
    "                        nicknames.update(name_nicks[name])\n",
    "            # remove padding\n",
    "            cluster_name = remove_padding(cluster_name)\n",
    "            names = [remove_padding(name) for name in names | nicknames]\n",
    "            # write cluster\n",
    "            f.write(f'{cluster_name}\\t{\" \".join(names)}\\n')\n",
    "    return cluster_id_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id_name_map = write_clusters(clusters_filename, cluster_names, name_freqs, name_nicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808019a1",
   "metadata": {},
   "source": [
    "### Create super-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276556b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_cluster_names, name_super_cluster = compute_clusters(\n",
    "    closure_ids, id_names, dist_matrix, cluster_linkage, super_cluster_distance_threshold, eps, max_dist\n",
    ")\n",
    "print(len(super_cluster_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_cluster_names, name_super_cluster = add_singleton_names(super_cluster_names, name_super_cluster, names_sample)\n",
    "print(len(super_cluster_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, total = get_precision_recall(\n",
    "    names_sample, all_candidates, input_names_test, weighted_actual_names_test, super_cluster_names, name_super_cluster\n",
    ")\n",
    "print(\"Total=\", total, \" Precision=\", precision, \" Recall=\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631413ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster names for each name in super cluster\n",
    "super_cluster_clusters = {\n",
    "    id: set([cluster_id_name_map[name_cluster[name]] for name in names]) for id, names in super_cluster_names.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36075a78",
   "metadata": {},
   "source": [
    "### Write super-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4df25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = write_clusters(super_clusters_filename, super_cluster_clusters, name_freqs, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28f317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
