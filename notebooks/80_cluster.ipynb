{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from tqdm import tqdm\n",
    "from matchers import dataset, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-international",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 0\n",
    "max_closure_size = 10000\n",
    "max_distance = 0.22\n",
    "cluster_distance_threshold = 0.155\n",
    "super_cluster_distance_threshold = 0.205\n",
    "num_candidates = 1000\n",
    "eps = 0.000001\n",
    "model_filename = 'ae-bilstm-100-ancestry-40-05.pth'\n",
    "\n",
    "s3 = None\n",
    "# s3 = s3fs.S3FileSystem(anon=False) # mount\n",
    "\n",
    "# process_nicknames = True\n",
    "# werelate_names_filename = 'givenname_similar_names.werelate.20210414.tsv'\n",
    "# name_freqs_filename = 'given-final.normal.txt'\n",
    "# clusters_filename = 'givenname_clusters.tsv'\n",
    "# super_clusters_filename = 'givenname_super_clusters.tsv'\n",
    "\n",
    "process_nicknames = False\n",
    "werelate_names_filename = 'surname_similar_names.werelate.20210414.tsv'\n",
    "name_freqs_filename = 'surname-final.normal.txt'\n",
    "clusters_filename = 'surname_clusters.tsv'\n",
    "super_clusters_filename = 'surname_super_clusters.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_open(filename, mode, encoding):\n",
    "    if s3:\n",
    "        return s3.open(f's3://private.werelate.org/names/{filename}', mode=mode, encoding=encoding)\n",
    "    else:\n",
    "        return open(f'../data/{filename}', mode=mode, encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-delta",
   "metadata": {},
   "source": [
    "### Read WeRelate names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_werelate_names(filename):\n",
    "    name_variants = {}\n",
    "    with my_open(filename, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        is_header = True\n",
    "        for line in f:\n",
    "            if is_header:\n",
    "                is_header = False\n",
    "                continue\n",
    "            fields = line.rstrip().split(\"\\t\")\n",
    "            name = utils.normalize(fields[0])\n",
    "            if not name:\n",
    "                continue\n",
    "            name_variants[name] = set()\n",
    "            confirmed_variants = fields[1].strip().split(\" \") if len(fields) >= 2 else []\n",
    "            computer_variants = fields[2].strip().split(\" \") if len(fields) == 3 else []\n",
    "            variants = confirmed_variants + computer_variants\n",
    "            for variant in variants:\n",
    "                variant = utils.normalize(variant)\n",
    "                if not variant:\n",
    "                    continue\n",
    "                name_variants[name].add(variant)\n",
    "    return name_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = set()\n",
    "\n",
    "name_variants = load_werelate_names(werelate_names_filename)\n",
    "print(len(name_variants))\n",
    "for k, v in name_variants.items():\n",
    "    all_names.add(utils.add_padding(k))\n",
    "    all_names.update(utils.add_padding(variant) for variant in v)\n",
    "print(len(all_names), next(iter(all_names)))\n",
    "\n",
    "name_variants = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-cooler",
   "metadata": {},
   "source": [
    "### Read nicknames and remove from names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nicknames(filename):\n",
    "    nicknames = defaultdict(set)\n",
    "    with my_open(filename, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            names = line.rstrip().split(\" \")\n",
    "            orig_name = utils.add_padding(utils.normalize(names[0]))\n",
    "            for nickname in names[1:]:\n",
    "                nicknames[utils.add_padding(utils.normalize(nickname))].add(orig_name)\n",
    "    return nicknames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "if process_nicknames:\n",
    "    nick_names = load_nicknames('../data/givenname_nicknames.txt')\n",
    "    name_nicks = defaultdict(set)\n",
    "    for nick, names in nick_names.items():\n",
    "        for name in names:\n",
    "            name_nicks[name].add(nick)\n",
    "    print(next(iter(nick_names.items())), \"nick_names\", len(nick_names.keys()), \"name_nicks\", len(name_nicks.keys()))\n",
    "    all_names -= set(nickname for nickname in nick_names.keys())\n",
    "    print(len(all_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-emerald",
   "metadata": {},
   "source": [
    "### Map names to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_names_to_ids(names):\n",
    "    ids = range(len(names))\n",
    "    return dict(zip(names, ids)), dict(zip(ids, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ids, id_names = map_names_to_ids(all_names)\n",
    "print(next(iter(name_ids.items())), next(iter(id_names.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-knitting",
   "metadata": {},
   "source": [
    "### Read name frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_name_freqs(filename):\n",
    "    name_freqs = defaultdict(int)\n",
    "    with my_open(filename, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            fields = line.rstrip().split(\"\\t\")\n",
    "            name_freqs[utils.normalize(fields[0])] = int(fields[1])\n",
    "    return name_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_freqs = load_name_freqs(name_freqs_filename)\n",
    "# keep only entries in all_names\n",
    "name_freqs = dict((utils.add_padding(k),v) for k,v in name_freqs.items() if utils.add_padding(k) in all_names)\n",
    "print(len(name_freqs), next(iter(name_freqs.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-glucose",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "if s3:\n",
    "    local_filename = model_filename\n",
    "    s3.get(f's3://private.werelate.org/names/{model_filename}', local_filename)\n",
    "else:\n",
    "    local_filename = f'../data/{model_filename}'\n",
    "model = torch.load(local_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-favorite",
   "metadata": {},
   "source": [
    "### Encode names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NAME_LENGTH=30\n",
    "char_to_idx_map, idx_to_char_map = utils.build_token_idx_maps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-boutique",
   "metadata": {},
   "source": [
    "#### Take a sample because encoded names require a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_size <= 0 or sample_size >= len(all_names):\n",
    "    names_sample = np.array(list(all_names))\n",
    "else:\n",
    "    names_sample = np.array(random.sample(all_names, sample_size))\n",
    "print(names_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "names_tensor, _ = utils.convert_names_to_model_inputs(names_sample,\n",
    "                                                   char_to_idx_map, \n",
    "                                                   MAX_NAME_LENGTH)\n",
    "# Get Embeddings for the names from the encoder\n",
    "names_encoded = model(names_tensor, just_encoder=True).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-maker",
   "metadata": {},
   "source": [
    "### Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_candidates = utils.get_candidates_batch(names_encoded, \n",
    "                                             names_encoded, \n",
    "                                             names_sample,\n",
    "                                             num_candidates=num_candidates,\n",
    "                                             metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.hstack((np.repeat(names_sample, num_candidates)[:, np.newaxis], name_candidates.reshape(-1,2)))\n",
    "# remove distances > max_distance\n",
    "distances = distances[distances[:, -1].astype('float') <= max_distance]\n",
    "# sort \n",
    "distances = distances[distances[:, -1].astype('float').argsort()]\n",
    "print(distances.shape)\n",
    "name_candidates = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-payment",
   "metadata": {},
   "source": [
    "### Compute closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all distances, create closures and save scores\n",
    "next_closure = 0\n",
    "closure_ids = {}\n",
    "id_closure = {}\n",
    "row_ixs = []\n",
    "col_ixs = []\n",
    "dists = []\n",
    "max_size = 0\n",
    "\n",
    "for row in tqdm(distances):\n",
    "    name1 = row[0]\n",
    "    name2 = row[1]\n",
    "    id1 = name_ids[name1]\n",
    "    id2 = name_ids[name2]\n",
    "    # each distance is in distances twice\n",
    "    if id1 > id2:\n",
    "        continue\n",
    "    distance = max(eps, float(row[2]))\n",
    "    closure1 = id_closure.get(id1)\n",
    "    closure2 = id_closure.get(id2)\n",
    "    if closure1 is None and closure2 is not None:        \n",
    "        id1, id2 = id2, id1\n",
    "        name1, name2 = name2, name1\n",
    "        closure1, closure2 = closure2, closure1\n",
    "    # add to distance matrix\n",
    "    row_ixs.append(id1)\n",
    "    col_ixs.append(id2)\n",
    "    dists.append(distance)\n",
    "    # skip if names are the same\n",
    "    if id1 == id2:\n",
    "        continue\n",
    "    row_ixs.append(id2)\n",
    "    col_ixs.append(id1)\n",
    "    dists.append(distance)\n",
    "    # create closures\n",
    "    if closure1 is None:\n",
    "        # if closure1 is None, then closure2 must be none also due to the above\n",
    "        # so create a new closure with id1 and id2\n",
    "        closure1 = next_closure\n",
    "        next_closure += 1\n",
    "        id_closure[id1] = closure1\n",
    "        id_closure[id2] = closure1\n",
    "        closure_ids[closure1] = [id1, id2]\n",
    "        next_closure += 1\n",
    "    elif closure2 is None:\n",
    "        # put id2 into id1's closure\n",
    "        id_closure[id2] = closure1\n",
    "        closure_ids[closure1].append(id2)\n",
    "    elif closure1 != closure2 and len(closure_ids[closure1]) + len(closure_ids[closure2]) <= max_closure_size:\n",
    "        # move all ids in closure2 into closure1\n",
    "        for id in closure_ids[closure2]:\n",
    "            id_closure[id] = closure1\n",
    "            closure_ids[closure1].append(id)\n",
    "        del closure_ids[closure2]\n",
    "    if len(closure_ids[closure1]) > max_size:\n",
    "        max_size = len(closure_ids[closure1])\n",
    "\n",
    "# create distances matrix\n",
    "dist_matrix = csr_matrix((dists, (row_ixs, col_ixs)))\n",
    "\n",
    "print(\"max closure_size\", max_size)\n",
    "print(\"number of closures\", len(closure_ids), \"number of names enclosed\", len(id_closure))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-physiology",
   "metadata": {},
   "source": [
    "### Compute clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clusters(closure_ids, id_names, dist_matrix, linkage, distance_threshold, eps, max_dist):\n",
    "    cluster_names = defaultdict(set)\n",
    "    name_cluster = {}\n",
    "    for closure, ids in tqdm(closure_ids.items()):\n",
    "        clusterer = AgglomerativeClustering(n_clusters=None, affinity='precomputed', linkage=linkage, distance_threshold=distance_threshold)\n",
    "        X = dist_matrix[ids][:, ids].todense()\n",
    "        X[X < eps] = max_dist\n",
    "        labels = clusterer.fit_predict(X)\n",
    "        for id, label in zip(ids, labels):\n",
    "            name = id_names[id]\n",
    "            cluster = f'{closure}_{label}'\n",
    "            cluster_names[cluster].add(name)\n",
    "            name_cluster[name] = cluster\n",
    "    return cluster_names, name_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try ward, average, single\n",
    "cluster_linkage = 'average'\n",
    "max_dist = 10.0\n",
    "\n",
    "cluster_names, name_cluster = compute_clusters(closure_ids, id_names, dist_matrix, cluster_linkage, cluster_distance_threshold, eps, max_dist)\n",
    "print(len(cluster_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-alias",
   "metadata": {},
   "source": [
    "#### Add unclustered names as singleton clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_singleton_names(cluster_names, name_cluster, names_sample):\n",
    "    for ix, name in enumerate(names_sample):\n",
    "        if name not in name_cluster:\n",
    "            cluster = f'{ix}'\n",
    "            cluster_names[cluster].add(name)\n",
    "            name_cluster[name] = cluster\n",
    "    return cluster_names, name_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names, name_cluster = add_singleton_names(cluster_names, name_cluster, names_sample)\n",
    "print(len(cluster_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-broadcasting",
   "metadata": {},
   "source": [
    "### Eval cluster P/R over Ancestry test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dataset.load_process_from_disk()\n",
    "\n",
    "_, _, candidates_train = train\n",
    "input_names_test, weighted_relevant_names_test, candidates_test = test\n",
    "\n",
    "all_candidates = np.concatenate((candidates_train, candidates_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(names_sample, all_candidates, input_names_test, weighted_relevant_names_test, cluster_names, name_cluster):\n",
    "    names_sample_set = set(names_sample.tolist())\n",
    "    all_candidates_set = set(all_candidates.tolist())\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for input_name, weighted_relevant_names in zip(input_names_test, weighted_relevant_names_test):\n",
    "        if input_name not in names_sample_set:\n",
    "            continue\n",
    "        cluster_id = name_cluster[input_name]\n",
    "        names_in_cluster = cluster_names[cluster_id] & all_candidates_set\n",
    "        found_recall = 0.0\n",
    "        total_recall = 0.0\n",
    "        found_count = 0\n",
    "        for name, weight, _ in weighted_relevant_names:\n",
    "            if name in names_sample_set:\n",
    "                total_recall += weight\n",
    "            if name in names_in_cluster:\n",
    "                found_recall += weight\n",
    "                found_count += 1\n",
    "        if total_recall == 0.0:\n",
    "            continue\n",
    "        precision = found_count / len(names_in_cluster) if len(names_in_cluster) > 0 else 1.0\n",
    "        recall = found_recall / total_recall\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    return avg_precision, avg_recall, len(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, total = get_precision_recall(names_sample, all_candidates, input_names_test, weighted_relevant_names_test, \n",
    "                                                cluster_names, name_cluster)\n",
    "print(\"Total=\", total, \" Precision=\", precision, \" Recall=\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-spine",
   "metadata": {},
   "source": [
    "### Write clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_clusters(filename, cluster_names, name_freqs, name_nicks):\n",
    "    cluster_id_name_map = {}\n",
    "    with my_open(filename, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "        for cluster_id, names in cluster_names.items():\n",
    "            # get most-frequent name\n",
    "            cluster_name = max(names, key=(lambda name: name_freqs.get(name, 0)))\n",
    "            # map cluster id to cluster name\n",
    "            cluster_id_name_map[cluster_id] = cluster_name\n",
    "            # add nicknames\n",
    "            nicknames = set()\n",
    "            if name_nicks:\n",
    "                for name in names:\n",
    "                    if name in name_nicks:\n",
    "                        nicknames.update(name_nicks[name])\n",
    "            # remove padding        \n",
    "            cluster_name = utils.remove_padding(cluster_name)\n",
    "            names = [utils.remove_padding(name) for name in names | nicknames]\n",
    "            # write cluster\n",
    "            f.write(f'{cluster_name}\\t{\" \".join(names)}\\n')\n",
    "    return cluster_id_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id_name_map = write_clusters(clusters_filename, cluster_names, name_freqs, name_nicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-student",
   "metadata": {},
   "source": [
    "### Create super-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_cluster_names, name_super_cluster = compute_clusters(closure_ids, id_names, dist_matrix, cluster_linkage, \n",
    "                                                           super_cluster_distance_threshold, eps, max_dist)\n",
    "print(len(super_cluster_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_cluster_names, name_super_cluster = add_singleton_names(super_cluster_names, name_super_cluster, names_sample)\n",
    "print(len(super_cluster_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, total = get_precision_recall(names_sample, all_candidates, input_names_test, weighted_relevant_names_test, \n",
    "                                                super_cluster_names, name_super_cluster)\n",
    "print(\"Total=\", total, \" Precision=\", precision, \" Recall=\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster names for each name in super cluster\n",
    "super_cluster_clusters = {id: set([cluster_id_name_map[name_cluster[name]] for name in names]) for id, names in super_cluster_names.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-intent",
   "metadata": {},
   "source": [
    "### Write super-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = write_clusters(super_clusters_filename, super_cluster_clusters, name_freqs, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-supplier",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:name-matching] *",
   "language": "python",
   "name": "conda-env-name-matching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
