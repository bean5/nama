{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import wandb\n",
    "\n",
    "from src.data.filesystem import fopen\n",
    "from src.data.utils import load_dataset\n",
    "from src.eval import metrics\n",
    "from src.models.swivel import SwivelDataset, SwivelModel, train_swivel, get_best_swivel_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "given_surname = \"given\"\n",
    "vocab_size = 610000 if given_surname == \"given\" else 2100000\n",
    "embed_dim = 100\n",
    "n_epochs = 200\n",
    "num_matches = 500\n",
    "Config = namedtuple(\"Config\", \"train_path eval_path vocab_size embed_dim confidence_base confidence_scale confidence_exponent n_epochs submatrix_size lr vocab_path model_path\")\n",
    "config = Config(\n",
    "    train_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train-augmented.csv.gz\",\n",
    "    eval_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train.csv.gz\",\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    confidence_base=0.18 if given_surname == \"given\" else 0.14,\n",
    "    confidence_scale=0.5 if given_surname == \"given\" else 0.45,\n",
    "    confidence_exponent=0.3 if given_surname == \"given\" else 0.3,\n",
    "    lr = 0.14 if given_surname == \"given\" else 0.24,\n",
    "    n_epochs = n_epochs,\n",
    "    submatrix_size = 4096,\n",
    "    vocab_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-vocab-{vocab_size}-augmented.csv\",\n",
    "    model_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-model-{vocab_size}-{embed_dim}-augmented.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"nama\",\n",
    "    entity=\"nama\",\n",
    "    name=\"61_swivel\",\n",
    "    group=given_surname,\n",
    "    notes=\"umap\",\n",
    "    config=config._asdict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_train, weighted_actual_names_train, candidate_names_train = \\\n",
    "    load_dataset(config.train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the most-frequent vocab_size names\n",
    "# input_names_train, weighted_actual_names_train, candidate_names_train = \\\n",
    "#     select_frequent_k(input_names_train, \n",
    "#                       weighted_actual_names_train, \n",
    "#                       candidate_names_train,\n",
    "#                       config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input_names_train\", len(input_names_train))\n",
    "print(\"weighted_actual_names_train\", sum(len(wan) for wan in weighted_actual_names_train))\n",
    "print(\"total pairs\", sum(freq for wans in weighted_actual_names_train for _, _, freq in wans))\n",
    "print(\"candidate_names_train\", len(candidate_names_train))\n",
    "print(\"total names\", len(set(input_names_train).union(set(candidate_names_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = SwivelDataset(input_names_train, weighted_actual_names_train, config.vocab_size, symmetric=True)\n",
    "vocab = dataset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab names in order by id\n",
    "vocab_names = list(name_id[0] for name_id in sorted(vocab.items(), key=lambda x: x[1]))\n",
    "print(len(vocab_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwivelModel(len(vocab), config.embed_dim, config.confidence_base, config.confidence_scale, config.confidence_exponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectors with tfidf values\n",
    "max_ngram = 3\n",
    "min_df = 10\n",
    "max_df = 0.8\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, max_ngram), analyzer=\"char_wb\", min_df=min_df, max_df=max_df)\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(vocab_names)\n",
    "print(tfidf_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducer = TruncatedSVD(n_components=config.embed_dim)\n",
    "reducer = umap.UMAP(n_components=config.embed_dim)\n",
    "tfidf_X_train = reducer.fit_transform(tfidf_X_train)\n",
    "print(tfidf_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale to uniform [-sqrt(1/embed_dim), sqrt(1/embed_dim)]\n",
    "scaler_max = math.sqrt(1 / config.embed_dim)\n",
    "scaler = MinMaxScaler(feature_range=(-scaler_max, scaler_max))\n",
    "tfidf_X_train = scaler.fit_transform(tfidf_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# init weights\n",
    "model.init_params(dataset.get_row_sums(), dataset.get_col_sums(), tfidf_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "n_steps_per_epoch = 0\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=config.lr)\n",
    "\n",
    "all_loss_values = []\n",
    "for e in tqdm(range(0, config.n_epochs)):\n",
    "    print(\"Epoch\", e, datetime.now())\n",
    "    loss_values = train_swivel(model, dataset, n_steps=n_steps_per_epoch, \n",
    "                     submatrix_size=config.submatrix_size, \n",
    "                     lr=config.lr, device=device, optimizer=optimizer)\n",
    "    all_loss_values.extend(loss_values)\n",
    "    torch.save(model.state_dict(), fopen(f\"{config.model_path}.{e}\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_df = pd.DataFrame(vocab.items(), columns=[\"name\", \"index\"])\n",
    "vocab_df.to_csv(fopen(config.vocab_path, \"wb\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), fopen(config.model_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Vocab and model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_df = pd.read_csv(fopen(config.vocab_path, \"rb\"))\n",
    "vocab = {name: _id for name, _id in zip(vocab_df[\"name\"], vocab_df[\"index\"])}\n",
    "model = SwivelModel(len(vocab), config.embed_dim)\n",
    "model.load_state_dict(torch.load(fopen(config.model_path, \"rb\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.set_ylim([0, 1.0])\n",
    "plt.plot(all_loss_values[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get best matches\n",
    "# NOTE: only considers as potential matches names in candidate_names_eval, not names in input_names_eval\n",
    "eval_batch_size = 1024\n",
    "add_context = True\n",
    "n_jobs=1\n",
    "input_names_sample = input_names_train[::10]\n",
    "weighted_actual_names_sample = weighted_actual_names_train[::10]\n",
    "best_matches = get_best_swivel_matches(model, \n",
    "                                       vocab, \n",
    "                                       input_names_sample,\n",
    "                                       candidate_names_train, \n",
    "                                       k=num_matches, \n",
    "                                       eval_batch_size,\n",
    "                                       add_context=add_context, \n",
    "                                       n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics.precision_weighted_recall_curve_at_threshold(\n",
    "    weighted_actual_names_sample, best_matches, min_threshold=0.01, max_threshold=2.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics.get_auc(\n",
    "    weighted_actual_names_sample, best_matches, min_threshold=0.01, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Eval on original (unaugmented) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_eval, weighted_actual_names_eval, candidate_names_eval = \\\n",
    "    load_dataset(config.eval_path, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure all the names are in the model\n",
    "for name in input_names_eval:\n",
    "    if name not in vocab.keys():\n",
    "        print(\"name missing\", name)\n",
    "        break\n",
    "for name in candidate_names_eval:\n",
    "    if name not in vocab.keys():\n",
    "        print(\"name missing\", name)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get best matches\n",
    "# NOTE: only considers as potential matches names in candidate_names_eval, not names in input_names_eval\n",
    "eval_batch_size = 1024\n",
    "add_context = True\n",
    "n_jobs=1\n",
    "input_names_sample = input_names_eval[::10]\n",
    "weighted_actual_names_sample = weighted_actual_names_eval[::10]\n",
    "best_matches = get_best_swivel_matches(model, \n",
    "                                       vocab, \n",
    "                                       input_names_sample,\n",
    "                                       candidate_names_eval, \n",
    "                                       k=num_matches, \n",
    "                                       eval_batch_size,\n",
    "                                       add_context=add_context, \n",
    "                                       n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 120\n",
    "input_names_sample[pos]\n",
    "# for ix, name in enumerate(input_names_sample):\n",
    "#     print(ix, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_test = input_names_sample[pos:pos+1]\n",
    "weighted_actual_names_test = weighted_actual_names_sample[pos:pos+1]\n",
    "print(weighted_actual_names_test)\n",
    "best_matches_test = best_matches[pos:pos+1]\n",
    "print(best_matches_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_at_threshold(weighted_actual_names_test[0], best_matches_test[0], 0.7725, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.weighted_recall_at_threshold(weighted_actual_names_test[0], best_matches_test[0], 0.7725, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics.precision_weighted_recall_curve_at_threshold(\n",
    "    weighted_actual_names_test, best_matches_test, min_threshold=0.01, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_weighted_recall_curve_at_threshold(\n",
    "    weighted_actual_names_sample, best_matches, min_threshold=0.01, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.get_auc(\n",
    "    weighted_actual_names_sample, best_matches, min_threshold=0.01, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "for i in range(100001, 400000, 10000):\n",
    "    print(i, input_names_eval[i])\n",
    "    matches_above_threshold = best_matches[i][best_matches[i,:,1] > threshold]\n",
    "    matched_wans = []\n",
    "    unmatched_wans = []\n",
    "    for wan in weighted_actual_names_eval[i]:\n",
    "        if wan[0] in matches_above_threshold[:, 0]:\n",
    "            matched_wans.append(wan)\n",
    "        elif wan[2] > 0:\n",
    "            unmatched_wans.append(wan)\n",
    "    print(\"  matched wans\", matched_wans)\n",
    "    print(\"  unmatched wans\", unmatched_wans)\n",
    "    print(\"  matches above threshold\", len(matches_above_threshold), matches_above_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 390000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_matches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weighted_actual_names_train[input_names_train.index(input_names_eval[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "variant = \"<shirley>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weighted_actual_names_train[input_names_train.index(variant)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weighted_actual_names_eval[input_names_eval.index(variant)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.models.swivel import get_swivel_embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# demo names\n",
    "input_names_train = [\"<john>\", \"<mary>\"]\n",
    "weighted_actual_names_train = [\n",
    "    [(\"<johnny>\", 0.2, 20), (\"<jonathan>\", 0.5, 50), (\"<jon>\", 0.3, 30)],\n",
    "    [(\"<marie>\", 0.7, 70), (\"<maria>\", 0.3, 30)],\n",
    "    # [(\"<johnny>\", 0.2, 20), (\"<jonathan>\", 0.5, 50), (\"<jon>\", 0.3, 30), (\"<mary>\", 0.0, 0.5)],\n",
    "    # [(\"<marie>\", 0.7, 70), (\"<maria>\", 0.3, 30), (\"<john>\", 0.0, 0.5)],\n",
    "    # [(\"<johnny>\", 0.2, 20), (\"<jonathan>\", 0.5, 50), (\"<jon>\", 0.3, 30), (\"<mary>\", 0.0, 1), (\"<maria>\", 0.0, 1), (\"<marie>\", 0.0, 1)],\n",
    "    # [(\"<marie>\", 0.7, 70), (\"<maria>\", 0.3, 30), (\"<john>\", 0.0, 1), (\"<johnny>\", 0.0, 1), (\"<jonathan>\", 0.0, 1), (\"<jon>\", 0.0, 1)],\n",
    "]\n",
    "candidate_names_train = np.array([\"<johnny>\", \"<jonathan>\", \"<marie>\", \"<maria>\", \"<jon>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "symmetric = True\n",
    "dataset = SwivelDataset(input_names_train, weighted_actual_names_train, config.vocab_size, symmetric=symmetric)\n",
    "vocab = dataset.get_vocab()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset._sparse_cooc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab names in order by id\n",
    "vocab_names = list(name_id[0] for name_id in sorted(vocab.items(), key=lambda x: x[1]))\n",
    "print(vocab_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectors with tfidf values\n",
    "max_ngram = 5  # 3\n",
    "min_df = 1  # 10\n",
    "max_df = 1.0  # 0.5\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, max_ngram), analyzer=\"char_wb\", min_df=min_df, max_df=max_df)\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(vocab_names)\n",
    "print(tfidf_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce tfidf values to embed_dim\n",
    "svd = TruncatedSVD(n_components=embed_dim)\n",
    "tfidf_X_train = svd.fit_transform(tfidf_X_train)\n",
    "tfidf_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create swivel model\n",
    "model = SwivelModel(len(vocab), embed_dim, config.confidence_base, config.confidence_scale, config.confidence_exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init weights to tfidf values\n",
    "# model.init_params(dataset.get_row_sums(), dataset.get_col_sums(), tfidf_X_train)\n",
    "model.init_params(dataset.get_row_sums(), dataset.get_col_sums(), tfidf_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# device=\"cpu\"\n",
    "n_steps = 10\n",
    "submatrix_size = 64\n",
    "learning_rate = 0.05\n",
    "loss_values = train_swivel(model, dataset, n_steps=n_steps, submatrix_size=submatrix_size, lr=learning_rate, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "# ax.set_ylim([0, 0.1])\n",
    "plt.plot(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "add_context = True\n",
    "\n",
    "all_names = np.array(input_names_train + candidate_names_train.tolist())\n",
    "all_embeddings = get_swivel_embeddings(model, vocab, all_names, add_context=add_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "demo_name = '<john>'\n",
    "demo_name_pos = 0\n",
    "demo_embeddings = get_swivel_embeddings(model, vocab, [demo_name], add_context=add_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try cosine similarity\n",
    "# totals = all_embeddings.sum(axis=0)\n",
    "# all_embeddings_norm = all_embeddings / totals\n",
    "# demo_embeddings_norm = all_embeddings_norm[[demo_name_pos]]\n",
    "# scores = cosine_similarity(demo_embeddings_norm, all_embeddings_norm)\n",
    "# ixs = np.argsort(-scores)[:, :k]\n",
    "# sorted_scores = scores[:, ixs[0]]\n",
    "# sorted_names = all_names[ixs[0]]\n",
    "# best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "# print(\"cosine_norm_0\", best_matches)\n",
    "\n",
    "# totals = demo_embeddings.sum(axis=1)\n",
    "# demo_embeddings_norm = demo_embeddings / totals[:, np.newaxis]\n",
    "# totals = all_embeddings.sum(axis=1)\n",
    "# all_embeddings_norm = all_embeddings / totals[:, np.newaxis]\n",
    "# scores = cosine_similarity(demo_embeddings_norm, all_embeddings_norm)\n",
    "# ixs = np.argsort(-scores)[:, :k]\n",
    "# sorted_scores = scores[:, ixs[0]]\n",
    "# sorted_names = all_names[ixs[0]]\n",
    "# best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "# print(\"cosine_norm_1\", best_matches)\n",
    "\n",
    "scores = cosine_similarity(demo_embeddings, all_embeddings)\n",
    "ixs = np.argsort(-scores)[:, :k]\n",
    "sorted_scores = scores[:, ixs[0]]\n",
    "sorted_names = all_names[ixs[0]]\n",
    "best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "print(\"cosine\", best_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# try euclidean similarity\n",
    "totals = all_embeddings.sum(axis=0)\n",
    "all_embeddings_norm = all_embeddings / totals\n",
    "demo_embeddings_norm = all_embeddings_norm[[demo_name_pos]]\n",
    "scores = euclidean_distances(demo_embeddings_norm, all_embeddings_norm)\n",
    "ixs = np.argsort(scores)[:, :k]\n",
    "sorted_scores = scores[:, ixs[0]]\n",
    "sorted_names = all_names[ixs[0]]\n",
    "best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "print(\"euclidean_norm_0\", best_matches)\n",
    "\n",
    "# totals = demo_embeddings.sum(axis=1)\n",
    "# demo_embeddings_norm = demo_embeddings / totals[:, np.newaxis]\n",
    "# totals = all_embeddings.sum(axis=1)\n",
    "# all_embeddings_norm = all_embeddings / totals[:, np.newaxis]\n",
    "# scores = euclidean_distances(demo_embeddings_norm, all_embeddings_norm)\n",
    "# ixs = np.argsort(scores)[:, :k]\n",
    "# sorted_scores = scores[:, ixs[0]]\n",
    "# sorted_names = all_names[ixs[0]]\n",
    "# best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "# print(\"euclidean_norm_1\", best_matches)\n",
    "\n",
    "scores = euclidean_distances(demo_embeddings, all_embeddings)\n",
    "ixs = np.argsort(scores)[:, :k]\n",
    "sorted_scores = scores[:, ixs[0]]\n",
    "sorted_names = all_names[ixs[0]]\n",
    "best_matches = np.dstack((sorted_names, sorted_scores))\n",
    "print(\"euclidean\", best_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot embeddings\n",
    "xs = list(x for x, _ in all_embeddings)\n",
    "ys = list(y for _, y in all_embeddings)\n",
    "plt.scatter(xs, ys)\n",
    "for ix, name in enumerate(all_names):\n",
    "    plt.annotate(name, xy=(xs[ix], ys[ix]), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "source_names = np.array([\"tom\", \"dick\", \"harry\"])\n",
    "source_names_X = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "rows = np.array([[1,2,3],[4,5,6],[7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cosine_similarity(rows, source_names_X)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores_idx = np.argsort(scores, axis=1)\n",
    "sorted_scores_idx = np.flip(sorted_scores_idx, axis=1)\n",
    "sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores_idx = sorted_scores_idx[:, :2]\n",
    "sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = np.take_along_axis(scores, sorted_scores_idx, axis=1)\n",
    "sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_source_names_X = source_names_X[sorted_scores_idx]\n",
    "sorted_source_names_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (row, source_names_X) in enumerate(zip(rows, sorted_source_names_X)):\n",
    "    for j, source_name_X in enumerate(source_names_X):\n",
    "        if np.array_equal(row, source_name_X):\n",
    "            sorted_scores[i, j] = 0\n",
    "sorted_scores                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sorted_scores_idx = np.argsort(sorted_scores, axis=1)\n",
    "re_sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sorted_scores_idx = np.flip(re_sorted_scores_idx, axis=1)\n",
    "re_sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sorted_scores_idx = re_sorted_scores_idx[:, :1]\n",
    "re_sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores = np.take_along_axis(sorted_scores, re_sorted_scores_idx, axis=1)\n",
    "sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores_idx = np.take_along_axis(sorted_scores_idx, re_sorted_scores_idx, axis=1)\n",
    "sorted_scores_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_source_names = source_names[sorted_scores_idx]\n",
    "sorted_source_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
