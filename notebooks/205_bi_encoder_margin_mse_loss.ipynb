{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139bee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893819e",
   "metadata": {},
   "source": [
    "# Train a bi-encoder to learn name-to-vec encodings\n",
    "Try phonemes, subwords, and/or n-grams using anchor-pos-neg triplets with MarginMSELoss\n",
    "\n",
    "| phon/sub | negs | notes  | uni/bi | size | loss | errors | negs |\n",
    "| -------- | ---- | ------ | ------ | ---- | ---- | ------ | ---- |\n",
    "| Phonemes | 100  |        | unigrams |      | 0.102 | ?, 155 | ?, ?, 146 |\n",
    "| Phonemes | 100  |        | bigrams  |      | 0.073 | 8.5, 94 | 12, 43, 56 |\n",
    "| Subwords | 100  |        | unigrams | 500  | 0.74 | ?, 89 | 12, 54, 60, |\n",
    "| Subwords | 100  |        | unigrams | 1000 | 0.063 | 7.6, 69 | 7, 22, 30 |\n",
    "| Subwords | 100  |        | unigrams | 2000 | 0.052 | 5.5, 61 | 7, 25, 32 |\n",
    "| Subwords | 200  |        | unigrams | 1000 | 0.055 | 9.6, 80 | 4, 27, 37 |\n",
    "| Subwords | 200  |        | unigrams | 2000 | 0.047 | 6.2, 56 | 5, 24, 32 |\n",
    "| Subwords | 200  | noback | unigrams | 2000 | 0.049 | 7.4, 46 | 6, 28, 39 |\n",
    "| Subwords | 200  | noback9 | unigrams | 2000 | 0.049 | 3.7, 62 | 7, 31, 46 |\n",
    "| Subwords | 200  | noback9mask | unigrams | 2000 | 0.049 | 7.6, 76 | 6, 30, 46 |\n",
    "| Subwords | 100+1000 | noback9mask | unigrams | 2000 | 0.050 | 2.6, 66 | 4, 21, 33 |\n",
    "| Subwords | 100+1000 | noback10mask | unigrams | 2000 | 0.049 | 11.6, 77 | 6, 24, 34 |\n",
    "| Subwords | 100+1000 | noback10mask 64dim 40epochs | unigrams | 2000 | 0.036 | 17.8, 75 | 2, 12, 21 |\n",
    "| Subwords | 100+1000 | noback10mask 64dim 10epochs | unigrams | 2000 | 0.037 | 11.2, 80 | 2, 10, 18 |\n",
    "| Subwords | 100+1000 | noback10mask 64dim 5epochs | unigrams | 2000 | 0.037 | 6, 75 | 1, 11, 22 |\n",
    "| Subwords | 150+1000 | noback10mask 64dim 5epochs | unigrams | 2000 | 0.035 | 11.2, 80 | 3, 15, 22 |\n",
    "| Subwords | 100+1500 | noback9mask | unigrams | 2000 | 0.046 | 6.4. 80 | 4, 22, 34 |\n",
    "| Subwords | 200 | nofront | unigrams | 2000 | 0.048 | 6.7, 56 | 6, 28, 37 |\n",
    "| Subwords | 200 | noback  | unigrams | 1500 | 0.052 | 6.3, 57 | 8, 31, 47 |\n",
    "| Subwords | 100  |        | bigrams  | 500  | 0.066 | 19.5, 85 | 13, 23, 31 |\n",
    "| Subwords | 100  |        | bigrams  | 1000 | 0.56 | 12, 63 | 7, 30, 40 |\n",
    "| Subwords | 100  |        | bigrams  | 2000 | 0.040 | 9.6, 64 | 16, 56, 66 |\n",
    "| Subwords | 100  | edit   | unigrams | 500  | | | |\n",
    "| Subwords | 100  | edit   | unigrams | 1000 | 0.065 | 12.2, 102, | 7, 34, 36 |\n",
    "| Subwords | 100  | edit   | unigrams | 2000 | 0.055 | 6.9, 77 | 11, 42, 46 |\n",
    "| Subwords | 100  | edit   | bigrams  | 500  | | | |\n",
    "| Subwords | 100  | edit   | bigrams  | 1000 | | | |\n",
    "| Subwords | 100  | edit   | bigrams  | 2000 | | | |\n",
    "\n",
    "**TODO:**\n",
    "- fix training data issues\n",
    "\n",
    "- try sin+cos pos emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0364890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "\n",
    "import boto3\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from phonemizer.separator import Separator\n",
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from src.data.filesystem import fopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774de195",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_surname = \"given\"\n",
    "sample_frac = 1.0\n",
    "num_common_names = 10000\n",
    "report_size = 10000\n",
    "max_tokens = 10\n",
    "\n",
    "use_phonemes = False\n",
    "use_edit_tokenizer = False\n",
    "use_bigrams = False\n",
    "subword_vocab_size = 2000  # 500, 1000, 1500, 2000\n",
    "num_easy_negs = 100  # 100, 150, 200\n",
    "num_common_negs = 1000\n",
    "\n",
    "embedding_dim = 64\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "pref_path = f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\"\n",
    "triplets_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-triplets.csv.gz\"\n",
    "tfidf_path=f\"s3://nama-data/data/models/fs-{given_surname}-tfidf-v2.joblib\"\n",
    "vocab_path = f\"s3://nama-data/data/models/fs-{given_surname}-espeak_phoneme_vocab.json\"\n",
    "bigrams_vocab_path = f\"s3://nama-data/data/models/fs-{given_surname}-espeak_phoneme_vocab_bigrams.json\"\n",
    "\n",
    "nama_bucket = 'nama-data'\n",
    "tokenizer_path=f\"data/models/fs-{given_surname}-subword-tokenizer-{subword_vocab_size}.json\"\n",
    "edit_tokenizer_path=f\"data/models/fs-{given_surname}-edit-subword-tokenizer-{subword_vocab_size}.json\"\n",
    "tokenizer_bigrams_vocab_path = f\"s3://nama-data/data/models/fs-{given_surname}-tokenizer_vocab_bigrams-{subword_vocab_size}.json\"\n",
    "edit_tokenizer_bigrams_vocab_path = f\"s3://nama-data/data/models/fs-{given_surname}-edit_tokenizer_vocab_bigrams-{subword_vocab_size}.json\"\n",
    "\n",
    "model_path = f\"../data/models/bi_encoder-{given_surname}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34474d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c030b7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read triplets\n",
    "triplets_df = pd.read_csv(triplets_path).sample(frac=sample_frac)\n",
    "print(len(triplets_df))\n",
    "triplets_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d0d78",
   "metadata": {},
   "source": [
    "### read common names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee56867",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_df = pd.read_csv(pref_path, keep_default_na=False)\n",
    "common_names = [name for name in pref_df['name'][:num_common_names].tolist() \\\n",
    "                if len(name) > 1 and re.fullmatch(r'[a-z]+', name)]\n",
    "pref_df = None\n",
    "len(common_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04500a43",
   "metadata": {},
   "source": [
    "### read phoneme vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b99074",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fopen(bigrams_vocab_path if use_bigrams else vocab_path, 'r') as f:\n",
    "    phoneme_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0429f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_vocab['[UNK]'] = len(phoneme_vocab)\n",
    "phoneme_vocab['[PAD]'] = len(phoneme_vocab)\n",
    "len(phoneme_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0904e3",
   "metadata": {},
   "source": [
    "### read subword tokenizer and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dec5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "path = edit_tokenizer_path if use_edit_tokenizer else tokenizer_path\n",
    "with open(f\"../{path}\", 'wb') as f:\n",
    "    s3.download_fileobj(nama_bucket, path, f)\n",
    "subword_tokenizer = PreTrainedTokenizerFast(tokenizer_file=f\"../{path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_bigrams:\n",
    "    path = edit_tokenizer_bigrams_vocab_path if use_edit_tokenizer else tokenizer_bigrams_vocab_path\n",
    "    with fopen(path, 'r') as f:\n",
    "        subword_vocab = json.load(f)  \n",
    "else:\n",
    "    subword_vocab = subword_tokenizer.get_vocab()\n",
    "print(len(subword_vocab), \n",
    "      subword_vocab['[UNK]'], \n",
    "      subword_vocab['[PAD]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc687d",
   "metadata": {},
   "source": [
    "## Set up generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d66f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "espeak = EspeakBackend('en-us')\n",
    "separator = Separator(phone=' ', syllable=None, word='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a6eec",
   "metadata": {},
   "source": [
    "## Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf41c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_phonemes(name):\n",
    "    return espeak.phonemize([name], separator=separator, strip=True)[0].split(' ')\n",
    "\n",
    "def tokenize_subwords(name):\n",
    "    return subword_tokenizer.convert_ids_to_tokens(subword_tokenizer.encode(name))\n",
    "\n",
    "# set up tokenizer and tokenizer_vocab\n",
    "tokenizer = tokenize_phonemes if use_phonemes else tokenize_subwords\n",
    "tokenizer_vocab = phoneme_vocab if use_phonemes else subword_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c39b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(name, max_tokens):\n",
    "    if name in name_tokens_cache:\n",
    "        return name_tokens_cache[name]\n",
    "    \n",
    "    result = [tokenizer_vocab['[PAD]']] * max_tokens\n",
    "    unk = tokenizer_vocab['[UNK]']\n",
    "    tokens = tokenizer(name)\n",
    "    context = 'START'\n",
    "    if use_bigrams:\n",
    "        tokens.append('END')\n",
    "    for ix, token in enumerate(tokens):\n",
    "        if ix == max_tokens:\n",
    "            break\n",
    "        if use_bigrams:\n",
    "            bigram = f\"{context},{token}\"\n",
    "            result[ix] = tokenizer_vocab.get(bigram, tokenizer_vocab.get(token, unk))\n",
    "        else:\n",
    "            result[ix] = tokenizer_vocab.get(token, unk)\n",
    "        context = token\n",
    "    name_tokens_cache[name] = result\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of (anchor_tokens, pos_tokens, neg_tokens, target_margin)\n",
    "all_data = []\n",
    "name_tokens_cache = {}\n",
    "seen_anchor_pos = set()\n",
    "for tup in tqdm(triplets_df.itertuples()):\n",
    "    anchor = tup.anchor[1:-1]\n",
    "    pos = tup.positive[1:-1]\n",
    "    neg = tup.negative[1:-1]\n",
    "    anchor_tokens = tokenize(anchor, max_tokens)\n",
    "    pos_tokens = tokenize(pos, max_tokens)\n",
    "    neg_tokens = tokenize(neg, max_tokens)\n",
    "    target_margin = tup.positive_score - tup.negative_score\n",
    "    # anchor, positive, hard-negative\n",
    "    all_data.append({\n",
    "        'anchor': torch.tensor(anchor_tokens),\n",
    "        'pos': torch.tensor(pos_tokens),\n",
    "        'neg': torch.tensor(neg_tokens),\n",
    "        'target': torch.tensor(target_margin, dtype=torch.float),\n",
    "    })\n",
    "    anchor_pos = f\"{anchor},{pos}\"\n",
    "    if anchor_pos in seen_anchor_pos:\n",
    "        continue\n",
    "    seen_anchor_pos.add(anchor_pos)\n",
    "    for ix in range(num_easy_negs):\n",
    "        # anchor, positive, easy-negative\n",
    "        easy_neg = random.choice(common_names)\n",
    "        easy_neg_tokens = tokenize(easy_neg, max_tokens)\n",
    "        if anchor_tokens == easy_neg_tokens:\n",
    "            continue\n",
    "        all_data.append({\n",
    "            'anchor': torch.tensor(anchor_tokens),\n",
    "            'pos': torch.tensor(pos_tokens),\n",
    "            'neg': torch.tensor(easy_neg_tokens),\n",
    "            'target': torch.tensor(tup.positive_score, dtype=torch.float)\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a15d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for pos in tqdm(common_names[:num_common_negs]):\n",
    "    pos_tokens = tokenize(pos, max_tokens)\n",
    "    for neg in common_names[:num_common_negs]:\n",
    "        if pos == neg:\n",
    "            continue\n",
    "        pos_neg = f\"{pos},{neg}\"\n",
    "        if pos_neg in seen_anchor_pos:\n",
    "            continue\n",
    "        neg_pos = f\"{neg},{pos}\"\n",
    "        if neg_pos in seen_anchor_pos:\n",
    "            continue\n",
    "        neg_tokens = tokenize(neg, max_tokens)\n",
    "        all_data.append({\n",
    "            'anchor': torch.tensor(pos_tokens),\n",
    "            'pos': torch.tensor(pos_tokens),\n",
    "            'neg': torch.tensor(neg_tokens),\n",
    "            'target': torch.tensor(1.0, dtype=torch.float)\n",
    "        })\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(all_data, test_size=0.10)\n",
    "print(len(train_data), len(val_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5e413",
   "metadata": {},
   "source": [
    "## Train bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(anchors, positives, negatives, labels):\n",
    "#     anchor_pos_sim = (anchors * positives).sum(dim=-1)\n",
    "#     anchor_neg_sim = (anchors * negatives).sum(dim=-1)\n",
    "    anchor_pos_sim = F.cosine_similarity(anchors, positives, dim=-1)\n",
    "    anchor_neg_sim = F.cosine_similarity(anchors, negatives, dim=-1)\n",
    "    margin_pred = anchor_pos_sim - anchor_neg_sim\n",
    "    return F.mse_loss(margin_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fbe744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your bi-encoder model\n",
    "class BiEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, max_tokens, pad_token):\n",
    "        super(BiEncoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_tokens = max_tokens\n",
    "        self.pad_token = pad_token\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.positional_embedding = nn.Embedding(num_embeddings=max_tokens, embedding_dim=embedding_dim)\n",
    "#         self.forward_positional_embedding = nn.Embedding(num_embeddings=max_tokens+1, embedding_dim=embedding_dim)\n",
    "#         self.backward_positional_embedding = nn.Embedding(num_embeddings=max_tokens+1, embedding_dim=embedding_dim)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)  # Pooling layer to create a single vector\n",
    "\n",
    "    def forward(self, input):\n",
    "        # get token embedding\n",
    "        embedded = self.embedding(input)  # Shape: (batch_size, max_tokens, embedding_dim)\n",
    "        # get mask\n",
    "        mask = torch.where(input == self.pad_token, 0, 1)[..., None]  # Shape: (batch_size, max_tokens, 1)\n",
    "        # get positional embedding\n",
    "        positions = torch.arange(start=0, end=self.max_tokens).repeat(input.shape[0], 1)\n",
    "        positional_embedded = self.positional_embedding(positions)\n",
    "#         # get forward positional embedding: pad token is position 0\n",
    "#         positions = torch.arange(start=1, end=self.max_tokens+1).repeat(input.shape[0], 1)\n",
    "#         forward_positions = torch.where(input == self.pad_token, 0, positions)\n",
    "#         forward_positional_embedded = self.forward_positional_embedding(forward_positions)\n",
    "        # get backward positional embedding\n",
    "#         backward_positions = torch.where(input == self.pad_token, 0, 1)\n",
    "#         backward_n_tokens = backward_positions.sum(dim=1)\n",
    "#         for ix in range(backward_n_tokens.shape[0]):\n",
    "#             n_tokens = backward_n_tokens[ix]\n",
    "#             backward = torch.arange(start=n_tokens, end=0, step=-1)\n",
    "#             backward_positions[ix][:n_tokens] = backward\n",
    "#         backward_positional_embedded = self.backward_positional_embedding(backward_positions)\n",
    "        # multiply embeddings\n",
    "#         embedded = embedded * forward_positional_embedded * backward_positional_embedded\n",
    "        embedded = embedded * positional_embedded * mask\n",
    "        pooled = self.pooling(embedded.permute(0, 2, 1)).squeeze(2)  # Shape: (batch_size, embedding_dim)\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, verbose=True):\n",
    "    for epoch in range(num_epochs):\n",
    "        # make sure gradient tracking is on\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for ix, data in enumerate(train_loader):\n",
    "            # get batch\n",
    "            anchors = data['anchor']\n",
    "            positives = data['pos']\n",
    "            negatives = data['neg']\n",
    "            target_margins = data['target']\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            anchor_embeddings = model(anchors)  # Shape: (batch_size, embedding_dim)\n",
    "            pos_embeddings = model(positives)  # Shape: (batch_size, embedding_dim)\n",
    "            neg_embeddings = model(negatives)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(anchor_embeddings, pos_embeddings, neg_embeddings, target_margins)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate loss and report\n",
    "            if verbose:\n",
    "                running_loss += loss.item()\n",
    "                if ix % report_size == report_size - 1:\n",
    "                    avg_loss = running_loss / report_size  # loss per batch\n",
    "                    print(f\"Epoch {epoch} batch {ix} loss {avg_loss}\")\n",
    "                    running_loss = 0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # disable gradient computation\n",
    "        running_loss = 0\n",
    "        num_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                anchors = data['anchor']\n",
    "                positives = data['pos']\n",
    "                negatives = data['neg']\n",
    "                target_margins = data['target']\n",
    "                anchor_embeddings = model(anchors)  # Shape: (batch_size, embedding_dim)\n",
    "                pos_embeddings = model(positives)  # Shape: (batch_size, embedding_dim)\n",
    "                neg_embeddings = model(negatives)  # Shape: (batch_size, embedding_dim)\n",
    "                loss = loss_fn(anchor_embeddings, pos_embeddings, neg_embeddings, target_margins)\n",
    "                running_loss += loss.item()  \n",
    "                num_val_batches += 1\n",
    "\n",
    "        # calculate average validation loss\n",
    "        val_loss = running_loss / num_val_batches\n",
    "        if verbose:\n",
    "            print(f\"VALIDATION: Epoch {epoch} loss {val_loss}\")\n",
    "        # epoch_model_path = f\"{model_path}-{epoch}\"\n",
    "        # torch.save(model.state_dict, epoch_model_path)\n",
    "        \n",
    "    # return final epoch validation loss\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fefd66",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_objective_function(train_data, \n",
    "                                val_data, \n",
    "                                vocab_size,\n",
    "                                max_tokens,\n",
    "                                pad_token,\n",
    "                                verbose=True,\n",
    "                               ):\n",
    "    \n",
    "    def objective(config):\n",
    "        learning_rate = config['learning_rate']\n",
    "        batch_size = config['batch_size']\n",
    "        embedding_dim = config['embedding_dim']\n",
    "        num_epochs = config['num_epochs']\n",
    "        \n",
    "        if verbose:\n",
    "            print('train', config)\n",
    "        \n",
    "        # Create an instance of the bi-encoder model\n",
    "        model = BiEncoder(embedding_dim, vocab_size, max_tokens, pad_token)\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # model.to(device)\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Create data loader\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "        val_loss = train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, verbose=False)\n",
    "        if verbose:\n",
    "            print('val_loss', val_loss)\n",
    "        \n",
    "        return {\n",
    "            'status': STATUS_OK,\n",
    "            'loss': val_loss,\n",
    "            'config': config,            \n",
    "        }\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperOpt search space\n",
    "search_space = {\n",
    "    \"learning_rate\": hp.loguniform('learning_rate', math.log(1e-4), math.log(1e-2)),\n",
    "    \"batch_size\": hp.choice('batch_size', [8,16,32,64]),\n",
    "    \"embedding_dim\": hp.choice('embedding_dim', [8,16,32,64]),\n",
    "    \"num_epochs\": hp.choice('num_epochs', [5,10,20,40]),\n",
    "}\n",
    "objective = hyperopt_objective_function(train_data=train_data,\n",
    "                                        val_data=val_data,\n",
    "                                        vocab_size=len(tokenizer_vocab),\n",
    "                                        max_tokens=max_tokens,\n",
    "                                        pad_token=tokenizer_vocab['[PAD]'],\n",
    "                                        verbose=True,\n",
    "                                       )\n",
    "trials = Trials()\n",
    "\n",
    "# minimize the objective over the space\n",
    "best = fmin(objective, \n",
    "            search_space, \n",
    "            algo=tpe.suggest, \n",
    "            trials=trials,\n",
    "            max_evals=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best\", best)\n",
    "print(\"results\", trials.results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = best_result.config['batch_size']\n",
    "# learning_rate = best_result.config['learning_rate']\n",
    "# embedding_dim = best_result.config['embedding_dim']\n",
    "# num_epochs = best_result.config['num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba77e31",
   "metadata": {},
   "source": [
    "## Train model and Review predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create an instance of the bi-encoder model\n",
    "model = BiEncoder(embedding_dim, len(tokenizer_vocab), max_tokens, tokenizer_vocab['[PAD]'])\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(name1, name2):\n",
    "    name1_tokens = tokenize(name1, max_tokens)\n",
    "    name2_tokens = tokenize(name2, max_tokens)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(torch.tensor([name1_tokens, name2_tokens]))\n",
    "    # return (embeddings[0] * embeddings[1]).sum(dim=-1).item()\n",
    "    return F.cosine_similarity(embeddings[0], embeddings[1], dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09002d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_error = 0\n",
    "for ix, row in enumerate(triplets_df.head(1000).itertuples()):\n",
    "    anchor = row.anchor[1:-1]\n",
    "    pos = row.positive[1:-1]\n",
    "    neg = row.negative[1:-1]\n",
    "    pos_predict = predict(anchor, pos)\n",
    "    neg_predict = predict(anchor, neg)\n",
    "    correct = pos_predict > neg_predict\n",
    "    if pos_predict < 0.5 or not correct:\n",
    "        num_error += 1\n",
    "        print(num_error, anchor, pos, pos_predict, row.positive_score, '' if correct else 'ERROR')\n",
    "        print('  ', anchor, neg, neg_predict, row.negative_score)\n",
    "print(num_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cnt = 0\n",
    "n_names = 1000\n",
    "for ix, pos in enumerate(common_names[:n_names]):\n",
    "    pos_tokens = tokenizer(pos)\n",
    "    for neg in common_names[ix+1:n_names]:\n",
    "        neg_tokens = tokenizer(neg)\n",
    "        sim = predict(pos, neg)\n",
    "        if sim > 0.5:\n",
    "            print(pos, neg)  # , sim, '***' if sim >= 0.6 else '', pos_tokens, neg_tokens)\n",
    "            cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e72dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
