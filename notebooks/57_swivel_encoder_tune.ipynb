{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray import tune\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from src.data.filesystem import fopen\n",
    "from src.data.utils import load_train_test\n",
    "from src.eval import metrics\n",
    "from src.models.swivel import get_swivel_embeddings, get_best_swivel_matches\n",
    "from src.models.swivel_encoder import SwivelEncoderModel, convert_names_to_model_inputs, train_swivel_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "given_surname = \"given\"\n",
    "size = \"freq\"\n",
    "vocab_size = 500000\n",
    "embed_dim = 200\n",
    "Config = namedtuple(\"Config\", \"train_path test_path embed_dim vocab_path model_path\")\n",
    "config = Config(\n",
    "    train_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-similar-train-{size}.csv.gz\",\n",
    "    test_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-similar-test-{size}.csv.gz\",\n",
    "    embed_dim=embed_dim,\n",
    "    vocab_path=f\"s3://nama-data/data/models/fs-{given_surname}-{size}-swivel-{vocab_size}-vocab-tfidf.csv\",\n",
    "    model_path=f\"s3://nama-data/data/models/fs-{given_surname}-{size}-swivel-{vocab_size}-{embed_dim}-tfidf.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"nama\",\n",
    "#     entity=\"nama\",\n",
    "#     name=\"54_swivel_encoder\",\n",
    "#     group=given_surname,\n",
    "#     notes=\"\",\n",
    "#     config=config._asdict()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[train, test] = load_train_test([config.train_path, config.test_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_train, weighted_actual_names_train, candidate_names_train = train\n",
    "input_names_test, weighted_actual_names_test, candidate_names_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"input_names_train\", len(input_names_train))\n",
    "print(\"weighted_actual_names_train\", sum(len(wan) for wan in weighted_actual_names_train))\n",
    "print(\"candidate_names_train\", len(candidate_names_train))\n",
    "\n",
    "print(\"input_names_test\", len(input_names_test))\n",
    "print(\"weighted_actual_names_test\", sum(len(wan) for wan in weighted_actual_names_test))\n",
    "print(\"candidate_names_test\", len(candidate_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_df = pd.read_csv(fopen(config.vocab_path, \"rb\"))\n",
    "print(vocab_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab = {name: _id for name, _id in zip(vocab_df[\"name\"], vocab_df[\"index\"])}\n",
    "id2name = {i: n for (n, i) in vocab.items()}\n",
    "print(vocab[\"<john>\"])\n",
    "print(id2name[vocab[\"<john>\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.load(fopen(config.model_path, \"rb\"))\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Optimize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train using all names in the vocabulary\n",
    "train_names = vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_model_inputs = convert_names_to_model_inputs(train_names)\n",
    "print(train_model_inputs.shape)\n",
    "print(train_model_inputs.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create optimization and validation sets from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split out 10% of the candidate names\n",
    "candidate_names_optimize, candidate_names_validate = train_test_split(candidate_names_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate weighted_actual_names_validate from weighted_actual_names_train and candidate_names_validate\n",
    "candidate_names_validate_set = set(candidate_names_validate)\n",
    "weighted_actual_names_validate = []\n",
    "for wans in weighted_actual_names_train:\n",
    "    total_freq = sum([freq for name, _, freq in wans if name in candidate_names_validate_set])\n",
    "    wans = [(name, freq/total_freq, freq) for name, _, freq in wans if freq > 0 and name in candidate_names_validate_set]\n",
    "    total_weight = sum([weight for _, weight, _ in wans])\n",
    "    if total_weight > 1.0001:\n",
    "        print(\"total\", total_weight, \"wans\", wans)\n",
    "    weighted_actual_names_validate.append(wans)\n",
    "print(weighted_actual_names_validate[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove validation names from optimization set\n",
    "optimize_names = list(set(vocab.keys()) - set(candidate_names_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimize_name_inputs = convert_names_to_model_inputs(optimize_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimize_embeddings = torch.Tensor(get_swivel_embeddings(model, vocab, optimize_names))\n",
    "print(optimize_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Use Ray to perform the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ray_training_function(config, data):\n",
    "    # Hyperparameters\n",
    "    n_layers, char_embed_dim, n_hidden_units, bidirectional, lr, batch_size, use_adam_opt = \\\n",
    "        config[\"n_layers\"], config[\"char_embed_dim\"], config[\"n_hidden_units\"], config[\"bidirectional\"], config[\"lr\"], config[\"batch_size\"], config[\"use_adam_opt\"]\n",
    "    model, optimize_name_inputs, optimize_embeddings, input_names_train, candidate_names_validate, weighted_actual_names_validate = \\\n",
    "        data[\"model\"], data[\"optimize_name_inputs\"], data[\"optimize_embeddings\"], data[\"input_names_train\"], data[\"candidate_names_validate\"], data[\"weighted_actual_names_validate\"]\n",
    "    # create model\n",
    "    embed_dim = 200\n",
    "    encoder_model = SwivelEncoderModel(n_layers=n_layers, char_embed_dim=char_embed_dim, n_hidden_units=n_hidden_units,\n",
    "                                       output_dim=embed_dim, bidirectional=bidirectional, device=device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) if use_adam_opt else torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "    n_epochs = 50\n",
    "    k = 100\n",
    "    eval_batch_size = 256\n",
    "    add_context = True\n",
    "    n_jobs=1\n",
    "    for step in range(n_epochs):\n",
    "        train_swivel_encoder(encoder_model, optimize_name_inputs, optimize_embeddings, num_epochs=n_epochs, batch_size=batch_size, lr=lr,\n",
    "                             use_adam_opt=use_adam_opt, silent=True, optimizer=optimizer)\n",
    "        # calculate loss over the validation set, which is 1 - area under the PR curve\n",
    "        best_matches = get_best_swivel_matches(None, vocab, input_names_train,\n",
    "                                               candidate_names_validate, k, eval_batch_size,\n",
    "                                               add_context=add_context, encoder_model=encoder_model, n_jobs=n_jobs)\n",
    "        auc = metrics.get_auc(\n",
    "            weighted_actual_names_validate, best_matches, min_threshold=0.01, max_threshold=2.0, step=0.05, distances=False\n",
    "        )\n",
    "        tune.report(mean_loss=1 - auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(_redis_max_memory=4*10**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# search_alg = BayesOptSearch(metric=\"mean_loss\", mode=\"min\")\n",
    "\n",
    "time_budget = 3600*4\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(ray_training_function, data={\n",
    "        \"model\": model,\n",
    "        \"optimize_name_inputs\": optimize_name_inputs,\n",
    "        \"optimize_embeddings\": optimize_embeddings,\n",
    "        \"input_names_train\": input_names_train,\n",
    "        \"candidate_names_validate\": candidate_names_validate,\n",
    "        \"weighted_actual_names_validate\": weighted_actual_names_validate,\n",
    "    }),\n",
    "    num_samples=10,\n",
    "    time_budget_s=time_budget,\n",
    "    config={\n",
    "        \"n_layers\": tune.choice([1, 2]),\n",
    "        \"char_embed_dim\": tune.choice([0, 16, 64]),\n",
    "        \"n_hidden_units\": tune.choice([200]),  #64, 100, 200]),\n",
    "        \"bidirectional\": tune.choice([True]),  #, False]),\n",
    "        \"lr\": tune.uniform(0.01, 0.10),\n",
    "        \"batch_size\": tune.choice([64, 256]),\n",
    "        \"use_adam_opt\": tune.choice([True, False]),\n",
    "    },\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 8,\n",
    "        \"gpu\": 1,\n",
    "    },\n",
    "    progress_reporter=tune.JupyterNotebookReporter(overwrite=True, max_report_frequency=5)\n",
    "    # search_alg=ConcurrencyLimiter(search_alg, max_concurrent=1),\n",
    ")\n",
    "print(\"Best config: \", analysis.get_best_config(\n",
    "    metric=\"mean_loss\", mode=\"min\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
