{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907945b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2dac44",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate Coder PR\n",
    "Calculate precision and recall for NYSIIS, Soundex, etc. code\n",
    "\n",
    "### TODO\n",
    "- Re-run FS but use the bi-encoder to get clusters for OOV names, just as in Nama (unaugmented) clusters to make sure that we don't have a bug\n",
    "\n",
    "#### Tiny Query\n",
    "\n",
    "| Experiment | Threshold   |Precision|Recall| F1 | F2 |\n",
    "|------------|-------------|---------|------|----|----|\n",
    "|Soundex     |             | 0.152   | 0.608|0.24|0.38|\n",
    "|Nysiis      |             | 0.157   | 0.425|0.23|0.32|\n",
    "|FamilySearch|             | 0.167   | 0.648|0.27|0.41|\n",
    "|Nama        |0.55/0.8 @0  | 0.164   | 0.617|0.26|0.40|\n",
    "|Nama        |0.55/0.8 @20 | 0.131   | 0.778|0.22|0.39|\n",
    "|Nama        |0.65/0.8 @20 | 0.147   | 0.729|0.24|0.41|\n",
    "|Nama        |0.65/0.85 @20| 0.152   | 0.723|0.25|0.41|\n",
    "|Nama        |0.65/1.0 @20 | 0.154   | 0.712|0.25|0.41|\n",
    "|Nama        |0.70/1.0 @20 | 0.160   | 0.689|0.26|0.41|\n",
    "|Nama        |0.75/0.9 @20 | 0.164   | 0.657|0.26|0.41|\n",
    "|Nama        |0.75/1.0 @20 | 0.165   | 0.655|0.26|0.41|\n",
    "|Nama none   |0.75/1.0 @20 | 0.164   | 0.651|0.26|0.41|\n",
    "|Nama CE     |0.10/1.0 @40 | 0.137   | 0.749|0.23|0.39|\n",
    "|Nama CE     |0.20/1.0 @40 | 0.159   | 0.649|0.25|0.40|\n",
    "|Nama CE     |0.30/1.0 @40 | 0.165   | 0.619|0.26|0.40|  40k lookups\n",
    "|Nama CE     |0.30/1.0 @0  | 0.161   | 0.602|0.25|0.39|  39k lookups\n",
    "|Nama CE     |0.15/1.0 @20 | 0.153   | 0.676|0.25|0.40|  56k lookups\n",
    "|Nama BE     |0.65/1.0 @20 | 0.153   | 0.725|0.26|0.42|  54k lookups\n",
    "|Nama BE     |0.75/1.0 @20 | 0.165   | 0.656|0.26|0.41|  43k lookups\n",
    "|Nama BE     |0.75/1.0 @40 | 0.165   | 0.657|0.26|0.41|  44k lookups\n",
    "\n",
    "#### Tiny Common\n",
    "\n",
    "| Experiment | Threshold  |Precision|Recall| F1 | F2 |\n",
    "|------------|------------|---------|------|----|----|\n",
    "|FamilySearch|            | 0.251   | 0.675|0.37|0.50|\n",
    "|Nama        |0.55/0.8 @20| 0.205   | 0.809|0.33|0.51|\n",
    "\n",
    "#### All Query\n",
    "\n",
    "| Experiment | Threshold  |Precision|Recall| F1 | F2 |\n",
    "|------------|------------|---------|------|----|----|\n",
    "|Soundex     |            | 0.343   | 0.920|0.50|0.69|\n",
    "|Nysiis      |            | 0.413   | 0.877|0.56|0.72|\n",
    "|FamilySearch|            | 0.379   | 0.953|0.54|0.73|\n",
    "|Nama        |0.55/0.8 @20| 0.301   | 0.977|0.46|0.67|\n",
    "|Nama BE     |0.75/1.0 @40| 0.376   | 0.957|0.54|0.73|  45k lookups\n",
    "|Nama BE     |0.60/1.0 @40| 0.327   | 0.973|0.49|0.70|  74k lookups\n",
    "\n",
    "#### Compare Nama clustering approaches on Tiny Query\n",
    "\n",
    "| Experiment | Threshold  |Precision|Recall| F1  | F2  |\n",
    "|------------|------------|---------|------|-----|-----|\n",
    "|BE          | 0.3        | 0.174   | 0.583|0.268|0.396|\n",
    "|CE          | 0.15       | 0.188   | 0.534|0.278|0.390|\n",
    "|CE          | 0.08       | 0.179   | 0.567|0.272|0.395|\n",
    "|CE          | 0.10       | 0.180   | 0.560|0.273|0.394|\n",
    "\n",
    "### Surname Tiny Query\n",
    "| Experiment | Threshold  |Precision|Recall| F1  | F2  |\n",
    "|------------|------------|---------|------|-----|-----|\n",
    "|FamilySearch|            | 0.355   | 0.545|0.430|0.492|\n",
    "|Nama BE     |0.75/1.0 @40|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d131857",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import boto3\n",
    "import jellyfish\n",
    "from mpire import WorkerPool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from py4j.java_gateway import JavaGateway\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.utils import read_csv\n",
    "from src.eval.freq_metrics import calc_avg_precision_recall\n",
    "from src.models.tokenizer import get_tokenize_function_and_vocab\n",
    "from src.models.utils import top_similar_names\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bdec3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# configure\n",
    "given_surname = \"surname\"\n",
    "\n",
    "nama_threshold = 0.1\n",
    "nama_limit = 40\n",
    "\n",
    "linkage = \"average\"  # average, complete\n",
    "similarity_threshold = 0.1 if given_surname == \"given\" else 0.25\n",
    "scorer = \"ce\"\n",
    "cluster_freq_normalizer = \"none\"  # log, log10, none\n",
    "clusters_path = f\"../data/processed/clusters_{given_surname}-{scorer}-{linkage}-{similarity_threshold}-{cluster_freq_normalizer}-augmented.json\"\n",
    "super_clusters_path = f\"../data/processed/super_clusters_{given_surname}-{scorer}-{linkage}-{similarity_threshold}-{cluster_freq_normalizer}.json\"\n",
    "max_tokens = 10\n",
    "nama_subwords_path=f\"../data/models/fs-{given_surname}-subword-tokenizer-2000f.json\"\n",
    "model_type = 'cecommon+0+aug-0-1'\n",
    "nama_model_path = f\"../data/models/bi_encoder-{given_surname}-{model_type}.pth\"\n",
    "\n",
    "train_path = f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train-v2.csv.gz\"\n",
    "test_path = f\"s3://familysearch-names/processed/tree-hr-{given_surname}-test-v2.csv.gz\"\n",
    "query_path = f\"s3://familysearch-names/processed/query-names-{given_surname}-v2.csv.gz\"\n",
    "pref_path = f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\"\n",
    "nickname_bucket = \"familysearch-names\"\n",
    "nickname_path = \"processed/givenname_nicknames.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190d2c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5157ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# these nicknames include nickname heads going to themselves (e.g., john -> john)\n",
    "nicknames = defaultdict(set)\n",
    "if given_surname == \"given\":\n",
    "    obj = s3.Object(nickname_bucket, nickname_path)\n",
    "    contents = obj.get()['Body'].read().decode('utf-8')\n",
    "    for ix, line in enumerate(contents.split('\\n')):\n",
    "        line = line.strip()\n",
    "        names = line.split(',')\n",
    "        headname = names[0]\n",
    "        for name in names:\n",
    "            nicknames[name].add(headname)\n",
    "print(len(nicknames))\n",
    "print(nicknames['zachery'])\n",
    "print(nicknames['zachariah'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b215bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_names = pd.read_csv(query_path, na_filter=False)[\"name\"].tolist()\n",
    "print(len(query_names))\n",
    "query_names[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pref names\n",
    "pref_df = read_csv(pref_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create common names pref names that occur >= common_name_threshold\n",
    "common_names = [name for name, freq in zip(pref_df['name'], pref_df['frequency']) \\\n",
    "                if len(name) > 1 and re.fullmatch(r'[a-z]+', name)]\n",
    "common_names = common_names[:10000]\n",
    "len(common_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74186497",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path, na_filter=False)\n",
    "print(train_df.shape)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b52952",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_path, na_filter=False)\n",
    "print(test_df.shape)\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([train_df, test_df])\n",
    "print(all_df.shape)\n",
    "all_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7ae378",
   "metadata": {},
   "outputs": [],
   "source": [
    "nama_name_cluster = {}       # name -> cluster position\n",
    "nama_cluster_centroids = []  # centroid for each cluster\n",
    "nama_cluster_labels = []     # label for each cluster\n",
    "nama_cluster_super_cluster = {}  # cluster label -> super_cluster label\n",
    "\n",
    "with open(clusters_path, 'r') as f:\n",
    "    nama_clusters = json.load(f)  # cluster label -> names, centroid\n",
    "\n",
    "with open(super_clusters_path, 'r') as f:\n",
    "    nama_super_clusters = json.load(f)  # super_cluster label -> cluster labels\n",
    "\n",
    "for label, cluster in nama_clusters.items():\n",
    "    for name in cluster['names']:\n",
    "        nama_name_cluster[name] = len(nama_cluster_labels)\n",
    "    nama_cluster_labels.append(label)\n",
    "    nama_cluster_centroids.append(np.array(cluster['centroid']))\n",
    "nama_cluster_labels = np.array(nama_cluster_labels)\n",
    "\n",
    "for super_cluster_label, super_cluster in nama_super_clusters.items():\n",
    "    for cluster_label in super_cluster:\n",
    "        nama_cluster_super_cluster[cluster_label] = super_cluster_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nama_name_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = set(all_df[\"tree_name\"]) | set(all_df[\"record_name\"])\n",
    "cnt = 0\n",
    "for ix, name in enumerate(names):\n",
    "    if name not in nama_name_cluster:\n",
    "        cnt += 1\n",
    "print(len(names), cnt, cnt/len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59809cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_freq = 0\n",
    "cluster_freq = 0\n",
    "for name, freq in zip(all_df['tree_name'], all_df['frequency']):\n",
    "    total_freq += freq\n",
    "    if name in nama_name_cluster:\n",
    "        cluster_freq += freq\n",
    "print(total_freq, cluster_freq, cluster_freq/total_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f5162",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_freq = 0\n",
    "cluster_freq = 0\n",
    "for name, freq in zip(all_df['record_name'], all_df['frequency']):\n",
    "    total_freq += freq\n",
    "    if name in nama_name_cluster:\n",
    "        cluster_freq += freq\n",
    "print(total_freq, cluster_freq, cluster_freq/total_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenize function\n",
    "tokenize, tokenizer_vocab = get_tokenize_function_and_vocab(\n",
    "    max_tokens=max_tokens,\n",
    "    subwords_path=nama_subwords_path,\n",
    ")\n",
    "len(tokenizer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a6efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "nama_model = torch.load(nama_model_path)\n",
    "nama_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d1200",
   "metadata": {},
   "source": [
    "## Set up FamilySearch coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match-spark/pipeline\n",
    "# java -cp target/spark-pipeline.jar org.familysearch.search.spark.py4j.Py4JGateway\n",
    "\n",
    "gateway = JavaGateway()\n",
    "\n",
    "def fs_coder(name):\n",
    "    # can result ever contain multiple comma-separated codes?\n",
    "    # if so, do we index both and query one, or index one and query both?\n",
    "    return gateway.getClusters(name, given_surname == 'surname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_coder('ab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf761e",
   "metadata": {},
   "source": [
    "## Set up nama coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb0943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(name):\n",
    "    embedding = nama_model.get_embedding(tokenize(name)) \n",
    "    embedding /= np.linalg.norm(embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nama_coder_threshold_limit(name, threshold, limit):\n",
    "    codes = []\n",
    "        \n",
    "    # get the primary (indexed) cluster\n",
    "    if name in nama_name_cluster:\n",
    "        # if it is in the cluster dictionary, get that cluster\n",
    "        cluster_label = nama_cluster_labels[nama_name_cluster[name]]\n",
    "    else:\n",
    "        # if it isn't, get the nearest cluster\n",
    "        emb = get_embedding(name)\n",
    "        cluster_label = top_similar_names(emb, nama_cluster_centroids, nama_cluster_labels, \n",
    "                                          threshold=0, top_n=1)[0][0]\n",
    "    # index it under this cluster\n",
    "    codes.append(cluster_label)\n",
    "    \n",
    "    # include additional clusters in this cluster's super-cluster\n",
    "    super_cluster_clusters = nama_super_clusters.get(nama_cluster_super_cluster.get(cluster_label, None), [])\n",
    "    for nearby_cluster in super_cluster_clusters:\n",
    "        # don't check length, because we want all clusters in the super-cluster\n",
    "        if nearby_cluster not in codes:\n",
    "            codes.append(nearby_cluster)\n",
    "\n",
    "    # include additional clusters near this cluster\n",
    "    if limit > len(codes):\n",
    "        emb = get_embedding(name)\n",
    "        nearby_clusters, similarities = top_similar_names(emb, nama_cluster_centroids, nama_cluster_labels,\n",
    "                                                          threshold=threshold, top_n=limit-len(codes))\n",
    "        for nearby_cluster, similarity in zip(nearby_clusters, similarities):\n",
    "            # print(name, nearby_cluster, similarity)\n",
    "            if len(codes) >= limit or similarity < threshold:\n",
    "                break\n",
    "            if nearby_cluster not in codes:\n",
    "                codes.append(nearby_cluster)\n",
    "            \n",
    "    return ','.join(codes)\n",
    "\n",
    "def nama_coder(name):\n",
    "    return nama_coder_threshold_limit(name, nama_threshold, nama_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25541951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample(code):\n",
    "    return ' '.join(nama_clusters[code]['names'][:8])\n",
    "\n",
    "total_codes = 0\n",
    "for name in ['dallan', 'richard', 'solveig', 'evelyn', 'barbara', 'susan', 'henry', 'becca']:\n",
    "    codes = nama_coder_threshold_limit(name, threshold=0.65, limit=40)\n",
    "    codes = codes.split(',')\n",
    "    print(name, len(codes))\n",
    "    total_codes += len(codes)\n",
    "    for code in codes:\n",
    "        print('   ', code, _sample(code))\n",
    "print(total_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802942e",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e69117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_name_codes(coder_name, coder, names):\n",
    "    \n",
    "    def _wrapped_coder(name):\n",
    "        return name, coder(name)\n",
    "    \n",
    "    if coder_name == 'familysearch' or coder_name == 'nama':\n",
    "        results = [_wrapped_coder(name) for name in tqdm(names, mininterval=5.0)]\n",
    "    else:\n",
    "        with WorkerPool() as pool:\n",
    "            results = pool.map(_wrapped_coder, names, progress_bar=True, progress_bar_options={'mininterval': 5.0})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d742bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codes(coder_name, coder, nicknames, names):\n",
    "    # name2codes simulates query: given a name, what codes to lookup\n",
    "    name2codes = defaultdict(set)\n",
    "    # code2names simulates index: given a code, what names are indexed under that code\n",
    "    code2names = defaultdict(set)\n",
    "    # get codes for name - index name under the first code, query name under all codes\n",
    "    for name, codes in get_all_name_codes(coder_name, coder, names):\n",
    "        for ix, code in enumerate(codes.split(',')):\n",
    "            # query code\n",
    "            name2codes[name].add(code)\n",
    "            # add name to code bucket\n",
    "            if ix == 0:\n",
    "                code2names[code].add(name)\n",
    "        if given_surname == \"given\" and name in nicknames:\n",
    "            # query codes for each nickhead of nickname\n",
    "            for nickhead in nicknames[name]:\n",
    "                codes = coder(nickhead)\n",
    "                for code in codes.split(','):\n",
    "                    name2codes[name].add(code)\n",
    "                    # make sure nickhead is added to the code bucket\n",
    "                    code2names[code].add(nickhead)\n",
    "    return name2codes, code2names\n",
    "\n",
    "def eval_clusters(coder_name, coder, nicknames, data_df, query_names):\n",
    "        name2codes, code2names = get_codes(coder_name, coder,\n",
    "                                           # familysearch and nama coders handle nicknames\n",
    "                                           [] if coder_name in ['familysearch', 'nama'] else nicknames,\n",
    "                                           set(data_df[\"tree_name\"]) | set(data_df[\"record_name\"]))\n",
    "        print(\"total names\", len(name2codes))\n",
    "        print(\"total index entries\", sum(len(names) for names in code2names.values()))\n",
    "        print(\"total codes\", len(code2names))\n",
    "        print(\"total queries\", len(query_names))\n",
    "        print(\"total lookups\", sum(len(name2codes[query]) for query in query_names))\n",
    "        precision, recall, f1, f2 = calc_avg_precision_recall(query_names, \n",
    "                                                                       name2codes, \n",
    "                                                                       code2names, \n",
    "                                                                       data_df)\n",
    "        print(f\"precision={precision}, recall={recall} f1={f1} f2={f2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_df = all_df.sample(n=100_000, random_state=42)\n",
    "len(tiny_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca59ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nama_threshold = 0.75\n",
    "nama_limit = 40\n",
    "\n",
    "coders = [\n",
    "#     ('soundex', jellyfish.soundex), \n",
    "#     ('nysiis', jellyfish.nysiis), \n",
    "    ('nama', nama_coder),\n",
    "#     ('familysearch', fs_coder),\n",
    "    ]\n",
    "data_sources = [\n",
    "    ('tiny', tiny_df),\n",
    "#     ('train', train_df),\n",
    "#     ('test', test_df),\n",
    "#     ('all', all_df),\n",
    "    ]\n",
    "for label, data_df in data_sources:\n",
    "    print(label)\n",
    "    for coder_name, coder in coders:\n",
    "        print(coder_name)\n",
    "        eval_clusters(coder_name, coder, nicknames, data_df, query_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
