{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671df72",
   "metadata": {},
   "source": [
    "# Augment standard with preferred names\n",
    "\n",
    "Load a standard and preferred tree names, and determine which common tree names do not appear in the standard.\n",
    "\n",
    "Try to figure out programmatically which bucket they should go into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from py4j.java_gateway import JavaGateway\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.data.normalize import normalize\n",
    "from src.data.utils import read_csv\n",
    "from src.models.biencoder import BiEncoder\n",
    "from src.models.tokenizer import get_tokenize_function_and_vocab\n",
    "from src.models.utils import top_similar_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff83f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "given_surname = \"given\"\n",
    "\n",
    "max_tokens = 10\n",
    "subwords_path=f\"../data/models/fs-{given_surname}-subword-tokenizer-2000f.json\"\n",
    "common_name_threshold = 105\n",
    "pref_path = f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\"\n",
    "std_path = f\"../references/std_{given_surname}.txt\"\n",
    "model_type = 'cecommon+0+aug-0-1'\n",
    "model_path = f\"../data/models/bi_encoder-{given_surname}-{model_type}.pth\"\n",
    "triplets_path=f\"../data/processed/tree-hr-{given_surname}-triplets-v2-1000.csv.gz\"\n",
    "tokenizer_max_length = 32\n",
    "cross_encoder_dir = f\"../data/models/cross-encoder-{given_surname}-10m-265-same-all\"\n",
    "\n",
    "std_augmented_path = f\"../data/processed/std_{given_surname}-augmented.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad87f02c",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read triplets\n",
    "triplets_df = read_csv(triplets_path)\n",
    "print(len(triplets_df))\n",
    "triplets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa1fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load buckets\n",
    "bucket_names = {}\n",
    "bucket_head_names = {}\n",
    "name_buckets = defaultdict(set)\n",
    "\n",
    "with open(std_path) as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        head_names, tail_names = line.split(':')\n",
    "        head_names = head_names.strip()\n",
    "        tail_names = tail_names.strip()\n",
    "        bucket_name = None\n",
    "        names = set()\n",
    "        heads = set()\n",
    "        for name in head_names.split(' '):\n",
    "            if len(name) == 0:\n",
    "                continue\n",
    "            if bucket_name is None:\n",
    "                bucket_name = name\n",
    "            names.add(name)\n",
    "            heads.add(name)\n",
    "        for name in tail_names.split(' '):\n",
    "            if len(name) == 0:\n",
    "                continue\n",
    "            names.add(name)\n",
    "        if len(names) < 1:\n",
    "            continue\n",
    "        for name in names:\n",
    "            name_buckets[name].add(bucket_name)\n",
    "        bucket_names[bucket_name] = names\n",
    "        bucket_head_names[bucket_name] = heads\n",
    "print(len(bucket_names), len(name_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pref names\n",
    "pref_df = read_csv(pref_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total frequency, including names w frequency=1 that aren't in pref_df\n",
    "total_freq = sum(pref_df['frequency']) + len(pref_df[pref_df['frequency'] == 2]) * 2\n",
    "total_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate % of total frequency of the top N names \n",
    "freq = sum(pref_df['frequency'][:117000])\n",
    "print(freq/total_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf12581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create common names pref names that occur >= common_name_threshold\n",
    "common_names = [name for name, freq in zip(pref_df['name'], pref_df['frequency']) \\\n",
    "                if len(name) > 1 and re.fullmatch(r'[a-z]+', name) and freq >= common_name_threshold]\n",
    "len(common_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenize function\n",
    "tokenize, tokenizer_vocab = get_tokenize_function_and_vocab(\n",
    "    max_tokens=max_tokens,\n",
    "    subwords_path=subwords_path,\n",
    ")\n",
    "len(tokenizer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3588988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = torch.load(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cross encoder model\n",
    "ce_model = CrossEncoder(cross_encoder_dir, max_length=tokenizer_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065abb3",
   "metadata": {},
   "source": [
    "## Which names are not in the standard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cnt = 10\n",
    "unseen_names = []\n",
    "for ix, name in enumerate(common_names):\n",
    "    name_pieces = normalize(name, is_surname=given_surname=='surname', dont_return_empty=False)\n",
    "    if len(name_pieces) != 1:\n",
    "        continue\n",
    "    name = name_pieces[0]\n",
    "    if ix % 1000 == 0 and len(unseen_names) > 0:\n",
    "        print(ix, len(unseen_names))\n",
    "        print_cnt = 10\n",
    "    if name in name_buckets:\n",
    "        continue\n",
    "    unseen_names.append(name)\n",
    "    if print_cnt > 0:\n",
    "        print('   ', ix, name)\n",
    "        print_cnt -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03775e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unseen_names))\n",
    "unseen_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34b468",
   "metadata": {},
   "source": [
    "### get name embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fd0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(name):\n",
    "    embedding = model.get_embedding(tokenize(name)) \n",
    "    embedding /= np.linalg.norm(embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d77de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_embeddings_names = np.array(list(name_buckets.keys()))\n",
    "name_embeddings = [get_embedding(name) for name in name_buckets.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df760a52",
   "metadata": {},
   "source": [
    "## Figure out which bucket to put the names into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1499bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_bi_encoder_names(name, threshold=0.5, limit=20):\n",
    "    embedding = get_embedding(name)\n",
    "    return top_similar_names(embedding, name_embeddings, name_embeddings_names, threshold, limit)\n",
    "\n",
    "def get_bi_encoder_bucket_score(name, other_names, other_scores, limit=1):\n",
    "    buckets = Counter()\n",
    "    ix = 0\n",
    "    for other_name, other_score in zip(other_names, other_scores):\n",
    "        if ix == limit:\n",
    "            break\n",
    "        if other_name in name_buckets:\n",
    "            bucket = next(iter(name_buckets[other_name]))\n",
    "            buckets[bucket] += other_score\n",
    "            ix += 1\n",
    "    if len(buckets) == 0:\n",
    "        return None, None\n",
    "    return buckets.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_mean(x,y):\n",
    "    return 2 / (1/x+1/y)\n",
    "\n",
    "def get_cross_encoder_score(name, other_name):\n",
    "    if name == other_name:\n",
    "        return 1.0\n",
    "    score1, score2 = ce_model.predict([[name, other_name], [other_name, name]])        \n",
    "    return harmonic_mean(score1, score2)\n",
    "    \n",
    "def get_cross_encoder_bucket_score(name, other_names, limit=3):\n",
    "    max_name = None\n",
    "    max_score = None\n",
    "    name_scores = []\n",
    "    for other_name in other_names:\n",
    "        if other_name not in name_buckets:\n",
    "            continue\n",
    "        score = get_cross_encoder_score(name, other_name)\n",
    "        name_scores.append((other_name, score))\n",
    "    if len(name_scores) == 0:\n",
    "        return None, None\n",
    "    name_scores = sorted(name_scores, key=lambda x: -x[1])\n",
    "#     print(name_scores)\n",
    "    buckets = Counter()\n",
    "    for ix in range(min(len(name_scores), limit)):\n",
    "        other_name, score = name_scores[ix]\n",
    "        bucket = next(iter(name_buckets[other_name]))\n",
    "        buckets[bucket] += score\n",
    "#     print(buckets)\n",
    "    return buckets.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98243845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match-spark/pipeline\n",
    "# java -cp target/spark-pipeline.jar org.familysearch.search.spark.py4j.Py4JGateway\n",
    "\n",
    "gateway = JavaGateway()\n",
    "\n",
    "def get_fs_bucket_score(name):\n",
    "    bucket = gateway.getClusters(name, given_surname == 'surname')\n",
    "    score = max([get_cross_encoder_score(name, bucket_name) for bucket_name in bucket_names[bucket]])\n",
    "    return bucket, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fa180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplets_bucket_score(name, threshold = 0.4):\n",
    "    df = triplets_df[(triplets_df['anchor'] == name) | (triplets_df['positive'] == name)]\n",
    "    df = df[(df['anchor'] != name) | (df['positive'] != name)]\n",
    "    df = df[df['positive_score'] > threshold]\n",
    "    df = df.sort_values(by='positive_score', ascending=False)\n",
    "    for i in range(len(df)):\n",
    "        top_row = df.iloc[i]\n",
    "        top_name = top_row['anchor'] if top_row['positive'] == name else top_row['positive']\n",
    "        if top_name in name_buckets:\n",
    "            return next(iter(name_buckets[top_name])), top_row['positive_score']\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "name = 'ivanovna'\n",
    "names, scores = get_nearest_bi_encoder_names(name, limit=20)\n",
    "print(names, scores)\n",
    "# names = names[1:]\n",
    "# scores = scores[1:]\n",
    "ce_bucket, ce_score = get_cross_encoder_bucket_score(name, names)\n",
    "print('cross-encoder', ce_bucket, ce_score)\n",
    "be_bucket, be_score = get_bi_encoder_bucket_score(name, names, scores, limit=3)\n",
    "print('bi-encoder3', be_bucket, be_score)\n",
    "be_bucket, be_score = get_bi_encoder_bucket_score(name, names, scores, limit=1)\n",
    "print('bi-encoder1', be_bucket, be_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_names(bucket):\n",
    "    if not bucket:\n",
    "        return ''\n",
    "    return ' '.join(list(bucket_names[bucket])[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e926c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bucket_names), len(name_buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc8118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where did these numbers come from?\n",
    "# they came by looking at individual name scores and fine-tuning by hand\n",
    "# no machine-learning was harmed in the preparation of these numbers\n",
    "# only humans were harmed :-)\n",
    "\n",
    "testing = False\n",
    "\n",
    "fs_weight = 1.65\n",
    "fs_boost = 0.28\n",
    "ce_weight = 1.0\n",
    "triplets_weight = 1.29\n",
    "unseen_boost = 2.0\n",
    "score_threshold = 1.01\n",
    "\n",
    "unseen_names_set = set(unseen_names)\n",
    "\n",
    "for name in unseen_names[:1000] if testing else tqdm(unseen_names):\n",
    "    # gather votes\n",
    "    votes = Counter()\n",
    "    # get fs vote\n",
    "    fs_bucket, fs_score = get_fs_bucket_score(name)\n",
    "    if fs_bucket is not None:\n",
    "        votes[fs_bucket] += fs_score * fs_weight + fs_boost\n",
    "    # get nearby names\n",
    "    names, scores = get_nearest_bi_encoder_names(name)\n",
    "    if len(names) > 0:\n",
    "        # get cross-encoder vote\n",
    "        ce_bucket, ce_score = get_cross_encoder_bucket_score(name, names)\n",
    "        # don't double-count cross-encoder if it already voted up fs\n",
    "        if ce_bucket is not None and ce_bucket != fs_bucket:\n",
    "            votes[ce_bucket] += ce_score * ce_weight\n",
    "        # get bi-encoder votes\n",
    "        # be_bucket, be_score = get_bi_encoder_bucket_score(name, names, scores, limit=1)\n",
    "    # get triplet vote\n",
    "    tri_bucket, tri_score = get_triplets_bucket_score(name)\n",
    "    if tri_bucket is not None:\n",
    "        votes[tri_bucket] += tri_score * triplets_weight\n",
    "\n",
    "    # get winner\n",
    "    winner, score = votes.most_common(1)[0]\n",
    "    \n",
    "    # print stuff if testing\n",
    "    if testing:\n",
    "        print()\n",
    "        print(name)\n",
    "        print('   fs', fs_bucket, fs_score, sample_names(fs_bucket))\n",
    "        print('   ce', ce_bucket, ce_score, sample_names(ce_bucket))\n",
    "        print('  tri', tri_bucket, tri_score, sample_names(tri_bucket))\n",
    "        print(votes)\n",
    "        if score > score_threshold:\n",
    "            print('WINNER', winner, score)\n",
    "        continue\n",
    "\n",
    "    # if winning bucket is unseen, then increase score, similar to as if FS had found it\n",
    "    if winner in unseen_names_set:\n",
    "        score = score * fs_weight + fs_boost\n",
    "        \n",
    "    # add name to existing bucket, or create a new bucket\n",
    "    if score > score_threshold:\n",
    "        name_buckets[name] = {winner}\n",
    "        bucket_names[winner].add(name)\n",
    "    else:\n",
    "        name_buckets[name] = {name}\n",
    "        bucket_names[name] = {name}\n",
    "        bucket_head_names[name] = {name}\n",
    "\n",
    "    # add embedding\n",
    "    name_embeddings_names = np.append(name_embeddings_names, [name], axis=0)\n",
    "    name_embeddings = np.append(name_embeddings, [get_embedding(name)], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bucket_names), len(name_buckets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd40d29",
   "metadata": {},
   "source": [
    "## Review names to see if they should be moved to other buckets\n",
    "\n",
    "**Actually, after looking through a few of these, they are generally better where they are**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015969e2",
   "metadata": {},
   "source": [
    "### calculate bucket centroids"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0e9d4ef",
   "metadata": {},
   "source": [
    "def get_bucket_centroid(bucket):\n",
    "    total_embedding = None\n",
    "    for name in bucket_names[bucket]:\n",
    "        embedding = get_embedding(name)\n",
    "        if total_embedding is None:\n",
    "            total_embedding = embedding\n",
    "        else:\n",
    "            total_embedding += embedding\n",
    "    return total_embedding / len(bucket_names[bucket])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de62f769",
   "metadata": {},
   "source": [
    "bucket_centroids_buckets = np.array(list(bucket_names.keys()))\n",
    "bucket_centroids = [get_bucket_centroid(bucket) for bucket in bucket_names.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9be817",
   "metadata": {},
   "source": [
    "### for each name, is it closer to another bucket's centroid than its own?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7374b49",
   "metadata": {},
   "source": [
    "def get_nearest_buckets(name, threshold=0.5, limit=20):\n",
    "    embedding = get_embedding(name)\n",
    "    return top_similar_names(embedding, bucket_centroids, bucket_centroids_buckets, threshold, limit)\n",
    "\n",
    "def get_nearest_bucket(name, near_buckets):\n",
    "    nearest_bucket = None\n",
    "    nearest_bucket_score = None\n",
    "    for bucket_name in near_buckets:\n",
    "        total_similarity = 0\n",
    "        for other_name in bucket_names[bucket_name]:\n",
    "            total_similarity += get_cross_encoder_score(name, other_name)\n",
    "        avg_similarity = total_similarity / len(bucket_names[bucket_name])\n",
    "        if nearest_bucket is None or avg_similarity > nearest_bucket_score:\n",
    "            nearest_bucket = bucket_name\n",
    "            nearest_bucket_score = avg_similarity\n",
    "    return nearest_bucket"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4203f7b4",
   "metadata": {},
   "source": [
    "# loop over all names\n",
    "for name in tqdm(name_buckets):\n",
    "    # ignore names that were already in buckets\n",
    "    if name not in unseen_names_set:\n",
    "        continue\n",
    "    # consider changing buckets\n",
    "    bucket = next(iter(name_buckets[name]))\n",
    "    # use bi-encoder to get nearest N buckets\n",
    "    nearest_buckets = get_nearest_buckets(name)\n",
    "    if len(nearest_buckets) == 0:\n",
    "        continue\n",
    "    # use cross-encoder to get the average similarity of this name to each name in the bucket\n",
    "    nearest_bucket = get_nearest_bucket(name, nearest_buckets)\n",
    "    # report if the name is closer to another bucket, and if this was an unseen name\n",
    "    if nearest_bucket not in name_buckets[name]:\n",
    "        print(name)\n",
    "        print('   OLD', bucket, ':', sample_names(bucket))\n",
    "        print('   NEW', nearest_bucket, ':', sample_names(nearest_bucket))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bddd9c7",
   "metadata": {},
   "source": [
    "## Save augmented buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(std_augmented_path, 'wt') as f:\n",
    "    for bucket in sorted(bucket_names.keys()):\n",
    "        heads = bucket_head_names[bucket]\n",
    "        head_names = ' '.join([bucket, *sorted([head for head in heads if head != bucket])]).strip()\n",
    "        tail_names = ' '.join([name for name in sorted(bucket_names[bucket]) if name not in heads]).strip()\n",
    "        line = f\"{head_names}: {tail_names}\".strip()\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_augmented_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af699f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
