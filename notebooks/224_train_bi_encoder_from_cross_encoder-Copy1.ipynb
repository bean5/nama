{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139bee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028823c5",
   "metadata": {},
   "source": [
    "# Train a bi-encoder from cross-encoder\n",
    "learn name-to-vec encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d9cdc",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Hyperparameters\n",
    "* epochs\n",
    "* embedding_dim\n",
    "* subword_vocab_size\n",
    "* add 20% to score\n",
    "\n",
    "All tests are score^20% trainall noback10mask 256dim 6epochs\n",
    "\n",
    "| phon/sub | notes  | uni/bi | size | loss | errors | negs |\n",
    "| -------- | ---- | ------ | ------ | ---- | ---- | ------ | ---- |\n",
    "| Subwords | base | unigrams | 2000f |  |  |  |\n",
    "\n",
    "\n",
    "new data @ 12 epochs:         ??? ??? ??, 189 7, 1011 193, 61728\n",
    "old data @ 12 epochs:         132 124 13,   1 0,  330 197, 56461\n",
    "old data w model @ 12 epochs: 145 110 76,   8 0,  482 246, 48875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0364890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.models.biencoder import BiEncoder\n",
    "from src.models.tokenizer import get_tokenize_function_and_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774de195",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_surname = \"given\"\n",
    "num_common_names = 10000\n",
    "report_size = 10000\n",
    "max_tokens = 10\n",
    "\n",
    "vocab_type = 'f'  # tokenizer based upon training name frequency\n",
    "use_pretrained_embeddings = False\n",
    "subword_vocab_size = 2000  # 500, 1000, 1500, 2000\n",
    "under_represented_threshold = 200\n",
    "\n",
    "# hyperparameters\n",
    "embedding_dim = 256\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 12\n",
    "use_amsgrad = False\n",
    "\n",
    "# triplets\n",
    "num_easy_negs = 5\n",
    "\n",
    "train_triplets_path=f\"../data/processed/cross-encoder-triplets-{given_surname}-{num_easy_negs}.csv\"\n",
    "\n",
    "pref_path = f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\"\n",
    "tfidf_path=f\"s3://nama-data/data/models/fs-{given_surname}-tfidf-v2.joblib\"\n",
    "phoneme_vocab_path = f\"s3://nama-data/data/models/fs-{given_surname}-espeak_phoneme_vocab.json\"\n",
    "phoneme_bigrams_vocab_path = f\"s3://nama-data/data/models/fs-{given_surname}-espeak_phoneme_vocab_bigrams.json\"\n",
    "\n",
    "nama_bucket = 'nama-data'\n",
    "subwords_path=f\"../data/models/fs-{given_surname}-subword-tokenizer-{subword_vocab_size}{vocab_type}.json\"\n",
    "\n",
    "common_non_negatives_path = f\"../references/common_{given_surname}_non_negatives.csv\"\n",
    "name_variants_path = f\"../references/{given_surname}_variants.csv\"\n",
    "given_nicknames_path = \"../references/givenname_nicknames.csv\"\n",
    "\n",
    "model_path = f\"../data/models/bi_encoder-{given_surname}-ce-{num_easy_negs}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34474d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c030b7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train triplets\n",
    "train_triplets_df = pd.read_csv(train_triplets_path, na_filter=False)\n",
    "print(len(train_triplets_df))\n",
    "train_triplets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique anchor-pos pairs and anchor-neg pairs\n",
    "anchor_pos = set()\n",
    "anchor_neg = set()\n",
    "for tup in tqdm(train_triplets_df.itertuples()):\n",
    "    anchor = tup.anchor\n",
    "    pos = tup.positive\n",
    "    neg = tup.negative\n",
    "    anchor_pos.add(f\"{anchor}:{pos}\")\n",
    "    anchor_neg.add(f\"{anchor}:{neg}\")\n",
    "print(len(anchor_pos))\n",
    "print(len(anchor_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d0d78",
   "metadata": {},
   "source": [
    "### read common names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee56867",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_df = pd.read_csv(pref_path, na_filter=False)\n",
    "common_names = [name for name in pref_df['name'][:num_common_names].tolist() \\\n",
    "                if len(name) > 1 and re.fullmatch(r'[a-z]+', name)]\n",
    "pref_df = None\n",
    "len(common_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8481a",
   "metadata": {},
   "source": [
    "### read common non-negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b99105",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_non_negatives = set()\n",
    "\n",
    "def add_common_non_negative(name1, name2):\n",
    "    if name1 > name2:\n",
    "        name1, name2 = name2, name1\n",
    "    common_non_negatives.add(f\"{name1}:{name2}\")\n",
    "\n",
    "def is_common_non_negative(name1, name2):\n",
    "    if name1 > name2:\n",
    "        name1, name2 = name2, name1\n",
    "    return f\"{name1}:{name2}\" in common_non_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3754a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_non_negatives_df = pd.read_csv(common_non_negatives_path, na_filter=False)\n",
    "for name1, name2 in common_non_negatives_df.values.tolist():\n",
    "    add_common_non_negative(name1, name2)\n",
    "len(common_non_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f44455",
   "metadata": {},
   "source": [
    "### add name variants to common non-negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af4eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_variants_df = pd.read_csv(name_variants_path, na_filter=False)\n",
    "print(len(name_variants_df))\n",
    "name_variants_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name1, name2 in name_variants_df.values.tolist():\n",
    "    add_common_non_negative(name1, name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce2634",
   "metadata": {},
   "source": [
    "### add given nicknames to common non-negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "if given_surname == \"given\":\n",
    "    with open(given_nicknames_path, \"rt\") as f:\n",
    "        for line in f.readlines():\n",
    "            names = line.split(',')\n",
    "            for name1 in names:\n",
    "                for name2 in names:\n",
    "                    if name1 > name2:\n",
    "                        add_common_non_negative(name1, name2)\n",
    "len(common_non_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CE model to re-score\n",
    "# from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "# cross_encoder_dir = f\"../data/models/cross-encoder-{given_surname}-10m-265-same-all\"\n",
    "# tokenizer_max_length = 32\n",
    "# model = CrossEncoder(cross_encoder_dir, max_length=tokenizer_max_length)\n",
    "\n",
    "# def harmonic_mean(x,y):\n",
    "#     return 2 / (1/x+1/y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a6eec",
   "metadata": {},
   "source": [
    "## Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47871b51",
   "metadata": {},
   "source": [
    "### Get tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize, tokenizer_vocab = get_tokenize_function_and_vocab(\n",
    "    max_tokens=max_tokens,\n",
    "    subwords_path=subwords_path,\n",
    "    nama_bucket=nama_bucket,\n",
    ")\n",
    "len(tokenizer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('dallan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181fba23",
   "metadata": {},
   "source": [
    "### Add anchor-pos-neg triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of (anchor_tokens, pos_tokens, neg_tokens, target_margin)\n",
    "all_data = []\n",
    "for anchor, pos, neg, pos_score, neg_score in tqdm(zip(\n",
    "    train_triplets_df['anchor'],\n",
    "    train_triplets_df['positive'],\n",
    "    train_triplets_df['negative'],\n",
    "    train_triplets_df['positive_score'],\n",
    "    train_triplets_df['negative_score'],\n",
    "), mininterval=2):\n",
    "    anchor_tokens = tokenize(anchor)\n",
    "    pos_tokens = tokenize(pos)\n",
    "    neg_tokens = tokenize(neg)\n",
    "    # use CE model to re-score\n",
    "#     anchor_pos1, anchor_pos2, anchor_neg1, anchor_neg2 = \\\n",
    "#         model.predict([[anchor, pos], [pos, anchor], [anchor, neg], [neg, anchor]])\n",
    "#     pos_score = harmonic_mean(anchor_pos1, anchor_pos2)\n",
    "#     neg_score = harmonic_mean(anchor_neg1, anchor_neg2)    \n",
    "    target_margin = pos_score - neg_score\n",
    "    # anchor, positive, hard-negative\n",
    "    all_data.append({\n",
    "        'anchor': torch.tensor(anchor_tokens),\n",
    "        'pos': torch.tensor(pos_tokens),\n",
    "        'neg': torch.tensor(neg_tokens),\n",
    "        'target': torch.tensor(target_margin, dtype=torch.float),\n",
    "    })\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ca1ed",
   "metadata": {},
   "source": [
    "### Add triplets for names we want to push apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32997ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_apart_pairs = []\n",
    "push_apart_copies = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb430d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anchor, neg in push_apart_pairs:\n",
    "    anchor_tokens = tokenize(anchor)\n",
    "    neg_tokens = tokenize(neg)\n",
    "    for _ in range(push_apart_copies):\n",
    "        all_data.append({\n",
    "            'anchor': torch.tensor(anchor_tokens),\n",
    "            'pos': torch.tensor(anchor_tokens),\n",
    "            'neg': torch.tensor(neg_tokens),\n",
    "            'target': torch.tensor(1.0, dtype=torch.float)\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99431d39",
   "metadata": {},
   "source": [
    "## Analyze training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c83ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id2text = {}\n",
    "for text, id_ in tokenizer_vocab.items():\n",
    "    token_id2text[id_] = text\n",
    "\n",
    "counter = Counter()\n",
    "for row in tqdm(all_data):\n",
    "    for key in ['anchor', 'pos', 'neg']:\n",
    "        for token in row[key].tolist():\n",
    "            if token == 1:\n",
    "                break\n",
    "            counter[token] += 1\n",
    "for ix, (token, cnt) in enumerate(counter.most_common()):\n",
    "    print(ix, token, token_id2text[token], cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('jewel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c6b49",
   "metadata": {},
   "source": [
    "### Find names that contain under-represented tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_represented_token_ids = set([id_ for id_ in tokenizer_vocab.values() if counter[id_] < under_represented_threshold])\n",
    "print(len(under_represented_token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = set()\n",
    "for tup in tqdm(train_triplets_df.itertuples()):\n",
    "    anchor = tup.anchor\n",
    "    pos = tup.positive\n",
    "    neg = tup.negative\n",
    "    all_names.add(anchor)\n",
    "    all_names.add(pos)\n",
    "    all_names.add(neg)\n",
    "all_names.update(common_names)\n",
    "len(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e66b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_represented_names = set()\n",
    "for name in all_names:\n",
    "    found_token = False\n",
    "    for token in tokenize(name):\n",
    "        if token == 1:  # pad\n",
    "            break\n",
    "        if token in under_represented_token_ids:\n",
    "            found_token = True\n",
    "            break\n",
    "    if found_token:\n",
    "        under_represented_names.add(name)\n",
    "print(len(under_represented_names))\n",
    "for name in under_represented_names:\n",
    "    token_counts = []\n",
    "    for token in tokenize(name):\n",
    "        if token == 1:\n",
    "            break\n",
    "        token_counts.append((token, token_id2text[token], counter[token]))\n",
    "    print(name, token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add any under-represented tokens that don't start with ## as under-represented names\n",
    "for token, id_ in tokenizer_vocab.items():\n",
    "    if counter[id_] >= under_represented_threshold:\n",
    "        continue\n",
    "    if '[' in token or '#' in token:\n",
    "        continue\n",
    "    print(token)\n",
    "    under_represented_names.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(under_represented_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0b3de",
   "metadata": {},
   "source": [
    "### Add names that contain under-represented tokens to all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce437155",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for pos in tqdm(under_represented_names):\n",
    "    pos_tokens = tokenize(pos)\n",
    "    for neg in common_names[:under_represented_threshold]:\n",
    "        if pos == neg or is_common_non_negative(pos, neg):\n",
    "            continue\n",
    "        neg_tokens = tokenize(neg)\n",
    "        all_data.append({\n",
    "            'anchor': torch.tensor(pos_tokens),\n",
    "            'pos': torch.tensor(pos_tokens),\n",
    "            'neg': torch.tensor(neg_tokens),\n",
    "            'target': torch.tensor(1.0, dtype=torch.float)\n",
    "        })\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f127ae",
   "metadata": {},
   "source": [
    "## Split training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(all_data, test_size=0.10, random_state=42)\n",
    "print(len(train_data), len(val_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5debcca1",
   "metadata": {},
   "source": [
    "## Re-Analyze training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for row in tqdm(train_data):\n",
    "    for key in ['anchor', 'pos', 'neg']:\n",
    "        for token in row[key].tolist():\n",
    "            if token == 1:\n",
    "                break\n",
    "            counter[token] += 1\n",
    "for ix, (token, cnt) in enumerate(counter.most_common()):\n",
    "    print(ix, token, token_id2text[token], cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, id_ in tokenizer_vocab.items():\n",
    "    if counter[id_] == 0:\n",
    "        print(id_, token, counter[id_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('zetty')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5e413",
   "metadata": {},
   "source": [
    "## Train bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbecdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(anchors, positives, negatives, labels):\n",
    "    # anchor_pos_sim = (anchors * positives).sum(dim=-1)\n",
    "    # anchor_neg_sim = (anchors * negatives).sum(dim=-1)\n",
    "    anchor_pos_sim = F.cosine_similarity(anchors, positives, dim=-1)\n",
    "    anchor_neg_sim = F.cosine_similarity(anchors, negatives, dim=-1)\n",
    "    margin_pred = anchor_pos_sim - anchor_neg_sim\n",
    "    return F.mse_loss(margin_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, verbose=True):\n",
    "    for epoch in range(num_epochs):\n",
    "        # make sure gradient tracking is on\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for ix, data in enumerate(train_loader):\n",
    "            # get batch\n",
    "            anchors = data['anchor']\n",
    "            positives = data['pos']\n",
    "            negatives = data['neg']\n",
    "            target_margins = data['target']\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            anchor_embeddings = model(anchors)  # Shape: (batch_size, embedding_dim)\n",
    "            pos_embeddings = model(positives)  # Shape: (batch_size, embedding_dim)\n",
    "            neg_embeddings = model(negatives)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(anchor_embeddings, pos_embeddings, neg_embeddings, target_margins)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate loss and report\n",
    "            if verbose:\n",
    "                running_loss += loss.item()\n",
    "                if ix % report_size == report_size - 1:\n",
    "                    avg_loss = running_loss / report_size  # loss per batch\n",
    "                    print(f\"Epoch {epoch} batch {ix} loss {avg_loss}\")\n",
    "                    running_loss = 0\n",
    "\n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # disable gradient computation\n",
    "        running_loss = 0\n",
    "        num_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                anchors = data['anchor']\n",
    "                positives = data['pos']\n",
    "                negatives = data['neg']\n",
    "                target_margins = data['target']\n",
    "                anchor_embeddings = model(anchors)  # Shape: (batch_size, embedding_dim)\n",
    "                pos_embeddings = model(positives)  # Shape: (batch_size, embedding_dim)\n",
    "                neg_embeddings = model(negatives)  # Shape: (batch_size, embedding_dim)\n",
    "                loss = loss_fn(anchor_embeddings, pos_embeddings, neg_embeddings, target_margins)\n",
    "                running_loss += loss.item()  \n",
    "                num_val_batches += 1\n",
    "\n",
    "        # calculate average validation loss\n",
    "        val_loss = running_loss / num_val_batches\n",
    "        if verbose:\n",
    "            print(f\"VALIDATION: Epoch {epoch} loss {val_loss}\")\n",
    "        # epoch_model_path = f\"{model_path}-{epoch}\"\n",
    "        # torch.save(model.state_dict, epoch_model_path)\n",
    "        \n",
    "    # return final epoch validation loss\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fefd66",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b2c2b4e",
   "metadata": {},
   "source": [
    "def hyperopt_objective_function(train_data, \n",
    "                                val_data, \n",
    "                                vocab_size,\n",
    "                                max_tokens,\n",
    "                                pad_token,\n",
    "                                verbose=True,\n",
    "                               ):\n",
    "    \n",
    "    def objective(config):\n",
    "        learning_rate = config['learning_rate']\n",
    "        batch_size = config['batch_size']\n",
    "        embedding_dim = config['embedding_dim']\n",
    "        num_epochs = config['num_epochs']\n",
    "        \n",
    "        if verbose:\n",
    "            print('train', config)\n",
    "        \n",
    "        # Create an instance of the bi-encoder model\n",
    "        model = BiEncoder(embedding_dim, vocab_size, max_tokens, pad_token)\n",
    "        # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # model.to(device)\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=use_amsgrad)\n",
    "\n",
    "        # Create data loader\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "        val_loss = train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, verbose=False)\n",
    "        if verbose:\n",
    "            print('val_loss', val_loss)\n",
    "        \n",
    "        return {\n",
    "            'status': STATUS_OK,\n",
    "            'loss': val_loss,\n",
    "            'config': config,            \n",
    "        }\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ccb3a65",
   "metadata": {},
   "source": [
    "# HyperOpt search space\n",
    "search_space = {\n",
    "    \"learning_rate\": hp.loguniform('learning_rate', math.log(1e-4), math.log(1e-2)),\n",
    "    \"batch_size\": hp.choice('batch_size', [8,16,32,64]),\n",
    "    \"embedding_dim\": hp.choice('embedding_dim', [8,16,32,64]),\n",
    "    \"num_epochs\": hp.choice('num_epochs', [5,10,20,40]),\n",
    "}\n",
    "objective = hyperopt_objective_function(train_data=train_data,\n",
    "                                        val_data=val_data,\n",
    "                                        vocab_size=len(tokenizer_vocab),\n",
    "                                        max_tokens=max_tokens,\n",
    "                                        pad_token=tokenizer_vocab['[PAD]'],\n",
    "                                        verbose=True,\n",
    "                                       )\n",
    "trials = Trials()\n",
    "\n",
    "# minimize the objective over the space\n",
    "best = fmin(objective, \n",
    "            search_space, \n",
    "            algo=tpe.suggest, \n",
    "            trials=trials,\n",
    "            max_evals=50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a278ccb0",
   "metadata": {},
   "source": [
    "print(\"best\", best)\n",
    "print(\"results\", trials.results) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a735dc7",
   "metadata": {},
   "source": [
    "batch_size = best_result.config['batch_size']\n",
    "learning_rate = best_result.config['learning_rate']\n",
    "embedding_dim = best_result.config['embedding_dim']\n",
    "num_epochs = best_result.config['num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba77e31",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b444fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_embeddings(tokenizer_vocab, embedding_dim):\n",
    "    embeddings = [[0.0 for _ in range(embedding_dim)] for _ in range(len(tokenizer_vocab))]\n",
    "    dims_per_char = embedding_dim // 26\n",
    "    ones = [1.0] * dims_per_char\n",
    "    for token, id_ in tokenizer_vocab.items():\n",
    "        embedding = embeddings[id_]\n",
    "        if token[0] == '[':\n",
    "            continue\n",
    "        for ch in token.strip('#'):\n",
    "            # set dimensions for ch in embedding to 1.0\n",
    "            start = (ord(ch) - ord('a')) * dims_per_char\n",
    "            embedding[start:start+dims_per_char] = ones\n",
    "        embeddings[id_] = embedding\n",
    "    return torch.FloatTensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Construct pre-trained embeddings based upon bag-of-characters\n",
    "pretrained_embeddings = None\n",
    "if use_pretrained_embeddings:\n",
    "    pretrained_embeddings = get_pretrained_embeddings(tokenizer_vocab, embedding_dim)\n",
    "    \n",
    "# Create an instance of the bi-encoder model\n",
    "model = BiEncoder(embedding_dim, len(tokenizer_vocab), max_tokens, tokenizer_vocab['[PAD]'], pretrained_embeddings)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=use_amsgrad)\n",
    "\n",
    "# Create data loader\n",
    "### NOTE: train on all_data in place of train_data\n",
    "train_loader = DataLoader(all_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5985e2a",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfcbaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
