{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.integration.wandb import WandbLoggerCallback\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from src.data.filesystem import fopen\n",
    "from src.data.utils import load_datasets, train_test_split\n",
    "from src.eval import metrics\n",
    "from src.models.swivel import SwivelModel, get_swivel_embeddings, get_best_swivel_matches\n",
    "from src.models.swivel_encoder import SwivelEncoderModel, convert_names_to_model_inputs, train_swivel_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "wandb_api_key_file = \"\"  # \"../.wandb-api-key\"\n",
    "given_surname = \"given\"\n",
    "vocab_size = 600000 if given_surname == \"given\" else 2100000\n",
    "embed_dim = 100\n",
    "n_epochs = 15\n",
    "optimize_size=0.8\n",
    "validate_size=0.2\n",
    "\n",
    "Config = namedtuple(\"Config\", \"train_path test_path embed_dim swivel_vocab_path swivel_model_path\")\n",
    "config = Config(\n",
    "    train_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train.csv.gz\",\n",
    "    test_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-test.csv.gz\",\n",
    "    embed_dim=embed_dim,\n",
    "    swivel_vocab_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-vocab-{vocab_size}.csv\",\n",
    "    # TODO fix\n",
    "    swivel_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-model-{vocab_size}-{embed_dim}-50.pth\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[train, test] = load_datasets([config.train_path, config.test_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_train, weighted_actual_names_train, candidate_names_train = train\n",
    "input_names_test, weighted_actual_names_test, candidate_names_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"input_names_train\", len(input_names_train))\n",
    "print(\"weighted_actual_names_train\", sum(len(wan) for wan in weighted_actual_names_train))\n",
    "print(\"candidate_names_train\", len(candidate_names_train))\n",
    "\n",
    "print(\"input_names_test\", len(input_names_test))\n",
    "print(\"weighted_actual_names_test\", sum(len(wan) for wan in weighted_actual_names_test))\n",
    "print(\"candidate_names_test\", len(candidate_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_df = pd.read_csv(fopen(config.swivel_vocab_path, \"rb\"))\n",
    "print(vocab_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swivel_vocab = {name: _id for name, _id in zip(vocab_df[\"name\"], vocab_df[\"index\"])}\n",
    "print(len(swivel_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swivel_model = SwivelModel(len(swivel_vocab), config.embed_dim)\n",
    "swivel_model.load_state_dict(torch.load(fopen(config.swivel_model_path, \"rb\")))\n",
    "swivel_model.eval()\n",
    "print(swivel_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Optimize Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create optimization and validation sets from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split out the candidate names into train and validate sets\n",
    "optimize, validate = train_test_split(input_names_train, weighted_actual_names_train, candidate_names_train,\n",
    "                                      train_size=optimize_size, test_size=validate_size)\n",
    "input_names_optimize, weighted_actual_names_optimize, candidate_names_optimize = optimize\n",
    "input_names_validate, weighted_actual_names_validate, candidate_names_validate = validate\n",
    "print(\"input_names_optimize\", len(input_names_optimize))\n",
    "print(\"candidate_names_optimize\", len(candidate_names_optimize))\n",
    "print(\"input_names_validate\", len(input_names_validate))\n",
    "print(\"candidate_names_validate\", len(candidate_names_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Use Ray to perform the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ray_training_function(config,\n",
    "                          swivel_model,\n",
    "                          swivel_vocab,\n",
    "                          input_names_optimize,\n",
    "                          candidate_names_optimize,\n",
    "                          input_names_validate,\n",
    "                          weighted_actual_names_validate,\n",
    "                          candidate_names_validate,\n",
    "                          device,\n",
    "                          checkpoint_dir=None):\n",
    "    names_optimize = list(set(input_names_optimize).union(set(candidate_names_optimize)))\n",
    "    names_optimize_inputs = convert_names_to_model_inputs(names_optimize)\n",
    "    names_optimize_embeddings = torch.Tensor(get_swivel_embeddings(swivel_model, swivel_vocab, names_optimize))\n",
    "\n",
    "    # create model\n",
    "    encoder_model = SwivelEncoderModel(n_layers=config[\"n_layers\"],\n",
    "                               char_embed_dim=config[\"char_embed_dim\"],\n",
    "                               n_hidden_units=config[\"n_hidden_units\"],\n",
    "                               output_dim=config[\"embed_dim\"],\n",
    "                               bidirectional=config[\"bidirectional\"],\n",
    "                               device=device)\n",
    "    encoder_model.to(device=device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(encoder_model.parameters(), lr=config[\"lr\"]) \\\n",
    "        if config[\"use_adam_opt\"] \\\n",
    "        else torch.optim.Adagrad(encoder_model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        encoder_model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    for epoch in range(config[\"n_epochs\"]):\n",
    "        losses = train_swivel_encoder(encoder_model,\n",
    "                                      names_optimize_inputs,\n",
    "                                      names_optimize_embeddings,\n",
    "                                      num_epochs=1,\n",
    "                                      batch_size=config[\"batch_size\"],\n",
    "                                      use_adam_opt=config[\"use_adam_opt\"],\n",
    "                                      pack=config[\"pack\"],\n",
    "                                      verbose=False,\n",
    "                                      optimizer=optimizer)\n",
    "        best_matches = get_best_swivel_matches(None,\n",
    "                                               None,\n",
    "                                               input_names_validate,\n",
    "                                               candidate_names_validate,\n",
    "                                               k=100,\n",
    "                                               batch_size=1024,\n",
    "                                               add_context=True,\n",
    "                                               encoder_model=encoder_model,\n",
    "                                               n_jobs=1)\n",
    "        auc = metrics.get_auc(\n",
    "            weighted_actual_names_validate, best_matches, min_threshold=0.01, max_threshold=2.0, step=0.05, distances=False\n",
    "        )\n",
    "\n",
    "        # Checkpoint the model\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((encoder_model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        # Report the metrics to Ray\n",
    "        tune.report(auc=auc, mean_loss=np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_params={\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"n_layers\": tune.grid_search([2, 3]),\n",
    "    \"char_embed_dim\": 32,  # tune.choice([0, 32, 64]),\n",
    "    \"n_hidden_units\": 400,  # tune.choice([200, 300, 400]),\n",
    "    \"bidirectional\": True,\n",
    "    \"lr\": tune.grid_search([0.02, 0.03]),  # tune.quniform(0.02, 0.07, 0.01),\n",
    "    \"batch_size\": tune.grid_search([128, 256]),  # tune.choice([128, 256]),\n",
    "    \"use_adam_opt\": False,\n",
    "    \"pack\": tune.grid_search([True, False])\n",
    "    \"n_epochs\": n_epochs\n",
    "}\n",
    "\n",
    "current_best_params = [{\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"n_layers\": 2,\n",
    "        \"char_embed_dim\": 32,\n",
    "        \"n_hidden_units\": 400,\n",
    "        \"bidirectional\": True,\n",
    "        \"lr\": 0.02,\n",
    "        \"batch_size\": 256,\n",
    "        \"use_adam_opt\": False,\n",
    "        \"pack\": False,\n",
    "        \"n_epochs\": n_epochs,\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Will try to terminate bad trials early\n",
    "# https://docs.ray.io/en/latest/tune/api_docs/schedulers.html\n",
    "scheduler = ASHAScheduler(max_t=100,\n",
    "                          grace_period=3,\n",
    "                          reduction_factor=4)\n",
    "\n",
    "# https://docs.ray.io/en/latest/tune/api_docs/suggestion.html#tune-hyperopt\n",
    "search_alg = HyperOptSearch(points_to_evaluate=current_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(_redis_max_memory=4*10**9)  # give redis extra memory\n",
    "\n",
    "callbacks = []\n",
    "if wandb_api_key_file:\n",
    "    callbacks.append(WandbLoggerCallback(\n",
    "        project=\"nama\",\n",
    "        entity=\"nama\",\n",
    "        group=\"62_swivel_encoder_tune_\"+given_surname,\n",
    "        notes=\"\",\n",
    "        config=config._asdict(),\n",
    "        api_key_file=wandb_api_key_file\n",
    "    ))\n",
    "\n",
    "result = tune.run(\n",
    "    tune.with_parameters(ray_training_function,\n",
    "                         swivel_model=swivel_model,\n",
    "                         swivel_vocab=swivel_vocab,\n",
    "                         input_names_optimize=input_names_optimize,\n",
    "                         candidate_names_optimize=candidate_names_optimize,\n",
    "                         input_names_validate=input_names_validate,\n",
    "                         weighted_actual_names_validate=weighted_actual_names_validate,\n",
    "                         candidate_names_validate=candidate_names_validate,\n",
    "                         device=device),\n",
    "    resources_per_trial={\"cpu\": 0.5, \"gpu\": 1.0},\n",
    "    config=config_params,\n",
    "#     num_samples=100,\n",
    "#     scheduler=scheduler,\n",
    "#     search_alg=search_alg,\n",
    "#     metric=\"auc\",\n",
    "#     mode=\"max\",\n",
    "#     checkpoint_score_attr=\"auc\",\n",
    "#     time_budget_s=0.3*3600,\n",
    "#     keep_checkpoints_num=100,\n",
    "#     progress_reporter=tune.JupyterNotebookReporter(\n",
    "#         overwrite=False,\n",
    "#         max_report_frequency=5*60\n",
    "#     ),\n",
    "#     callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get trial that has the highest AUC (can also do with mean_loss or any other metric)\n",
    "best_trial_auc = result.get_best_trial(metric='auc', mode='max', scope='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters with the highest AUC\n",
    "best_trial_auc.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Best trial final train loss: {best_trial_auc.last_result['mean_loss']}\")\n",
    "print(f\"Best trial final train auc: {best_trial_auc.last_result['auc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get checkpoint dir for best model\n",
    "best_checkpoint_dir = best_trial_auc.checkpoint.value\n",
    "\n",
    "# Load best model\n",
    "model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, 'checkpoint'))\n",
    "best_trained_model = SwivelEncoderModel(n_layers=best_trial_auc.config[\"n_layers\"],\n",
    "                                        char_embed_dim=best_trial_auc.config[\"char_embed_dim\"],\n",
    "                                        n_hidden_units=best_trial_auc.config[\"n_hidden_units\"],\n",
    "                                        output_dim=embed_dim,\n",
    "                                        bidirectional=best_trial_auc.config[\"bidirectional\"],\n",
    "                                        device=device)\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "best_trained_model.eval()\n",
    "best_trained_model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all trials as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All trials as pandas dataframe\n",
    "df = result.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"auc\"] > 0.78) & (df[\"mean_loss\"] < 0.31)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PR curve on validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pr curve with best model\n",
    "best_matches = get_best_swivel_matches(None,\n",
    "                                       None,\n",
    "                                       input_names_validate,\n",
    "                                       candidate_names_validate,\n",
    "                                       k=100,\n",
    "                                       batch_size=256,\n",
    "                                       add_context=True,\n",
    "                                       encoder_model=best_trained_model,\n",
    "                                       n_jobs=4)\n",
    "\n",
    "metrics.precision_weighted_recall_curve_at_threshold(weighted_actual_names_validate,\n",
    "                                                     best_matches,\n",
    "                                                     min_threshold=0.01,\n",
    "                                                     max_threshold=2.0,\n",
    "                                                     step=0.05,\n",
    "                                                     distances=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PR curve on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot pr curve with best model\n",
    "best_matches = get_best_swivel_matches(swivel_model,\n",
    "                                       swivel_vocab,\n",
    "                                       input_names_test,\n",
    "                                       candidate_names_test,\n",
    "                                       k=100,\n",
    "                                       batch_size=1024,\n",
    "                                       add_context=True,\n",
    "                                       encoder_model=best_trained_model,\n",
    "                                       n_jobs=8)\n",
    "\n",
    "metrics.precision_weighted_recall_curve_at_threshold(weighted_actual_names_test,\n",
    "                                                     best_matches,\n",
    "                                                     min_threshold=0.01,\n",
    "                                                     max_threshold=2.0,\n",
    "                                                     step=0.05,\n",
    "                                                     distances=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rndmx_name_idx = np.random.randint(len(input_names_test))\n",
    "print(f\"Input name:  {input_names_test[rndmx_name_idx]}\")\n",
    "print(\"Nearest names:\")\n",
    "print(best_matches[rndmx_name_idx][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Actual names:\")\n",
    "sorted(weighted_actual_names_test[rndmx_name_idx][:10], key=lambda k: k[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a specific threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision and recall at a specific threshold\n",
    "\n",
    "from src.eval.metrics import precision_at_threshold, weighted_recall_at_threshold\n",
    "\n",
    "threshold = 0.4\n",
    "precision = np.mean([precision_at_threshold(a, c, threshold, distances=False) \\\n",
    "                     for a, c in zip(weighted_actual_names_validate, best_matches)])\n",
    "recall = np.mean([weighted_recall_at_threshold(a, c, threshold, distances=False) \\\n",
    "                     for a, c in zip(weighted_actual_names_validate, best_matches)])\n",
    "\n",
    "print(precision, recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
