{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd0eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b4aaf4",
   "metadata": {},
   "source": [
    "# Generate triplets from cross encoder\n",
    "\n",
    "Generate 500M triplets using the cross encoder from 222\n",
    "- get all pairs from tree-record-attachments and existing standard buckets\n",
    "- add easy-negs from most-frequent 10000 tree pref names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.utils import load_dataset_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5420dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_surname = 'given'\n",
    "\n",
    "num_training_examples = 5_000_000\n",
    "num_easy_negs = 4\n",
    "keep_anchor = False\n",
    "allow_dups = False\n",
    "\n",
    "num_common_names = 10000\n",
    "\n",
    "tokenizer_max_length = 32\n",
    "pref_path = f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\"\n",
    "train_path = f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train-v2.csv.gz\"\n",
    "\n",
    "std_path = f\"../references/std_{given_surname}.txt\"\n",
    "cross_encoder_dir = f\"../data/models/cross-encoder-{given_surname}-10m-265-same-all\"\n",
    "\n",
    "triplets_path=f\"../data/processed/cross-encoder-triplets-{given_surname}-{num_easy_negs}{'-keep' if keep_anchor else ''}{'-dups' if allow_dups else ''}.csv\"\n",
    "triplets_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ac45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe2f0a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b762546",
   "metadata": {},
   "source": [
    "### load common names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_df = pd.read_csv(pref_path, na_filter=False)\n",
    "common_names = [name for name in pref_df['name'][:num_common_names].tolist() \\\n",
    "                if len(name) > 1 and re.fullmatch(r'[a-z]+', name)]\n",
    "pref_df = None\n",
    "len(common_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43e65e",
   "metadata": {},
   "source": [
    "### load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_names_train, attached_names_train, record_names_train = \\\n",
    "    load_dataset_v2(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tree_names_train\", len(tree_names_train))\n",
    "print(\"attached_names_train\", sum(len(attachments) for attachments in attached_names_train))\n",
    "print(\"total pairs\", sum(freq for attachments in attached_names_train for _, freq in attachments))\n",
    "print(\"record_names_train\", len(record_names_train))\n",
    "print(\"total names\", len(set(tree_names_train).union(set(record_names_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdffb457",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree_name, attachments in zip(tree_names_train, attached_names_train):\n",
    "    names = [tree_name]\n",
    "    for name, _ in attachments:\n",
    "        if name not in names:\n",
    "            names.append(name)\n",
    "    if len(names) < 2:\n",
    "        continue\n",
    "    buckets.append(names)\n",
    "print(len(buckets), sum(len(bucket) for bucket in buckets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd03786",
   "metadata": {},
   "source": [
    "### load std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66101a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(std_path) as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        head_names, tail_names = line.split(':')\n",
    "        head_names = head_names.strip()\n",
    "        tail_names = tail_names.strip()\n",
    "        if len(head_names) == 0:\n",
    "            continue\n",
    "        # put all head-names into a bucket\n",
    "        names = []\n",
    "        for name in head_names.split(' '):\n",
    "            if name not in names:\n",
    "                names.append(name)\n",
    "        if len(names) >= 2:\n",
    "            buckets.append(names)\n",
    "        if len(tail_names) == 0:\n",
    "            continue\n",
    "        # for each head-name, create a bucket with head-name and tail-names\n",
    "        for name in head_names.split(' '):\n",
    "            names = [name]\n",
    "            for name in tail_names.split(' '):\n",
    "                if name not in names:\n",
    "                    names.append(name)\n",
    "            if len(names) >= 2:\n",
    "                buckets.append(names)\n",
    "print(len(buckets), sum(len(bucket) for bucket in buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([bucket for bucket in buckets if len(bucket) == 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9eb85d",
   "metadata": {},
   "source": [
    "## Generate and write triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossEncoder(cross_encoder_dir, max_length=tokenizer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_mean(x,y):\n",
    "    return 2 / (1/x+1/y)\n",
    "\n",
    "def choose3(names, keep_anchor, allow_dups):\n",
    "    while True:\n",
    "        anchor = 0 if keep_anchor else random.randrange(len(names))\n",
    "        pos = random.randrange(len(names))\n",
    "        neg = random.randrange(len(names))\n",
    "        if anchor != neg and pos != neg and (anchor != pos or len(names) == 2):\n",
    "            if names[pos] < names[neg]:  # always return pos > neg\n",
    "                neg, pos = pos, neg\n",
    "            return names[anchor], names[pos], names[neg]\n",
    "        \n",
    "def clamp(score):\n",
    "    return max(0.0, min(1.0, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4699d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open each path, get names, write triplets\n",
    "cnt = 0\n",
    "seen_triples = set()\n",
    "seen_cnt = 0\n",
    "with open(triplets_path, 'w', newline='') as f:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.DictWriter(f, fieldnames=['anchor','positive','positive_score','negative','negative_score'])\n",
    "\n",
    "    # Write the column headers\n",
    "    writer.writeheader()\n",
    "\n",
    "    while cnt < num_training_examples:\n",
    "        ix = random.randrange(len(buckets))\n",
    "        if not allow_dups and len(buckets[ix]) < 3:\n",
    "            continue\n",
    "        triple = choose3(buckets[ix], keep_anchor=keep_anchor, allow_dups=allow_dups)\n",
    "        if triple in seen_triples:\n",
    "            seen_cnt += 1\n",
    "            continue\n",
    "        seen_triples.add(triple)\n",
    "        anchor, pos, neg = triple\n",
    "        \n",
    "        # Write the triple\n",
    "        anchor_pos1, anchor_pos2, anchor_neg1, anchor_neg2 = \\\n",
    "            model.predict([[anchor, pos], [pos, anchor], [anchor, neg], [neg, anchor]])\n",
    "        anchor_pos = harmonic_mean(anchor_pos1, anchor_pos2)\n",
    "        anchor_neg = harmonic_mean(anchor_neg1, anchor_neg2)\n",
    "        if anchor == pos:\n",
    "            anchor_pos = 1.0\n",
    "        if anchor == neg:\n",
    "            anchor_neg = 1.0\n",
    "        anchor_pos = clamp(anchor_pos)\n",
    "        anchor_neg = clamp(anchor_neg)\n",
    "        if anchor_pos < anchor_neg:\n",
    "            pos, neg = neg, pos\n",
    "            anchor_pos, anchor_neg = anchor_neg, anchor_pos\n",
    "        writer.writerow({\n",
    "            'anchor': f\"<{anchor}>\", \n",
    "            'positive': f\"<{pos}>\", \n",
    "            'positive_score': anchor_pos, \n",
    "            'negative': f\"<{neg}>\", \n",
    "            'negative_score': anchor_neg,\n",
    "        })\n",
    "        cnt += 1\n",
    "        if cnt % 10_000 == 0:\n",
    "            print(cnt, seen_cnt, datetime.now())\n",
    "\n",
    "        # add easy negatives\n",
    "        i = 0\n",
    "        while (i < num_easy_negs):\n",
    "            easy_neg = random.choice(common_names)\n",
    "            scores = model.predict([[anchor, easy_neg], [easy_neg, anchor]])\n",
    "            score = harmonic_mean(scores[0], scores[1])\n",
    "            \n",
    "            # write anchor, pos, easy-neg\n",
    "            if score >= anchor_pos:\n",
    "                continue\n",
    "            writer.writerow({\n",
    "                'anchor': f\"<{anchor}>\", \n",
    "                'positive': f\"<{pos}>\", \n",
    "                'positive_score': anchor_pos, \n",
    "                'negative': f\"<{easy_neg}>\", \n",
    "                'negative_score': score,\n",
    "            })\n",
    "            i += 1\n",
    "            cnt += 1\n",
    "            if cnt % 10_000 == 0:\n",
    "                print(cnt, seen_cnt, datetime.now())            \n",
    "                \n",
    "            # write anchor, neg, easy-neg\n",
    "            if i == num_easy_negs:\n",
    "                break\n",
    "            if score >= anchor_neg:\n",
    "                continue\n",
    "            writer.writerow({\n",
    "                'anchor': f\"<{anchor}>\", \n",
    "                'positive': f\"<{neg}>\", \n",
    "                'positive_score': anchor_neg, \n",
    "                'negative': f\"<{easy_neg}>\", \n",
    "                'negative_score': score,\n",
    "            })\n",
    "            i += 1\n",
    "            cnt += 1\n",
    "            if cnt % 10_000 == 0:\n",
    "                print(cnt, seen_cnt, datetime.now())             \n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700a261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
