{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671df72",
   "metadata": {},
   "source": [
    "# Generate clusters using the trained bi-encoder model\n",
    "\n",
    "**Deprecated in favor of 241_create_clusters_from_buckets**\n",
    "\n",
    "Load the parser and trained model, and use a hierarchal agglomerative clustering algorithm to cluster the most-frequent names into clusters based upon similarity computed using the trained model. Each cluster will contain names that the model determines are similar to each other. \n",
    "\n",
    "We want to create cohesive clusters, but not too many, because we will ultimately map the existing Buckets to one or more clusters. Each cluster can appear in multiple Buckets, but again we want to limit the number of Buckets that contain the same cluster.\n",
    "\n",
    "At index time, each name will be mapped to a single cluster and indexed under that cluster. At query time, each name will again be mapped to a single cluster, but we will look up all clusters in the Bucket(s) in which the queried cluster appears.\n",
    "\n",
    "Each cluster will contain:\n",
    "\n",
    "1. a list of names, \n",
    "2. the most-common name as the cluster label, and \n",
    "3. a cluster centroid: a vector depicting the centroid of the cluster. \n",
    "\n",
    "## Todo\n",
    "- try ward, complete linkage\n",
    "- merge clusters that only have names in the same bucket?\n",
    "- use buckets to guide merging and splitting clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.data.utils import read_csv\n",
    "from src.models.biencoder import BiEncoder\n",
    "from src.models.tokenizer import get_tokenize_function_and_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff83f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "given_surname = \"given\"\n",
    "\n",
    "linkage = \"average\"  # ward, average, complete, single\n",
    "similarity_threshold = 0.73\n",
    "\n",
    "max_tokens = 10\n",
    "subwords_path=f\"../data/models/fs-{given_surname}-subword-tokenizer-2000f.json\"\n",
    "num_common_names = 100_000\n",
    "pref_path = f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\"\n",
    "std_path = f\"../references/std_{given_surname}.txt\"\n",
    "model_type = 'cecommon+0+aug-0-1'\n",
    "model_path = f\"../data/models/bi_encoder-{given_surname}-{model_type}.pth\"\n",
    "\n",
    "clusters_path = f\"../data/models/clusters_{given_surname}-{similarity_threshold}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad87f02c",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa1fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load buckets\n",
    "bucket_names = []\n",
    "name_buckets = defaultdict(set)\n",
    "\n",
    "with open(std_path) as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        head_names, tail_names = line.split(':')\n",
    "        head_names = head_names.strip()\n",
    "        tail_names = tail_names.strip()\n",
    "        names = set()\n",
    "        for name in head_names.split(' '):\n",
    "            if len(name) > 0 and name not in names:\n",
    "                names.add(name)\n",
    "        for name in tail_names.split(' '):\n",
    "            if len(name) > 0 and name not in names:\n",
    "                names.add(name)\n",
    "        if len(names) < 1:\n",
    "            continue\n",
    "        for name in names:\n",
    "            name_buckets[name].add(len(bucket_names))\n",
    "        bucket_names.append(names)\n",
    "print(len(bucket_names), len(name_buckets), \n",
    "      sum(len(names) for names in bucket_names), \n",
    "      sum(len(buckets) for buckets in name_buckets.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf12581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pref names\n",
    "pref_df = read_csv(pref_path)\n",
    "common_names = [name for name in pref_df['name'][:num_common_names].tolist() \\\n",
    "                if len(name) > 1 and re.fullmatch(r'[a-z]+', name)]\n",
    "pref_df = None\n",
    "len(common_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_cluster = []\n",
    "seen_names = set()\n",
    "for name in common_names:\n",
    "    if name not in seen_names:\n",
    "        names_to_cluster.append(name)\n",
    "        seen_names.add(name)\n",
    "for names in bucket_names:\n",
    "    for name in names:\n",
    "        if name not in seen_names:\n",
    "            names_to_cluster.append(name)\n",
    "            seen_names.add(name)\n",
    "del seen_names\n",
    "len(names_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba263e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenize function\n",
    "tokenize, tokenizer_vocab = get_tokenize_function_and_vocab(\n",
    "    max_tokens=max_tokens,\n",
    "    subwords_path=subwords_path,\n",
    ")\n",
    "len(tokenizer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = torch.load(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df760a52",
   "metadata": {},
   "source": [
    "## Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_embedding = {}\n",
    "for name in tqdm(names_to_cluster):\n",
    "    embedding = model.get_embedding(tokenize(name))\n",
    "    if linkage == \"ward\":\n",
    "        embedding /= np.linalg.norm(embedding)            \n",
    "    name_embedding[name] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.get_embedding(tokenize('dallan'))))\n",
    "model.get_embedding(tokenize('dallan'))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceeba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test normalize\n",
    "embedding = model.get_embedding(tokenize('dallan'))\n",
    "print(embedding[:20])\n",
    "norm = np.linalg.norm(embedding)\n",
    "print(norm)\n",
    "embedding /= norm\n",
    "print(math.sqrt(sum([v*v for v in embedding])))\n",
    "embedding[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test embeddings\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# aaltje 0.21964864\n",
    "# altgen 0.45249435\n",
    "# eltje 0.08212702\n",
    "# aeltje 0.18246093\n",
    "# aalken 0.11775353\n",
    "# aaltjen 0.253144\n",
    "\n",
    "bucket = ['altgen', 'altgen', 'altgin', 'altino', 'aaltje', 'eltje', 'aeltje', 'aalken', 'aaltjen', ]\n",
    "emb1 = model.get_embedding(tokenize(bucket[0]))\n",
    "print(bucket[0])\n",
    "for name in bucket[1:]:\n",
    "    emb2 = model.get_embedding(tokenize(name))\n",
    "    torch_sim = F.cosine_similarity(torch.Tensor(emb1), torch.Tensor(emb2), dim=-1)\n",
    "    sklearn_sim = cosine_similarity([emb1], [emb2])[0]\n",
    "    print(name, torch_sim, sklearn_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a78cef2",
   "metadata": {},
   "source": [
    "## Create closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def create_transitive_closures(vectors, threshold):\n",
    "#     # Function to find the connected components (closures)\n",
    "#     def find_connected_components(graph):\n",
    "#         visited = set()\n",
    "#         components = []\n",
    "\n",
    "#         def dfs(node, component):\n",
    "#             visited.add(node)\n",
    "#             component.add(node)\n",
    "#             for neighbor in graph[node]:\n",
    "#                 if neighbor not in visited:\n",
    "#                     dfs(neighbor, component)\n",
    "\n",
    "#         for node in graph:\n",
    "#             if node not in visited:\n",
    "#                 component = set()\n",
    "#                 dfs(node, component)\n",
    "#                 components.append(component)\n",
    "\n",
    "#         return components\n",
    "\n",
    "#     # Normalize vectors for cosine similarity calculation\n",
    "#     vectors = np.array(vectors)\n",
    "#     norms = np.linalg.norm(vectors, axis=1)\n",
    "#     normalized_vectors = vectors / norms[:, np.newaxis]\n",
    "\n",
    "#     # Create an adjacency list for the graph\n",
    "#     graph = defaultdict(set)\n",
    "\n",
    "#     # Populate the graph based on the cosine similarity threshold\n",
    "#     for i in tqdm(range(len(normalized_vectors))):\n",
    "#         # Compute cosine similarity of vector i with all other vectors\n",
    "#         similarities = np.dot(normalized_vectors, normalized_vectors[i])\n",
    "#         for j, similarity in enumerate(similarities):\n",
    "#             if i != j and similarity > threshold:\n",
    "#                 graph[i].add(j)\n",
    "#                 graph[j].add(i)\n",
    "\n",
    "#     # Find the connected components (transitive closures)\n",
    "#     closures = find_connected_components(graph)\n",
    "\n",
    "#     # Convert set to list for each closure\n",
    "#     closures = [list(closure) for closure in closures]\n",
    "\n",
    "#     return closures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92207009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embeddings_to_cluster = [name_embedding[name] for name in names_to_cluster]\n",
    "# closures = create_transitive_closures(embeddings_to_cluster, threshold=similarity_threshold)\n",
    "# print(len(closures))\n",
    "# for closure in closures:\n",
    "#     if len(closure) > 50_000:\n",
    "#         print(len(closure))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3367ceb",
   "metadata": {},
   "source": [
    "## Cluster names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    metric=\"euclidean\" if linkage == \"ward\" else \"cosine\",\n",
    "    linkage=linkage,\n",
    "    distance_threshold=(1.0 - similarity_threshold),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test clusterer\n",
    "bucket = ['abraham','abe','aabraham','ab','abaham','abaraham','abarham','abb','abelarde','abera','aberaham']\n",
    "X = []\n",
    "names = []\n",
    "for name in bucket:\n",
    "    embedding = name_embedding[name]\n",
    "    names.append(name)\n",
    "    X.append(embedding)\n",
    "clustering = clusterer.fit(X)\n",
    "cluster_names = [set() for _ in range(clustering.n_clusters_)]\n",
    "print('n_clusters', clustering.n_clusters_)\n",
    "print('labels', clustering.labels_)\n",
    "print('names', names)\n",
    "for name, cluster in zip(names, clustering.labels_):\n",
    "    cluster_names[cluster].add(name)\n",
    "for ix, names in enumerate(cluster_names):\n",
    "    print(ix, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30331a86",
   "metadata": {},
   "source": [
    "### run clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3171458",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_size = 80_000\n",
    "\n",
    "X = []\n",
    "clustered_names = []\n",
    "for name in names_to_cluster[:sample_size] if sample_size else names_to_cluster:\n",
    "    embedding = name_embedding[name]\n",
    "    clustered_names.append(name)\n",
    "    X.append(embedding)\n",
    "    \n",
    "clustering = clusterer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55325152",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = [set() for _ in range(clustering.n_clusters_)]\n",
    "for name, cluster in zip(clustered_names, clustering.labels_):\n",
    "    cluster_names[cluster].add(name)\n",
    "    \n",
    "len(cluster_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36337a99",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c319d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cluster = {}\n",
    "for ix, names in enumerate(cluster_names):\n",
    "    for name in names:\n",
    "        name_cluster[name] = ix\n",
    "len(name_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_buckets(cluster, verbose=False):\n",
    "    \"\"\"Return all of the buckets each name in the cluster appears in.\"\"\"\n",
    "    buckets = []\n",
    "    for name in cluster_names[cluster]:\n",
    "        if verbose:\n",
    "            print('  get_cluster_buckets', name, name_buckets.get(name, []))\n",
    "        buckets.extend(name_buckets.get(name, []))\n",
    "    return list(set(buckets))\n",
    "\n",
    "def get_cluster_lookups(name, verbose=False):\n",
    "    \"\"\"Return all of the clusters that have to be looked up when a name is searched.\"\"\"\n",
    "    # get the cluster of the name\n",
    "    cluster = name_cluster[name]\n",
    "    if verbose:\n",
    "        print('cluster', cluster, cluster_names[cluster])\n",
    "    # get all of the buckets that names in this cluster appear in\n",
    "    buckets = get_cluster_buckets(cluster)\n",
    "    if verbose:\n",
    "        print('buckets', buckets)\n",
    "    # for each bucket, get all of the clusters associated with the names in that bucket\n",
    "    clusters = set([cluster])\n",
    "    for bucket in buckets:\n",
    "        if verbose:\n",
    "            print('> bucket', bucket, bucket_names[bucket])\n",
    "        for bucket_name in bucket_names[bucket]:\n",
    "            if bucket_name in name_cluster:\n",
    "                if verbose:\n",
    "                    print('    bucket name', bucket_name, \n",
    "                          name_cluster[bucket_name], cluster_names[name_cluster[bucket_name]])\n",
    "                clusters.add(name_cluster[bucket_name])\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9560554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average number of buckets per cluster\n",
    "total_buckets = sum([len(get_cluster_buckets(cluster)) for cluster in range(len(cluster_names))])\n",
    "print('total, avg #buckets per cluster', total_buckets, total_buckets / len(cluster_names))\n",
    "\n",
    "# average number of clusters per bucket\n",
    "total_clusters = 0\n",
    "for bucket in range(len(bucket_names)):\n",
    "    clusters = set()\n",
    "    for name in bucket_names[bucket]:\n",
    "        if name in name_cluster:\n",
    "            clusters.add(name_cluster[name])\n",
    "    total_clusters += len(clusters)\n",
    "print('total, avg #clusters per bucket', total_clusters, total_clusters / len(bucket_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of lookups for each common name\n",
    "name_lookups = []\n",
    "for name in common_names[:sample_size] if sample_size else common_names:\n",
    "    lookups = get_cluster_lookups(name)\n",
    "    name_lookups.append(len(lookups))\n",
    "    \n",
    "print('top 100', sum(name_lookups[:100]) / 100)\n",
    "print('top 1000', sum(name_lookups[:1000]) / 1000)\n",
    "print('top 10000', sum(name_lookups[:10000]) / 10000)\n",
    "print('all', sum(name_lookups) / len(name_lookups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ffeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['richard', 'james', 'susan', 'elizabeth', 'mary', 'john']:\n",
    "    print('\\nNAME', name)\n",
    "    \n",
    "    # print all of the names in the bucket for name\n",
    "    temp_bucket_names = set()\n",
    "    for bucket in name_buckets[name]:\n",
    "        temp_bucket_names |= bucket_names[bucket]\n",
    "        print('bucket', bucket, bucket_names[bucket])\n",
    "\n",
    "    # print all of names in each cluster looked up\n",
    "    all_new_names = set()\n",
    "    for cluster in get_cluster_lookups(name, verbose=True):\n",
    "        new_names = cluster_names[cluster] - temp_bucket_names\n",
    "        all_new_names |= new_names\n",
    "        old_names = cluster_names[cluster] - new_names\n",
    "        print('cluster', cluster, 'IN BUCKET', old_names, 'NEW', new_names)\n",
    "    print('all new names', all_new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e31383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5bde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather every 25'th name into an experiment\n",
    "experiment = {}\n",
    "for ix, (label, sub_buckets) in enumerate(bucket_sub_buckets.items()):\n",
    "    if ix % 25 != 0:\n",
    "        continue\n",
    "    experiment[label] = sub_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_sort_key(name):\n",
    "    freq = name_freq.get(name, 0)\n",
    "    return f\"{freq:12d}:{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "lines.append(f\"Experiment: {experiment_name}\")\n",
    "lines.append(f\"sub-buckets={sub_bucket_count}\")\n",
    "for label, sub_buckets in experiment.items():\n",
    "    lines.append(label)\n",
    "    sub_buckets.sort(key=lambda bucket: name_sort_key(get_most_freq_name(bucket)), reverse=True)\n",
    "    for sub_bucket in sub_buckets:\n",
    "        sub_bucket.sort(key=name_sort_key, reverse=True)\n",
    "        lines.append(f\"- {get_most_freq_name(sub_bucket)}: {' '.join(sub_bucket)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9181ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828af73",
   "metadata": {},
   "source": [
    "## Save experiment report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46716fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_filename = f\"{experiment_name}.txt\"\n",
    "with open(os.path.join(experiment_dir, experiment_filename), 'wt') as f:\n",
    "    f.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbfb4ce",
   "metadata": {},
   "source": [
    "## Save sub-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for sub_buckets in bucket_sub_buckets.values():\n",
    "    all_names = [name for sub_bucket in sub_buckets for name in sub_bucket]\n",
    "    cluster_label = get_most_freq_name(all_names)\n",
    "    clusters[cluster_label] = {}\n",
    "    for sub_bucket in sub_buckets:\n",
    "        sub_cluster_label = get_most_freq_name(sub_bucket)\n",
    "        clusters[cluster_label][sub_cluster_label] = sub_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_clusters_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sub_clusters_path, 'wt') as f:\n",
    "    json.dump(clusters, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae3e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
