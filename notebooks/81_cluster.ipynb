{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907945b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc7fe6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train a clustering model\n",
    "Once we have a good scoring function (using the ensemble model), we can use a standard clustering algorithm to\n",
    "group names into clusters.\n",
    "We've found that many smaller clusters gives better F1 and F2 measures than fewer larger clusters, so long as you're willing to search multiple clusters scoring above a threshold.\n",
    "In addition, FamilySearch wants to split records into different partitions (index shards) based upon the surname.\n",
    "Partitions are higher-level groupings of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d131857",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "from collections import namedtuple, defaultdict\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from src.data.normalize import normalize_freq_names\n",
    "from src.data.utils import load_dataset\n",
    "from src.data.filesystem import fopen\n",
    "from src.models.cluster import (\n",
    "    get_names_to_cluster,\n",
    "    get_distances,\n",
    "    generate_clusters_from_distances,\n",
    "    write_clusters,\n",
    "    read_clusters,\n",
    ")\n",
    "from src.models.swivel import SwivelModel\n",
    "from src.models.utils import remove_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bdec3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# configure\n",
    "given_surname = \"given\"\n",
    "vocab_size = 610000 if given_surname == \"given\" else 2100000\n",
    "save_partitions = given_surname != \"given\"\n",
    "max_partitions = 0 if given_surname == \"given\" else 720\n",
    "n_to_cluster = 200000 if given_surname == \"given\" else 500000\n",
    "cluster_threshold = 0.4 if given_surname == \"given\" else 0.6\n",
    "n_jobs = 64\n",
    "\n",
    "embed_dim = 100\n",
    "encoder_layers = 2\n",
    "num_matches = 1000\n",
    "batch_size = 256\n",
    "verbose = True\n",
    "\n",
    "Config = namedtuple(\"Config\", [\n",
    "    \"eval_path\",\n",
    "    \"tree_freq_path\",\n",
    "    \"hr_freq_path\",\n",
    "    \"embed_dim\",\n",
    "    \"swivel_vocab_path\",\n",
    "    \"swivel_model_path\",\n",
    "    \"tfidf_path\",\n",
    "    \"ensemble_model_path\",\n",
    "    \"cluster_partition_path\",\n",
    "    \"cluster_path\",\n",
    "])\n",
    "config = Config(\n",
    "    eval_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train.csv.gz\",\n",
    "    tree_freq_path=f\"s3://familysearch-names/processed/tree-preferred-{given_surname}-aggr.csv.gz\",\n",
    "    hr_freq_path=f\"s3://familysearch-names-private/hr-preferred-{given_surname}-aggr.csv.gz\",\n",
    "    embed_dim=embed_dim,\n",
    "    swivel_vocab_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-vocab-{vocab_size}-augmented.csv\",\n",
    "    swivel_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-model-{vocab_size}-{embed_dim}-augmented.pth\",\n",
    "    tfidf_path=f\"s3://nama-data/data/models/fs-{given_surname}-tfidf.joblib\",\n",
    "    ensemble_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-ensemble-model-{vocab_size}-{embed_dim}-augmented-100.joblib\",\n",
    "    cluster_partition_path=f\"s3://nama-data/data/models/fs-{given_surname}-cluster-partitions.csv\",\n",
    "    cluster_path=f\"s3://nama-data/data/models/fs-{given_surname}-cluster-names.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfc8f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"nama\",\n",
    "    entity=\"nama\",\n",
    "    name=\"81_cluster\",\n",
    "    group=given_surname,\n",
    "    notes=\"\",\n",
    "    config=config._asdict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190d2c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa76e0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b8c92",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_eval, weighted_actual_names_eval, candidate_names_eval = load_dataset(config.eval_path, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06cd44e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "actual_names_eval = set([name for wans in weighted_actual_names_eval for name, _, _ in wans])\n",
    "candidate_names_eval = np.array(list(actual_names_eval))\n",
    "del actual_names_eval\n",
    "print(len(candidate_names_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282f6c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = pd.read_csv(config.tree_freq_path, na_filter=False)\n",
    "tree_name_freq = normalize_freq_names(freq_df, is_surname=given_surname != \"given\", add_padding=True)\n",
    "freq_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b42b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = pd.read_csv(config.hr_freq_path, na_filter=False)\n",
    "hr_name_freq = normalize_freq_names(freq_df, is_surname=given_surname != \"given\", add_padding=True)\n",
    "freq_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe2878",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create partitions based upon hr freq\n",
    "# but clusters based upon tree freq so we get consistent cluster names\n",
    "partition_name_freq = hr_name_freq\n",
    "cluster_name_freq = tree_name_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ca49f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_df = pd.read_csv(fopen(config.swivel_vocab_path, \"rb\"))\n",
    "swivel_vocab = {name: _id for name, _id in zip(vocab_df[\"name\"], vocab_df[\"index\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597e75d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swivel_model = SwivelModel(len(swivel_vocab), config.embed_dim)\n",
    "swivel_model.load_state_dict(torch.load(fopen(config.swivel_model_path, \"rb\"), map_location=torch.device(device)))\n",
    "swivel_model.to(device)\n",
    "swivel_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f139a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = joblib.load(fopen(config.tfidf_path, mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b5fcf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_model = joblib.load(fopen(config.ensemble_model_path, mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948df4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get names to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a943799",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO pass in swivel_vocab and ensure that only names in swivel_vocab were selected to cluster\n",
    "names_to_cluster = get_names_to_cluster(cluster_name_freq, n_to_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80414d65",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute cluster hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a6e62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "distances = get_distances(cluster_name_freq, \n",
    "                          names_to_cluster,\n",
    "                          swivel_model=swivel_model,\n",
    "                          swivel_vocab=swivel_vocab,\n",
    "                          tfidf_vectorizer=tfidf_vectorizer,\n",
    "                          ensemble_model=ensemble_model,\n",
    "                          num_matches=num_matches,\n",
    "                          verbose=verbose,\n",
    "                          n_jobs=n_jobs,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fff038",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model, name_cluster = generate_clusters_from_distances(\n",
    "                            cluster_algo=\"agglomerative\",\n",
    "                            cluster_linkage=\"average\",\n",
    "                            cluster_threshold=-10.0,  # initially put everything into a single cluster\n",
    "                            distances=distances,\n",
    "                            names_to_cluster=names_to_cluster,\n",
    "                            verbose=verbose,\n",
    "                            n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9682df",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save names to cluster\n",
    "with fopen(f\"s3://nama-data/data/models/fs-{given_surname}-cluster-model-names.pickle\", \"wb\") as f:\n",
    "    pickle.dump(names_to_cluster, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d35812",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "joblib.dump(model, fopen(f\"s3://nama-data/data/models/fs-{given_surname}-cluster-model.joblib\", mode=\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c5237",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load names to cluster and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420c869",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load names to cluster\n",
    "with fopen(f\"s3://nama-data/data/models/fs-{given_surname}-cluster-model-names.pickle\", \"rb\") as f:\n",
    "    names_to_cluster = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dfdb2d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "with fopen(f\"s3://nama-data/data/models/fs-{given_surname}-cluster-model.joblib\", \"rb\") as f:\n",
    "    model = joblib.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16f182",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Split surnames into partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e309a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(names_to_cluster))\n",
    "print(len(model.children_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547bd5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model.children_ is a list of all non-leaf nodes in the cluster hierarchy that contains their immediate children\n",
    "missing_names = 0\n",
    "leaf_node_count = len(names_to_cluster)\n",
    "non_leaf_node_count = len(model.children_)\n",
    "total_node_count = leaf_node_count + non_leaf_node_count\n",
    "\n",
    "# count the total name frequency in each leaf and non-leaf node\n",
    "cluster_freq = np.zeros(total_node_count)\n",
    "for ix in range(0, leaf_node_count):\n",
    "    if names_to_cluster[ix] not in partition_name_freq:\n",
    "        missing_names += 1\n",
    "    cluster_freq[ix] = partition_name_freq.get(names_to_cluster[ix], 1)\n",
    "\n",
    "for ix in range(0, non_leaf_node_count):\n",
    "    count = 0\n",
    "    for child in model.children_[ix]:\n",
    "        count += cluster_freq[child]\n",
    "    cluster_freq[ix + leaf_node_count] = count\n",
    "\n",
    "print(\"missing names\", missing_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b10d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make bisect with a key function work in python 3.9\n",
    "class KeyWrapper:\n",
    "    def __init__(self, iterable, key):\n",
    "        self.it = iterable\n",
    "        self.key = key\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.key(self.it[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.it)\n",
    "\n",
    "def insert_sorted(items, item):\n",
    "    ix = bisect_left(KeyWrapper(items, key=lambda item: item[0]), item[0])\n",
    "    items.insert(ix, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56470d7a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# starting with the partition at the root of the cluster hierarchy, split the largest partition until you have max_partitions\n",
    "initial_node = total_node_count - 1\n",
    "print(\"total = \", cluster_freq[initial_node])\n",
    "print(\"average=\", cluster_freq[initial_node] / max_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9867d7b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate max_partitions within min..max size \n",
    "max_partition_size = 13137845\n",
    "min_partition_size = 10000000\n",
    "total_partitions = 1\n",
    "partitions = []\n",
    "insert_sorted(partitions, (cluster_freq[initial_node], [initial_node], 1))\n",
    "\n",
    "# split phase\n",
    "while save_partitions and partitions[-1][0] > max_partition_size:\n",
    "    # split the largest partition\n",
    "    freq, nodes, n_partitions = partitions.pop()\n",
    "    total_partitions -= n_partitions\n",
    "    node = nodes[0]\n",
    "    # if this is a merged partition, then split it apart\n",
    "    if len(nodes) > 1:\n",
    "        for node in nodes:\n",
    "            insert_sorted(partitions, (cluster_freq[node], [node], 1))\n",
    "            total_partitions += 1\n",
    "    # if this is a leaf node that needs to be split, this will be a multi-partition leaf\n",
    "    elif node < leaf_node_count:\n",
    "        n_partitions += 1\n",
    "        total_partitions += n_partitions\n",
    "        insert_sorted(partitions, (cluster_freq[node] / n_partitions, nodes, n_partitions))\n",
    "    # else split this node\n",
    "    else:\n",
    "        for child in model.children_[node - leaf_node_count]:\n",
    "            total_partitions += 1\n",
    "            insert_sorted(partitions, (cluster_freq[child], [child], 1))\n",
    "print(\"num partitions after split\", total_partitions)\n",
    "\n",
    "# merge phase: merge partitions that are smaller than the smallest max_partitions\n",
    "merged = True\n",
    "i = j = 0\n",
    "while save_partitions and total_partitions > max_partitions:\n",
    "    # merge the largest small partition that isn't a multi-partition leaf\n",
    "    if merged:\n",
    "        print(len(partitions), i, j)\n",
    "        i = len(partitions) - 1\n",
    "        merged = False\n",
    "    else:\n",
    "        i -= 1\n",
    "    while i >= 0:\n",
    "        freq_i, nodes_i, n_partitions_i = partitions[i]\n",
    "        if n_partitions_i == 1 and freq_i < min_partition_size:\n",
    "            break\n",
    "        i -= 1\n",
    "    if i < 0:\n",
    "        # couldn't find a partition\n",
    "        print(\"unable to merge\")\n",
    "        break\n",
    "    # merge into the largest partition such that the combination is <= max_partition_size\n",
    "    j = len(partitions) - 1\n",
    "    while j > i:\n",
    "        freq_j, nodes_j, n_partitions_j = partitions[j]\n",
    "        if n_partitions_j == 1 and freq_i + freq_j <= max_partition_size:\n",
    "            break\n",
    "        j -= 1\n",
    "    if j == i:\n",
    "        # couldn't find a partition to merge into\n",
    "        continue\n",
    "    # merge partitions\n",
    "    merged = True\n",
    "    del partitions[j]\n",
    "    del partitions[i]\n",
    "    insert_sorted(partitions, (freq_i + freq_j, nodes_i + nodes_j, 1))\n",
    "    total_partitions -= 1\n",
    "print(\"total_partitions=\", total_partitions, len(partitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07279bf3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# histo on partition sizes\n",
    "partition_sizes = []\n",
    "n_small_partitions = 0\n",
    "for freq, _, n_partitions in partitions:\n",
    "    for ix in range(0, n_partitions):\n",
    "        partition_sizes.append(freq)\n",
    "        if freq < min_partition_size:\n",
    "            n_small_partitions += 1\n",
    "partition_sizes_df = pd.DataFrame(partition_sizes)\n",
    "partition_sizes_df.hist(bins=40)\n",
    "print(\"small partitions\", n_small_partitions)\n",
    "print(\"all partitions\", len(partition_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953faed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# histo in machine sizes assuming round-robin placement of partitions onto machines\n",
    "n_machines = 40\n",
    "machine_sizes = [0] * n_machines\n",
    "ix = 0\n",
    "for freq, _, n_partitions in partitions:\n",
    "    for _ in range(0, n_partitions):\n",
    "        machine_sizes[ix % n_machines] += freq\n",
    "        ix += 1\n",
    "machine_sizes_df = pd.DataFrame(machine_sizes)\n",
    "machine_sizes_df.hist(bins=n_machines) \n",
    "print(machine_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee456a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Split partition(s) into clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86196f50",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clusters = []\n",
    "# start with the partition nodes\n",
    "for _, nodes, _ in partitions:\n",
    "    for node in nodes:\n",
    "        distance = 0.0 if node < leaf_node_count else model.distances_[node - leaf_node_count]\n",
    "        insert_sorted(clusters, (distance, node))\n",
    "\n",
    "# then split each node into clusters if the node's distance is above threshold\n",
    "while True:\n",
    "    distance, cluster = clusters.pop()\n",
    "    if distance <= 1 - cluster_threshold:  # cluster threshold is measured in terms of (1 - distance)\n",
    "        insert_sorted(clusters, (distance, cluster))\n",
    "        break\n",
    "    for child in model.children_[cluster - leaf_node_count]:\n",
    "        distance = 0.0 if child < leaf_node_count else model.distances_[child - leaf_node_count]\n",
    "        insert_sorted(clusters, (distance, child))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2ffb9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save partitions and clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06120121",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_most_frequent_name(names):\n",
    "    most_freq_name = None\n",
    "    highest_freq = -1\n",
    "    for name in names:\n",
    "        freq = partition_name_freq.get(name, 0)\n",
    "        if freq > highest_freq:\n",
    "            most_freq_name = name\n",
    "            highest_freq = freq\n",
    "    return most_freq_name\n",
    "\n",
    "def name_finder(node_id):\n",
    "    return names_to_cluster[node_id] if node_id < leaf_node_count else None\n",
    "\n",
    "def gather_children(node_id, fn, result):\n",
    "    item = fn(node_id)\n",
    "    if item:\n",
    "        result.append(item)\n",
    "    elif node_id >= leaf_node_count:\n",
    "        for child in model.children_[node_id - leaf_node_count]:\n",
    "            gather_children(child, fn, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30c34a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# walk the cluster hierarchy to get the names in each cluster\n",
    "cluster2names = {}\n",
    "n_names_oov = 0\n",
    "for _, cluster in clusters:\n",
    "    names = []\n",
    "    gather_children(cluster, name_finder, names)\n",
    "    n_names = len(names)\n",
    "    # remove names not in vocab\n",
    "    names = [name for names if name in swivel_vocab]\n",
    "    n_names_oov += n_names - len(names)\n",
    "    if len(names) == 0:\n",
    "        print(\"WARN: empty cluster\", cluster)\n",
    "        continue\n",
    "    # the name of the cluster is the most-frequent name\n",
    "    freq_name = remove_padding(get_most_frequent_name(names))\n",
    "    cluster2names[freq_name] = names\n",
    "print(\"removed names out of vocab:\", n_names_oov)\n",
    "\n",
    "# invert cluster2names\n",
    "name_cluster = {}\n",
    "for cluster, names in cluster2names.items():\n",
    "    for name in names:\n",
    "        name_cluster[name] = cluster\n",
    "\n",
    "# write the dataframe to a csv file\n",
    "write_clusters(config.cluster_path, name_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d4d20",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if save_partitions:\n",
    "    # walk the cluster hierarchy to get the names in each partition\n",
    "    partition_clusters = []\n",
    "    for _, nodes, n_partitions in partitions:\n",
    "        names = []\n",
    "        for node in nodes:\n",
    "            gather_children(node, name_finder, names)\n",
    "        # remove names not in vocab\n",
    "        names = [name for names if name in swivel_vocab]\n",
    "        if len(names) == 0:\n",
    "            print(\"WARN empty partition\", nodes)\n",
    "            continue\n",
    "        partition_clusters.append((list(set(name_cluster[name] for name in names)), n_partitions))\n",
    "\n",
    "    # invert partition2names to get a dataframe with name, partition pairs\n",
    "    partition_number = 0\n",
    "    cluster_partition_cluster = []\n",
    "    cluster_partition_partition = []\n",
    "    cluster_partition_count = []\n",
    "    for clusts, n_partitions in partition_clusters:\n",
    "        for cluster in clusts:\n",
    "            cluster_partition_cluster.append(cluster)\n",
    "            cluster_partition_partition.append(partition_number)\n",
    "            cluster_partition_count.append(n_partitions)\n",
    "        partition_number += n_partitions\n",
    "    cluster_partition_df = pd.DataFrame({\n",
    "        \"cluster\": cluster_partition_cluster,\n",
    "        \"start_partition\": cluster_partition_partition,\n",
    "        \"n_partitions\": cluster_partition_count,\n",
    "    })\n",
    "\n",
    "    # write the dataframe to a csv file\n",
    "    cluster_partition_df.to_csv(config.cluster_partition_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cfb178",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if save_partitions:\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    cluster_partition_df[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae1f38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
