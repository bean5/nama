{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from src.data.filesystem import fopen\n",
    "from src.data.utils import load_dataset, select_frequent_k\n",
    "from src.eval import metrics\n",
    "from src.models.swivel import SwivelModel, get_swivel_embeddings, get_best_swivel_matches\n",
    "from src.models.swivel_encoder import SwivelEncoderModel, convert_names_to_model_inputs, train_swivel_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "given_surname = \"given\"\n",
    "vocab_size = 610000 if given_surname == \"given\" else 2100000\n",
    "encoder_vocab_size = vocab_size\n",
    "sample_size = 10000\n",
    "embed_dim = 100\n",
    "n_layers = 2\n",
    "n_epochs = 100 if n_layers == 1 else 200 if n_layers == 2 else 400 if n_layers == 3 else 800\n",
    "DROPOUT = 0.0\n",
    "num_matches = 500\n",
    "\n",
    "Config = namedtuple(\"Config\", \"train_path eval_path test_path embed_dim n_layers char_embed_dim n_hidden_units bidirectional lr batch_size use_adam_opt pack n_epochs swivel_vocab_path swivel_model_path encoder_model_path\")\n",
    "config = Config(\n",
    "    train_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train-augmented.csv.gz\",\n",
    "    eval_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-train.csv.gz\",\n",
    "    test_path=f\"s3://familysearch-names/processed/tree-hr-{given_surname}-test.csv.gz\",\n",
    "    embed_dim=embed_dim,\n",
    "    n_layers = n_layers,\n",
    "    char_embed_dim = 64,\n",
    "    n_hidden_units = 400,\n",
    "    bidirectional = True,\n",
    "    lr = 0.03,\n",
    "    batch_size = 256,\n",
    "    use_adam_opt = False,\n",
    "    pack = True,\n",
    "    n_epochs=n_epochs,\n",
    "    swivel_vocab_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-vocab-{vocab_size}-augmented.csv\",\n",
    "    swivel_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-swivel-model-{vocab_size}-{embed_dim}-augmented.pth\",\n",
    "    encoder_model_path=f\"s3://nama-data/data/models/fs-{given_surname}-encoder-model-{encoder_vocab_size}-{embed_dim}-{n_layers}-augmented.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(\"cuda total\", torch.cuda.get_device_properties(0).total_memory)\n",
    "print(\"cuda reserved\", torch.cuda.memory_reserved(0))\n",
    "print(\"cuda allocated\", torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"nama\",\n",
    "    entity=\"nama\",\n",
    "    name=\"63_swivel_encoder\",\n",
    "    group=given_surname,\n",
    "    notes=\"\",\n",
    "    config=config._asdict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_names_train, weighted_actual_names_train, candidate_names_train = load_dataset(config.train_path)\n",
    "input_names_eval, weighted_actual_names_eval, candidate_names_eval = load_dataset(config.eval_path, is_eval=True)\n",
    "input_names_test, weighted_actual_names_test, candidate_names_test = load_dataset(config.test_path, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"input_names_eval\", len(input_names_eval))\n",
    "print(\"weighted_actual_names_eval\", sum(len(wan) for wan in weighted_actual_names_eval))\n",
    "print(\"candidate_names_eval\", len(candidate_names_eval))\n",
    "\n",
    "print(\"input_names_test\", len(input_names_test))\n",
    "print(\"weighted_actual_names_test\", sum(len(wan) for wan in weighted_actual_names_test))\n",
    "print(\"candidate_names_test\", len(candidate_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swivel_vocab_df = pd.read_csv(fopen(config.swivel_vocab_path, \"rb\"))\n",
    "print(swivel_vocab_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swivel_vocab = {name: _id for name, _id in zip(swivel_vocab_df[\"name\"], swivel_vocab_df[\"index\"])}\n",
    "print(swivel_vocab[\"<john>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "swivel_model = SwivelModel(len(swivel_vocab), config.embed_dim)\n",
    "swivel_model.load_state_dict(torch.load(fopen(config.swivel_model_path, \"rb\")))\n",
    "swivel_model.eval()\n",
    "swivel_model.to(device)\n",
    "print(swivel_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train using all names in the vocabulary or a subset\n",
    "if encoder_vocab_size >= len(swivel_vocab):\n",
    "    train_names = list(swivel_vocab.keys())\n",
    "else:\n",
    "    input_names_train, weighted_actual_names_train, candidate_names_train = \\\n",
    "        select_frequent_k(input_names_train,\n",
    "                          weighted_actual_names_train,\n",
    "                          candidate_names_train,\n",
    "                          encoder_vocab_size)\n",
    "    train_names = list(set(input_names_train).union(set(candidate_names_train)))\n",
    "print(\"train_names\", len(train_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# free memory\n",
    "input_names_train = weighted_actual_names_train = candidate_names_train = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_embeddings = torch.Tensor(get_swivel_embeddings(swivel_model, swivel_vocab, train_names))\n",
    "print(train_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_inputs = convert_names_to_model_inputs(train_names)\n",
    "print(train_inputs.shape)\n",
    "print(train_inputs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "encoder_model = SwivelEncoderModel(n_layers=config.n_layers,\n",
    "                                   char_embed_dim=config.char_embed_dim,\n",
    "                                   n_hidden_units=config.n_hidden_units,\n",
    "                                   output_dim=config.embed_dim,\n",
    "                                   bidirectional=config.bidirectional,\n",
    "                                   pack=config.pack,\n",
    "                                   dropout=DROPOUT,\n",
    "                                   device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = train_swivel_encoder(encoder_model,\n",
    "                         train_inputs,\n",
    "                         train_embeddings,\n",
    "                         num_epochs=config.n_epochs,\n",
    "                         batch_size=config.batch_size,\n",
    "                         lr=config.lr,\n",
    "                         use_adam_opt=config.use_adam_opt,\n",
    "                         use_mse_loss=False,\n",
    "                         checkpoint_path=config.encoder_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(encoder_model.state_dict(), fopen(config.encoder_model_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model = SwivelEncoderModel(n_layers=config.n_layers,\n",
    "                                   char_embed_dim=config.char_embed_dim,\n",
    "                                   n_hidden_units=config.n_hidden_units,\n",
    "                                   output_dim=config.embed_dim,\n",
    "                                   bidirectional=config.bidirectional,\n",
    "                                   pack=config.pack,\n",
    "                                   device=device)\n",
    "encoder_model.load_state_dict(torch.load(fopen(config.encoder_model_path, \"rb\")))\n",
    "encoder_model.eval()\n",
    "encoder_model.device = device\n",
    "encoder_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sample data\n",
    "_, input_names_sample, _, weighted_actual_names_sample = \\\n",
    "   train_test_split(input_names_eval, weighted_actual_names_eval, test_size=sample_size)\n",
    "candidate_names_sample = candidate_names_eval\n",
    "print(\"input_names_sample\", len(input_names_sample))\n",
    "print(\"canidate_names_sample\", len(candidate_names_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get best matches\n",
    "# NOTE: only considers as potential matches names in candidate_names, not names in input_names\n",
    "batch_size = 256\n",
    "add_context = True\n",
    "n_jobs=1\n",
    "best_matches = get_best_swivel_matches(model=None,\n",
    "                                       vocab=None,\n",
    "                                       input_names=input_names_sample,\n",
    "                                       candidate_names=candidate_names_sample,\n",
    "                                       k=num_matches,\n",
    "                                       batch_size=batch_size,\n",
    "                                       add_context=add_context,\n",
    "                                       encoder_model=encoder_model,\n",
    "                                       n_jobs=n_jobs,\n",
    "                                       progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics.precision_weighted_recall_curve_at_threshold(\n",
    "    weighted_actual_names_sample, best_matches, min_threshold=0.01, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(metrics.get_auc(\n",
    "    weighted_actual_names_sample, best_matches, min_threshold=0.1, max_threshold=1.0, step=0.05, distances=False\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "_, input_names_sample, _, weighted_actual_names_sample = \\\n",
    "   train_test_split(input_names_test, weighted_actual_names_test, test_size=sample_size)\n",
    "candidate_names_sample = candidate_names_test\n",
    "print(\"input_names_sample\", len(input_names_sample))\n",
    "print(\"canidate_names_sample\", len(candidate_names_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_zero = n_one = n_two = 0\n",
    "for input_name, wans in zip(input_names_sample, weighted_actual_names_sample):\n",
    "    for actual_name, _, _ in wans:\n",
    "        if input_name in swivel_vocab and actual_name in swivel_vocab and input_name != actual_name:\n",
    "            n_two += 1\n",
    "        elif input_name in swivel_vocab or actual_name in swivel_vocab:\n",
    "            n_one += 1\n",
    "        else:\n",
    "            n_zero += 1\n",
    "print(\"two names in vocab (should not be possible)\", n_two)\n",
    "print(\"one name in vocab\", n_one)\n",
    "print(\"zero names in vocab\", n_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best matches\n",
    "# NOTE: only considers as potential matches names in candidate_names, not names in input_names\n",
    "batch_size = 256\n",
    "add_context = True\n",
    "n_jobs=1\n",
    "best_matches = get_best_swivel_matches(model=None,\n",
    "                                       vocab=None,\n",
    "                                       input_names=input_names_sample,\n",
    "                                       candidate_names=candidate_names_sample,\n",
    "                                       k=num_matches,\n",
    "                                       batch_size=batch_size,\n",
    "                                       add_context=add_context,\n",
    "                                       encoder_model=encoder_model,\n",
    "                                       n_jobs=n_jobs,\n",
    "                                       progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_weighted_recall_curve_at_threshold(\n",
    "    weighted_actual_names_sample, best_matches, min_threshold=0.01, max_threshold=1.0, step=0.05, distances=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.get_auc(\n",
    "    weighted_actual_names_sample, best_matches, min_threshold=0.1, max_threshold=1.0, step=0.05, distances=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_names = [\"<john>\", \"<johnny>\", \"<jonathan>\",\n",
    "              \"<mary>\", \"<marie>\", \"<maria>\"]\n",
    "test_embeddings = torch.Tensor(get_swivel_embeddings(swivel_model, swivel_vocab, test_names))\n",
    "print(test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(test_names[0:3])\n",
    "print(cosine_similarity(test_embeddings[0:1], test_embeddings[0:3]))\n",
    "print(test_names[3:])\n",
    "print(cosine_similarity(test_embeddings[0:1], test_embeddings[3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_model_inputs = convert_names_to_model_inputs(test_names)\n",
    "print(test_model_inputs.shape)\n",
    "print(test_model_inputs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "n_layers = 1\n",
    "char_embed_dim = 64\n",
    "n_hidden_units = 200\n",
    "embed_dim = 100\n",
    "bidirectional = True\n",
    "pack = False\n",
    "encoder_model = SwivelEncoderModel(n_layers=n_layers, char_embed_dim=char_embed_dim, n_hidden_units=n_hidden_units,\n",
    "                                   output_dim=embed_dim, bidirectional=bidirectional, pack=pack, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epochs=100\n",
    "use_adam_opt = False\n",
    "use_mse_loss = False\n",
    "train_swivel_encoder(encoder_model, test_model_inputs, test_embeddings, num_epochs=n_epochs, batch_size=64, lr=lr,\n",
    "                     use_adam_opt=use_adam_opt, use_mse_loss=use_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_embeddings_predicted = encoder_model(test_model_inputs).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_embeddings_numpy = test_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cosine_similarity(test_embeddings_numpy, test_embeddings_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cosine_similarity(test_embeddings_numpy, test_embeddings_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Replicate model training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create optimizer and loss function\n",
    "batch_size = 16\n",
    "lr = 0.05\n",
    "\n",
    "optimizer = torch.optim.Adam(encoder_model.parameters(), lr=lr)\n",
    "# optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create data loader\n",
    "dataset_train = torch.utils.data.TensorDataset(test_model_inputs, test_embeddings)\n",
    "data_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get batch\n",
    "train_batch, targets_batch = next(iter(data_loader))\n",
    "print(train_batch.shape)\n",
    "print(targets_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.data import constants\n",
    "X = train_batch\n",
    "encoder_model.to(device=device)\n",
    "\n",
    "# Compute forward pass\n",
    "# x_prime = model(train_batch)\n",
    "\n",
    "# Clear out gradient\n",
    "encoder_model.zero_grad()\n",
    "\n",
    "# forward pass\n",
    "X = X.to(device=device)\n",
    "batch_size, seq_len = train_batch.size()\n",
    "print(\"batch_size\", batch_size, \"seq_len\", seq_len)\n",
    "\n",
    "# init hidden state before each batch\n",
    "n_directions = 2 if bidirectional else 1\n",
    "# hidden = (\n",
    "#     torch.randn(n_layers * n_directions, batch_size, n_hidden_units).to(device=device),  # initial hidden state\n",
    "#     torch.randn(n_layers * n_directions, batch_size, n_hidden_units).to(device=device),  # initial cell state\n",
    "# )\n",
    "\n",
    "# sort batch by sequence length\n",
    "# X_lengths = torch.count_nonzero(X, dim=1).to(device=\"cpu\").type(torch.int64)\n",
    "# ixs = torch.argsort(X_lengths, descending=True)\n",
    "# X = X[ixs]\n",
    "# X_lengths = X_lengths[ixs]\n",
    "# print(\"X\", X.get_device(), \"X_lengths\", X_lengths.get_device())\n",
    "\n",
    "\n",
    "eye = torch.eye(constants.VOCAB_SIZE + 1).to(device=device)\n",
    "X = eye[X]\n",
    "\n",
    "# pack sequences\n",
    "# X = pack_padded_sequence(X, X_lengths, batch_first=True, enforce_sorted=True)\n",
    "\n",
    "# run through LSTM\n",
    "# all, hidden = encoder_model.lstm(X.to(device), hidden)\n",
    "all, (hidden, cell) = encoder_model.lstm(X.to(device))\n",
    "print(\"hidden\", hidden.shape, cell.shape)\n",
    "\n",
    "embeddings = encoder_model.linear(hidden[0][-1])  # compute the linear model based on the last hidden state of the last layer\n",
    "print(\"embeddings\", embeddings.shape)\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fn(embeddings, targets_batch.to(encoder_model.device))\n",
    "# do the backward pass and update parameters\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nama",
   "language": "python",
   "name": "nama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
